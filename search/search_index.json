{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"documentations materials","title":"Home"},{"location":"#documentations-materials","text":"","title":"documentations &amp; materials"},{"location":"Curamus_project/","text":"Titre du projet STARTbio, un programme de formation \u00e0 l\u2019analyse bioinformatique des donn\u00e9es de canc\u00e9rologie pour les chercheurs du SIRIC CURAMUS R\u00e9sum\u00e9 scientifique (15 lignes max) Les nouvelles technologies de s\u00e9quen\u00e7age g\u00e9n\u00e8rent d\u2019\u00e9normes quantit\u00e9s de donn\u00e9es qui n\u00e9cessitent un traitement informatique complexe pour leur analyse. Ainsi, les \u00e9quipes de biologistes et de m\u00e9decins qui n\u2019ont pas de comp\u00e9tences solides en bioinformatique se trouvent aujourd\u2019hui limit\u00e9es dans leurs recherches. Pour aider \u00e0 d\u00e9passer cette limitation, la plateforme de bioinformatique ARTbio propose aux chercheurs du SIRIC Curamus le programme de formation STARTbio, S erial T raining with ARTbio . STARTbio \u00e9tablira les bases m\u00e9thodologiques de l\u2019analyse informatique et couvrira, graduellement, les principales approches analytiques en canc\u00e9rologie: analyse des variants g\u00e9n\u00e9tiques, profilage du transcriptome et des ARN non codants, profilage de cellules uniques et d\u00e9tection/assemblage des g\u00e9nomes viraux. La formation alliera \u00e9troitement th\u00e9ories et pratiques et s\u2019appuiera sur des m\u00e9thodes p\u00e9dagogiques innovantes: cahiers num\u00e9riques versionn\u00e9s, environnements d'entra\u00eenement Galaxy, Rstudio et Jupyter, pratique quotidienne \u201cflash\u201d, documentation ex\u00e9cutable et r\u00e9seau d\u2019entraide en ligne. STARTbio est ainsi une opportunit\u00e9 pour le SIRIC Curamus de mettre en place un programme de diffusion des connaissances et des nouvelles pratiques n\u00e9cessaires \u00e0 la bioinformatique des donn\u00e9es de canc\u00e9rologie. Projet de recherche (5 pages max) \u00c9tat de la question La recherche en biologie est entr\u00e9e assez brusquement dans l\u2019\u00e8re du \u201cBig Data\u201d avec l\u2019av\u00e8nement des nouvelles technologies telles que le s\u00e9quen\u00e7age \u00e0 haut d\u00e9bit, les approches \u201cmulti-omics\u201d ou l\u2019imagerie haute-r\u00e9solution ou en continu. Le s\u00e9quen\u00e7age de cellules uniques qui \u00e9tait par exemple une approche confidentielle il y a seulement 5 ans, impacte aujourd\u2019hui tr\u00e8s fortement la biologie comme la m\u00e9decine, et notamment la canc\u00e9rologie. On peut citer aussi le s\u00e9quen\u00e7age haut-d\u00e9bit de tr\u00e8s longues mol\u00e9cules d\u2019ADN qui permet aujourd\u2019hui d\u2019assembler des g\u00e9nomes ou d\u2019identifier des liaisons g\u00e9n\u00e9tiques \u00e0 une vitesse et avec une pr\u00e9cision d\u00e9cupl\u00e9es. On peut anticiper enfin qu\u2019\u00e0 br\u00e8ve \u00e9ch\u00e9ance, les m\u00e9thodes d\u2019intelligence artificielles s\u2019imposeront aux biologistes et m\u00e9decins car elles permettent d\u2019extraire des informations de donn\u00e9es de grandes dimensions avec des performances in\u00e9gal\u00e9es. Ainsi, l\u2019informatique et les statistiques deviennent des domaines de comp\u00e9tence cl\u00e9s en biologie, simplement parce que cette science, comme d\u2019autres avant (Physique, Astronomie, etc.), devient une \u201cscience de donn\u00e9es\u201d. Or, les biologistes et les m\u00e9decins n\u2019ont g\u00e9n\u00e9ralement pas eu le temps d\u2019acqu\u00e9rir ces comp\u00e9tences durant leurs formations initiales et ces manques tendent \u00e0 se creuser avec l\u2019\u00e9volution rapide des m\u00e9thodes et des technologies. Dans ce contexte, les plateformes de bioinformatique rencontrent une forte demande, en particulier d\u2019\u00e9quipes qui n\u2019ont pas l\u2019opportunit\u00e9 d\u2019int\u00e9grer des bioinformaticiens dans leur effectif. Cependant, cette solution d\u2019analyses sur prestations n\u2019est pas satisfaisante, ni dans la pratique, ni sur le fond. Dans la pratique, \u00e0 moins de multiplier les plateformes d\u2019analyses, ce qui n\u2019est pas possible, celles-ci ne pourront satisfaire une demande en croissance et r\u00e9clamant des comp\u00e9tences toujours plus diverses et sp\u00e9cifiques; il est \u00e0 noter que pour les m\u00eame raisons de moyens, la solution consistant \u00e0 immerger, m\u00eame temporairement, un bioinformaticien dans toute \u00e9quipe de recherche le n\u00e9cessitant n\u2019est pas r\u00e9aliste. Sur le fond, si l\u2019analyse des donn\u00e9es de grands volumes ou de hautes dimensions traduit bien le changement de paradigme entam\u00e9 depuis quelques ann\u00e9es, les biologistes int\u00e9greront t\u00f4t ou tard l\u2019expertise requise dans leur profil et deviendront, en quelque sorte, leurs propres informaticiens ou statisticiens. C\u2019est en effet dans la nature de la recherche scientifique que d\u2019int\u00e9grer les concepts et m\u00e9thodes n\u00e9cessaires \u00e0 ses explorations. Pour acqu\u00e9rir des comp\u00e9tences solides en bioinformatique, les chercheurs peuvent se tourner vers des programmes propos\u00e9s par les universit\u00e9s, le CNRS, l\u2019Inserm, ou des plateformes de bioinformatiques qui maillent le territoire acad\u00e9mique et sont affili\u00e9es \u00e0 l\u2019Institut Fran\u00e7ais de Bioinformatiques (1) . A l\u2019exception de l\u2019offre de formation continue de l\u2019Universit\u00e9 Paris Diderot d\u00e9velopp\u00e9e en partenariat avec l\u2019IFB, ces formations en bioinformatique peuvent \u00eatre class\u00e9es en deux cat\u00e9gories : des introductions g\u00e9n\u00e9ralistes, comme par exemple une initiation au langage de programmation R. Ces formations sont n\u00e9cessaires et adapt\u00e9es au plus grand nombre mais ne permettent pas d\u2019acqu\u00e9rir un niveau suffisant pour aborder r\u00e9ellement des probl\u00e8mes de recherche. des formations avanc\u00e9es \u00e0 des m\u00e9thodologies pointues (par exemple \u201cDNA methylation and bisulfite treatment\u201d, \u201cPhylog\u00e9nie mol\u00e9culaire\u201d, etc), qui s\u2019adressent \u00e0 des chercheurs ayant d\u00e9j\u00e0 des bases solides en bioinformatique, bases que les formations g\u00e9n\u00e9ralistes \u00e9voqu\u00e9es ci-dessus peinent \u00e0 d\u00e9livrer. Ainsi, bien que les formations de qualit\u00e9 soient nombreuses, elles sont morcel\u00e9es th\u00e9matiquement et g\u00e9ographiquement. Pour doter les biologistes et les m\u00e9decins d\u2019une expertise de fond, de nature \u00e0 les rendre autonome dans la fouille de leurs donn\u00e9es, il nous semble important d\u2019 int\u00e9grer les diff\u00e9rentes \u00e9tapes n\u00e9cessaire \u00e0 l\u2019apprentissage de la bioinformatique, en partant des notions de bases jusqu\u2019au m\u00e9thodes sp\u00e9cifiques appropri\u00e9es \u00e0 leurs recherches, et de construire un savoir-faire durable en favorisant la pratique et les situations de recherche r\u00e9elles de fa\u00e7on r\u00e9currente et sur une dur\u00e9e significative. Justification de la demande Bien que les Siric aient pour mission de mettre en place des programmes de diffusion des connaissances et des nouvelles pratiques, et ce dans le but d\u2019am\u00e9liorer la recherche translationnelle en canc\u00e9rologie, les biologistes et m\u00e9decins du SIRIC Curamus n\u2019ont pas \u00e0 leur disposition de programme de formations d\u00e9di\u00e9 pour se former \u00e0 l\u2019analyse bioinformatique des donn\u00e9es biologiques. La formation des chercheurs est l\u2019un des objectifs affich\u00e9s d\u00e8s sa cr\u00e9ation \u00e0 l\u2019IBPS en 2015. Pour aborder cet objectif, la plateforme s\u2019appuie sur plusieurs atouts. Son responsable, CA, a conduit auparavant une \u00e9quipe de recherche en g\u00e9n\u00e9tique et conna\u00eet bien les questions qui se posent aux chercheurs en situation d\u2019analyse de grand volume de donn\u00e9es. La plateforme est par ailleurs en contact permanent, \u00e0 travers ses prestations d\u2019analyse, avec les biologistes et m\u00e9decins, ce qui lui permet d\u2019avoir une vision pr\u00e9cise et pratique de leur besoins, de leurs difficult\u00e9s, et de leurs contraintes. ARTbio s\u2019est impliqu\u00e9e dans le SIRIC Curamus depuis sa candidature \u00e0 l\u2019INCA. Elle collabore \u00e9galement activement avec plusieurs \u00e9quipes affili\u00e9es aux SIRIC: L\u2019\u00e9quipe de F. Delhommeau (Leuc\u00e9mie Aigu\u00eb My\u00e9locytaire et ses pr\u00e9dispositions g\u00e9n\u00e9tiques), de M.P. Junnier H. Chneiweiss (Profilage du Glioblastome en cellule uniques (2) ) et de A.G. Marcellin et V. Calvez (recherche de loci de l\u2019herp\u00e8svirus impliqu\u00e9s dans les maladies de Kaposi et de Castleman). Elle conna\u00eet ainsi tr\u00e8s bien les axes th\u00e9matiques de Curamus ainsi que les besoins en bioinformatique qu\u2019ils suscitent. Finalement, ARTbio a d\u00e9j\u00e0 une exp\u00e9rience importante en mati\u00e8re de formation que ce soit dans le cadre de Masters de Sorbonne-Universit\u00e9 (Biologie des G\u00e9nomes et G\u00e9n\u00e9tique de la souris), celui du r\u00e9seau Elixir (analyse g\u00e9nomique sous Galaxy, Ljubljana 2018) ou \u00e0 l\u2019occasion de la journ\u00e9e \u201cGalaxy Spring Day\u201d (3) consacr\u00e9e \u00e0 l\u2019analyse RNAseq et financ\u00e9e par le SIRIC. De plus, la plateforme propose depuis 2019, une programme de compagnonnage, qui est un accompagnement didactique des chercheurs dans un projet de recherche, ayant pour but de r\u00e9aliser leurs analyses tout en leur apprenant les m\u00e9thodologies et proc\u00e9dures mises en oeuvre. L\u2019ensemble de ces points motive la proposition de la plateforme ARTbio pour le projet \u201c S erial T raining with ARTbio \u201d (STARTbio), qui vise \u00e0 mettre en oeuvre un programme de formation \u00e0 la bioinformatique des chercheurs du SIRIC CURAMUS adapt\u00e9 \u00e0 leurs besoins d\u2019analyse, bas\u00e9 sur l\u2019enseignement de notions th\u00e9oriques comme sur la pratique construit sur la longueur et la r\u00e9currence des cours pour un meilleurs ancrage de l\u2019apprentissage. Positionnement des \u00e9quipes postulantes par rapport \u00e0 la comp\u00e9tition internationale Plusieurs offres de formations sont disponibles \u00e0 l\u2019\u00e9chelle internationale, \u00e9manants par exemple de l\u2019organisation europ\u00e9enne Elixir, ou du projet international Galaxy. Ces initiatives ne peuvent \u00eatre consid\u00e9r\u00e9es comme comp\u00e9titives, bien au contraire. Non seulement, ARTbio contribue directement \u00e0 leur organisation, acqu\u00e9rant ainsi un savoir-faire pr\u00e9cieux, mais elle peut aussi b\u00e9n\u00e9ficier en retour du mat\u00e9riel p\u00e9dagogique tr\u00e8s abondant (Cours et pr\u00e9sentations en ligne, tutoriaux ex\u00e9cutables, listes bibliographiques, r\u00e9seaux d\u2019experts) d\u00e9ploy\u00e9 par la communaut\u00e9 Galaxy. Cette communaut\u00e9 Galaxy offre par exemple, via le Galaxy Training Network (4) plus de 80 tutoriaux qui peuvent \u00eatre directement \u201cex\u00e9cut\u00e9s\u201d en ligne sur des serveurs Galaxy d\u00e9di\u00e9s. Notre serveur Galaxy Mississippi est d\u2019ailleurs l\u2019un des serveurs propos\u00e9s pour suivre certains de ces tutoriaux. Il est aussi \u00e0 noter que la plateforme ARTbio est reconnue au sein du Galaxyproject pour son savoir-faire en mati\u00e8re de d\u00e9ploiement de serveurs d\u2019analyse bioinformatique. Cette position lui permet d\u2019envisager sereinement l\u2019organisation des ressources informatiques n\u00e9cessaire \u00e0 la formation, d\u2019autant plus que ses liens avec la communaut\u00e9 offre l\u2019opportunit\u00e9 de pouvoir disposer d\u2019acc\u00e8s aux grappes de machines de l\u2019universit\u00e9 de Freiburg (usegalaxy.eu) si cela est n\u00e9cessaire. Finalement, la plateforme ARTbio vient d\u2019\u00eatre labellis\u00e9e par l\u2019Institut Fran\u00e7ais de Bioinformatique, l\u2019IFB, qui si elle n\u2019est pas une organisation internationale, contribue de fa\u00e7on majeure \u00e0 structurer les efforts acad\u00e9miques en mati\u00e8re de bioinformatique. Ceci est une opportunit\u00e9 de b\u00e9n\u00e9ficier du savoir-faire de l\u2019IFB en mati\u00e8re d\u2019organisation de formations, ainsi que d\u2019une aide de la communaut\u00e9 fran\u00e7aise des plateformes de bioinformatique. Objectifs et m\u00e9thodologie d\u00e9velopp\u00e9e L\u2019essentiel du programme La formation couvrira 5 domaines de l\u2019analyse des donn\u00e9es en canc\u00e9rologie : Analyse des variants g\u00e9n\u00e9tiques Profilage du transcriptome Profilage des ARN non codants Profilage des cellules uniques M\u00e9tag\u00e9nomique virale. Ces 5 domaines seront abord\u00e9s apr\u00e8s l\u2019enseignement et la pratique d\u2019un socle de notions indispensables: Formation aux m\u00e9thodes d\u2019Int\u00e9gration Continue et de travail collaboratif en informatique Introduction au syst\u00e8me Unix et \u00e0 la manipulation des fichiers Formation au Cloud Computing (informatique sans machine) Introduction et utilisation des bases de donn\u00e9es biologiques, g\u00e9n\u00e9ralistes ou d\u00e9di\u00e9es aux cancers Utilisation de l\u2019environnement R pour les statistiques et la visualisation des donn\u00e9es Introduction \u00e0 l\u2019utilisation du langage de programmation Python Le contenu d\u00e9taill\u00e9 de la formation est pr\u00e9cis\u00e9 en Table 1. Analyse des variants - Responsable: Leonardo Panunzi La formation sera ax\u00e9e sur les m\u00e9thodes et outils pour caract\u00e9riser et annoter les variants g\u00e9nomiques: Single Nucleotide Polymorphism (SNP), Petites insertions ou d\u00e9l\u00e9tions (indels) et variants structurels issus de larges r\u00e9arrangements chromosomiques. Nous montrerons \u00e9galement comment r\u00e9aliser l'interpr\u00e9tation fonctionnelle et \u00e9valuer l\u2019impact des variants identifi\u00e9s \u00e0 l\u2019aide des bases de donn\u00e9es d\u00e9di\u00e9es (Table 1). Nous discuterons les avantages et les inconv\u00e9nients de chaque m\u00e9thode pr\u00e9sent\u00e9e. Les m\u00e9thodes et outils abord\u00e9s lors de la formation sont test\u00e9s et mis en place dans le cadre du projet national CONECT-AML (COllaborative NEtwork on research for Children and Teenagers with Acute Myeloid Leukemia) dans lequel nous sommes impliqu\u00e9s. Profilage du transcriptome - Responsable: Na\u00efra Naouar Nous exposerons aux participants les diff\u00e9rentes \u00e9tapes de l\u2019analyse, les outils \u00e0 utiliser et comment les param\u00e9trer. Nous pr\u00e9senterons les diff\u00e9rents outils d\u2019alignement avec leurs avantages et inconv\u00e9nients (Bowtie, Hisat2, STAR). Nous aborderons diff\u00e9rentes m\u00e9thodes de quantification d\u2019expression (Feature count, HTSeq-count) et leurs probl\u00e9matiques, en particulier avec les fichiers d\u2019annotation g\u00e9nomiques (GTF). Nous r\u00e9aliserons l\u2019analyse de l\u2019expression diff\u00e9rentielle et exposeront les limites statistiques \u00e0 l\u2019analyse : fitting de la variance, lois statistiques et probl\u00e8mes des tests multiples. Nous r\u00e9aliserons des analyses d\u2019enrichissement en se servant des ontologies, telle que la Gene Ontology, et argumenteront sur les limites de ce type d\u2019analyse. Nous caract\u00e9riserons les voies m\u00e9taboliques alt\u00e9r\u00e9es. Finalement, nous confronterons nos listes de g\u00e8nes d\u2019int\u00e9r\u00eat aux bases de donn\u00e9es d\u00e9di\u00e9es \u00e0 la canc\u00e9rologie (TCGA, The Human Protein Atlas, MutaGene, COSMIC, Genomic Data Commons Data Portal) de mani\u00e8re \u00e0 aller plus en avant dans l\u2019interpr\u00e9tation biologique des r\u00e9sultats avec les connaissances disponibles. Profilage des ARN non codants - Responsable: Christophe Antoniewski Nous utiliserons la suite d\u2019outils Mississippi qui a \u00e9t\u00e9 d\u00e9velopp\u00e9e \u00e0 cet effet. Nous apprendrons \u00e0 contr\u00f4ler et \u00e0 pr\u00e9parer des jeux de donn\u00e9es de s\u00e9quen\u00e7age des petits ARN (smallRNAseq), et, \u00e0 annoter les diff\u00e9rentes cat\u00e9gories de petits ARN d\u00e9tect\u00e9es apr\u00e8s alignement sur les g\u00e9nomes (miRNA, rRNA, siRNA, piRNA, snoRNA, snRNA, scaRNA, tRNA-tRF, \u2026). Nous montrerons comment visualiser les structures communes au sein de ces classes d\u2019ARN (tailles, compositions nucl\u00e9otidiques, structures secondaires, signatures g\u00e9nomiques, etc.) et comment regrouper les alignements pour identifier des loci producteurs de petits ARN. Nous aborderons les probl\u00e9matiques d\u2019expression diff\u00e9rentielle (miRNA profiling). Finalement, nous introduirons la probl\u00e9matique des petits ARN issus des s\u00e9quences g\u00e9nomiques r\u00e9p\u00e9t\u00e9es (profiling des transposons et r\u00e9trotransposons) Profilage des cellules uniques - Responsable: L\u00e9a Bellenger Nous nous attacherons aux m\u00e9thodes permettant de caract\u00e9riser l\u2019h\u00e9t\u00e9rog\u00e9n\u00e9it\u00e9 des transcriptomes de cellules canc\u00e9reuses. Pour cela, nous reprendrons les techniques permettant d\u2019effectuer les alignements de s\u00e9quences et comptages d\u2019un tr\u00e8s grand nombre d\u2019\u00e9chantillons gr\u00e2ce \u00e0 des environnements de calcul hautement parall\u00e9lis\u00e9es (Galaxy). Nous pr\u00e9senterons en d\u00e9tail les principales m\u00e9thodes statistiques pour analyser et visualiser les espaces de tr\u00e8s hautes dimensions et introduiront les m\u00e9thodes de r\u00e9duction dimensionnelles (PCA, t-SNE, UMAP) et de classifications (classification hi\u00e9rarchique, k-means, PAM, HCPC, r\u00e9gression logistique) des donn\u00e9es. Nous expliciterons les proc\u00e9dures \u00e0 suivre pour isoler des signatures g\u00e9niques permettant de diff\u00e9rencier les sous-populations cellulaires d\u2019une tumeur. Pour finir, nous aborderons les analyses diff\u00e9rentielles entre populations cellulaires multiples et l\u2019inf\u00e9rence de trajectoires entre ces populations. Metag\u00e9nomique virale - Responsable: Christophe Antoniewski Nous aborderons les m\u00e9thodes de metag\u00e9nomique pour aligner et analyser des s\u00e9quen\u00e7ages d\u2019\u00e9chantillons h\u00f4te/virus. Nous d\u00e9taillerons l\u2019utilisation des outils d\u2019assemblage de novo de s\u00e9quences et nous reprendrons en d\u00e9tail les alignements des s\u00e9quences nucl\u00e9iques et prot\u00e9iques avec les algorithmes blast (blastn, blastx, tblastx). la suite d\u2019outils Metavisitor (5, 6) d\u00e9velopp\u00e9e par ARTbio sera utilis\u00e9e. Cette suite disponible sur Galaxy permet de r\u00e9aliser une analyse compl\u00e8te de d\u00e9tection, d\u2019assemblage, de quantification et d\u2019annotation de l\u2019ensemble des g\u00e9nomes viraux. Plan exp\u00e9rimental Le programme de formation STARTbio (Table 1) pourra d\u00e9marrer en mars 2020 et s\u2019\u00e9chelonnera sur environ une ann\u00e9e. Il alliera diff\u00e9rents types de formats p\u00e9dagogiques: Des sessions courtes de 2 \u00e0 4 heures seront anim\u00e9es par les membres d\u2019ARTbio avec l\u2019aide d\u2019intervenants invit\u00e9s affili\u00e9s \u00e0 Curamus et serviront \u00e0 enseigner les bases th\u00e9oriques et pratiques de l\u2019analyse bioinformatique (Table 1). Ces sessions seront r\u00e9alis\u00e9es au sein des campus de Sorbonne-Universit\u00e9, en favorisant la proximit\u00e9 avec les sites de travail des participants du SIRIC (nous serions favorable \u00e0 une localisation dans un campus hospitalier si cela est techniquement r\u00e9alisable). Les sessions courtes allieront des pr\u00e9sentations th\u00e9oriques et des mises en pratiques instantan\u00e9es et individuelles. En effet, chaque participant pour toute la dur\u00e9e du programme, un environnement d\u2019analyse personnel en ligne regroupant un serveur Rstudio, Jupyter et Galaxy. Cinq sessions longues de 2 ou 3 jours sont pr\u00e9vue pour couvrir les 5 th\u00e9matiques principales de la formation. Elles inclueront des expos\u00e9s d\u2019introduction th\u00e9orique et pratique et mettront les participants en situation individuelle d\u2019analyse de jeux de donn\u00e9es r\u00e9cents ayant fait l\u2019objet d\u2019une publication. Ces sessions pourraient \u00eatre film\u00e9es et mises en ligne de mani\u00e8re \u00e0 constituer une base documentaire de tutoriaux en vid\u00e9o. Des tutoriaux ex\u00e9cutables seront pr\u00e9par\u00e9s et mis en ligne afin que les participants puissent revisiter \u00e0 leur rythme des notions ou protocoles d\u2019analyses pr\u00e9sent\u00e9s pendant les sessions courtes ou longues. Ces tutoriaux seront d\u00e9velopp\u00e9s dans les notebooks R-studio et Jupyter, ou \u00e0 l\u2019aide des workflow et des fonctions interactive tours de Galaxy. De nombreux exemples de ce type de tutoriaux peuvent \u00eatre trouv\u00e9s sur le site du Galaxy Training Network. Des exercices flash , \u00e0 ex\u00e9cuter en moins de 10 min sur le mode \u201ccomment faire pour ?\u201d, seront mis \u00e0 disposition des participants dans une documentation en ligne au format \u201creadthedocs\u201d. Ils permettront de constituer une base documentaire de savoir-faire bioinformatique cibl\u00e9 sur les th\u00e9matiques de Curamus et offrirons aux participants \u00e0 la formation l\u2019occasion d\u2019une pratique courte mais quotidienne des techniques d\u2019analyse des donn\u00e9es, qui dans notre exp\u00e9rience est une approche performante pour acqu\u00e9rir un savoir-faire durable. Les principes du FAIR data ( findable, accessible, interoperable, reusable ) seront enseign\u00e9s aux participants et appliqu\u00e9s au mat\u00e9riel p\u00e9dagogique de programme STARTbio : toutes les documentations et \u00e9l\u00e9ments de formation seront librement accessibles sur des d\u00e9p\u00f4ts Github et des serveurs publics Galaxy. Les \u00e9changes entre formateurs et participants seront favoris\u00e9s par l\u2019utilisation d\u2019outils de travail collaboratifs : le gestionnaire de projet Trello, un canal de communication Gitter li\u00e9 aux d\u00e9p\u00f4ts Github de STARbio et des notebooks num\u00e9riques partag\u00e9s. Le programme STARTbio s\u2019adresse \u00e0 des scientifiques confirm\u00e9s, experts dans leurs domaines mais aussi souvent dans d\u2019autres disciplines. Nous favoriserons donc les situations de p\u00e9dagogie invers\u00e9e o\u00f9 les \u00e9l\u00e8ves partagent leur savoir et savoir-faire, et tirerons parti de cette richesse pour instaurer un esprit de travail collaboratif et d'entraide entre tous les participants. Des \u00e9valuations et enqu\u00eates de satisfaction seront r\u00e9alis\u00e9es \u00e0 toutes les \u00e9tapes de la formation pour suivre les progr\u00e8s des participants, identifier les difficult\u00e9s rencontr\u00e9es et prendre en compte les critiques pour adapter les enseignements. Finalement, nous allons prendre contact avec CAPSULE , le Centre d\u2019Accompagnement pour la P\u00e9dagogie et SUpport \u00e0 L\u2019Exp\u00e9rimentation de Sorbonne-Universit\u00e9 afin de pouvoir b\u00e9n\u00e9ficier de leur exp\u00e9rience et de leur soutien logistique. R\u00e9sultats attendus Le projet STARTbio est une opportunit\u00e9 pour le SIRIC Curamus de mettre en place un programme de diffusion des connaissances et des nouvelles pratiques n\u00e9cessaires au traitement bioinformatique des donn\u00e9e de canc\u00e9rologie. Ax\u00e9 sur une volume d\u2019enseignement important et dot\u00e9 d\u2019outils p\u00e9dagogiques innovant, de former en profondeur les participants aux analyses dont ils ont besoins, tout en les sensibilisant aux principes FAIR adopt\u00e9s \u00e0 l\u2019\u00e9chelle europ\u00e9enne. Les mat\u00e9riaux p\u00e9dagogiques g\u00e9n\u00e9r\u00e9s et accessibles en ligne et la mise en place d\u2019un r\u00e9seau d'entraide faciliteront \u00e0 moyen terme l\u2019autoformation des personnels Les \u00e9valuations des connaissances et les enqu\u00eates effectu\u00e9es aupr\u00e8s des participants permettront \u00e9galement de dresser un typologie plus pr\u00e9cise des besoins en bioinformatique g\u00e9n\u00e9r\u00e9s par les programme de recherche du SIRIC Curamus. Bibliographie Liste des formations en Bioinformatique \\ https://docs.google.com/spreadsheets/d/1QUiakFq0xpjoVvIM1s92c_fWMgEB2h_Qx7WQ2ELNP4M/edit?usp=sharing . Saurty-Seerunghen,M.S., Bellenger,L., El-Habr,E.A., Delaunay,V., Garnier,D., Chneiweiss,H., Antoniewski,C., Morvan-Dubois,G. and Junier,M.-P. (2019) Capture at the single cell level of metabolic modules distinguishing aggressive and indolent glioblastoma cells. Acta Neuropathol Commun, 7, 155. Galaxy Spring Day. Training material \\ https://artbio.github.io/springday/#galaxy-spring-day-2019-reference-based-rnaseq-analysis . Galaxy Training https://galaxyproject.github.io/training-material/ Galaxy Training Network. Carissimo,G., van den Beek,M., Vernick,K.D. and Antoniewski,C. (2017) Metavisitor, a Suite of Galaxy Tools for Simple and Rapid Detection and Discovery of Viruses in Deep Sequence Data. PLoS One, 12, e0168397. Carissimo,G., Eiglmeier,K., Reveillaud,J., Holm,I., Diallo,M., Diallo,D., Vantaux,A., Kim,S., M\u00e9nard,D., Siv,S., et al. (2016) Identification and Characterization of Two Novel RNA Viruses from Anopheles gambiae Species Complex Mosquitoes. PLoS One, 11, e0153881. \u00c9quipes participantes Personnes impliqu\u00e9es, statut, et pourcentage de temps consacr\u00e9 \u00e0 la recherche - \u00c9quipes participantes - Principales publications des 5 derni\u00e8res ann\u00e9es Plateforme ARTbio Nom 1\u00e8re ann\u00e9e 2nde ann\u00e9e Responsabilit\u00e9 Christophe Antoniewski DR, CNRS 20% 315h 10% 158h Profilage des ARNs non codants ; M\u00e9tag\u00e9nomique virale Int\u00e9gration Continue Cloud Galaxy Python Na\u00efra Naouar IR, SU 20% 315h 10% 158h Profilage du transcriptome Unix RStudio Pr\u00e9sentation et utilisation des bases de donn\u00e9es Logistique L\u00e9a Bellenger IE, Inserm 15% 236h 10% 158h Profilage des cellules uniques Leonardo G Panunzi, Post-Doc, CNRS 15% 236h 10% 158h Analyse des variants g\u00e9n\u00e9tiques Totaux horaires 1102h 632h Toutes les \u00e9quipes int\u00e9ress\u00e9es de Curamus. Principales publications des 5 derni\u00e8res ann\u00e9es Mirca S. Saurty-Seerunghen, L\u00e9a Bellenger , Elias A. El-Habr, Virgile Delaunay, Delphine Garnier, Herv\u00e9 Chneiweiss, Christophe Antoniewski , Ghislaine Morvan-Dubois, Marie-Pierre Junier. 2019. \u201cCapture at the single cell level of metabolic modules distinguishing aggressive and indolent glioblastoma cells\u201d, Acta Neuropathologica Communications. van den Beek M, da Silva B, Pouch J, Ali Chaouche MEA, Carr\u00e9 C, Antoniewski C . 2018. \"Dual-layer transposon repression in heads of Drosophila melanogaster.\" RNA (12):1749-1760 Castel, David, Meryem B. Baghdadi, S\u00e9bastien Mella, Barbara Gayraud-Morel, Virginie Marty, J\u00e9r\u00f4me Cavaill\u00e9, Christophe Antoniewski, and Shahragim Tajbakhsh. 2018. \u201cSmall-RNA Sequencing Identifies Dynamic microRNA Deregulation during Skeletal Muscle Lineage Progression.\u201d Scientific Reports 8 (1): 4208. Carissimo, Guillaume, Marius van den Beek, Kenneth D. Vernick, and Christophe Antoniewski. 2017. \u201cMetavisitor, a Suite of Galaxy Tools for Simple and Rapid Detection and Discovery of Viruses in Deep Sequence Data.\u201d PloS One 12 (1): e0168397. Carissimo, Guillaume, Karin Eiglmeier, Julie Reveillaud, Inge Holm, Mawlouth Diallo, Diawo Diallo, Am\u00e9lie Vantaux, Christophe Antoniewski and Kenneth Vernick. 2016. \u201cIdentification and Characterization of Two Novel RNA Viruses from Anopheles Gambiae Species Complex Mosquitoes.\u201d PloS One 11 (5): e0153881. Budget pour le programme STARTbio limit\u00e9 \u00e0 16 participants par \u00e9dition: Co\u00fbt de pr\u00e9paration et de l'ex\u00e9cution de la formation STARTbio Premi\u00e8re ann\u00e9e - Mars 2020 \u00e0 F\u00e9vrier 2021 volume horaire par les membres d\u2019ARTbio : 1102 h co\u00fbt horaire ARTbio : 40 \u20ac Total 1\u00e8re ann\u00e9e : 44 080 \u20ac Deuxi\u00e8me ann\u00e9e - Mars 2021 \u00e0 F\u00e9vrier 2022 volume horaire par les membres d\u2019ARTbio: 632 h co\u00fbt horaire : 40 \u20ac Total 2nde ann\u00e9e: 25 280 \u20ac Total: 69 360 \u20ac Co\u00fbt support\u00e9 par 2 * 16 participants : 29 360 \u20ac soit 920 \u20ac par participant Montant demand\u00e9 au SIRIC Curamus : 40 000\u20ac Nous proposons que les co\u00fbts de la formation soit pris en charge en partie par les participants, soit 920 \u20ac pour une participation au programme, et en partie par le SIRIC, soit un financement de 40 000 \u20ac vers\u00e9s \u00e0 ARTbio. Table 1. Programme d\u00e9taill\u00e9 de la formation Volume horaire en pr\u00e9sence d\u2019un encadrant Timeline Volume horaire r\u00e9alis\u00e9 de mani\u00e8re autonome* Int\u00e9gration Continue 8h Mars 10h Cloud 4h Mars 5h Unix 4h Avril 5h Pr\u00e9sentation et Utilisation des bases de donn\u00e9es 8h Avril - Mai 20h Th\u00e9matiques Principales 69h Avril - F\u00e9vrier 10h Partie I : Initiation et Galaxy Training Network 2h Avril - F\u00e9vrier 10h Partie II : Analyse de donn\u00e9es Profilage du transcriptome M\u00e9tag\u00e9nomique virale Profilage des ARN non codants Profilage des cellules uniques Analyse des variants g\u00e9n\u00e9tiques 67h 14h 14h 14h 18h 21h Avril - Janvier Avril Juin Septembre Novembre Janvier - Python 6h Sept. - F\u00e9vrier 20h Partie I : Initiation - Jupyter Notebook 6h Sept. 10h Partie II : Analyse de donn\u00e9es - Sept. - F\u00e9vrier 10h Rstudio 14h Oct. - F\u00e9vrier 50h Partie I : Initiation 14h Oct. - Nov. 10h Partie II : Analyse de donn\u00e9es - Nov. - F\u00e9vrier 40h Bilan 4h F\u00e9vrier - Totaux 117h Mars 19 - F\u00e9vrier 20 120h * Tutoriaux ex\u00e9cutables ou exercices \u201cflash\u201d Contenu d\u00e9taill\u00e9 : Int\u00e9gration Continue Introduction (1h) Partage des donn\u00e9es et transparence - Open access Open source - FAIR data (1h) GitHub : Introduction (2h) - Exercices pratiques* (5h) Travail en \u00e9quipe - Google Apps : Introduction (4h) - Exercices pratiques* (5h) Cloud Principes et outils (2h) Stockage Computing : Introduction (2h) - Exercices pratiques* (5h) Unix Introduction (2h) Expressions r\u00e9guli\u00e8res (2h) Exercices pratiques* (5h) Pr\u00e9sentation et Utilisation des bases de donn\u00e9es G\u00e9n\u00e9ralistes : Introduction (4h) - Exercices pratiques* (10h) Ensembl, NCBI, UCSC D\u00e9di\u00e9es au Cancer : Introduction (4h) - Exercices pratiques* (10h) TCGA, The Human Protein Atlas, MutaGene, COSMIC, Genomic Data Commons Data Portal Th\u00e9matiques principales Partie I : Initiation (2h) Galaxy Training Network* (10h) Th\u00e9matiques principales Partie II : Profilage du transcriptome (14h - Avril ) M\u00e9tag\u00e9nomique virale(14h - Juin ) Profilage des ARN non codants (14h - Septembre ) Profilage des cellules uniques (18h - Novembre ) Analyse des variants g\u00e9n\u00e9tiques (21h - Janvier ) Python Partie I : Initiation (2h) Jupyter Notebook : Introduction (4h) - Exercices pratiques* (10h) Python Partie II : Exercices pratiques* (20h) RStudio Partie I : Initiation (4h) Introduction \u00e0 R Markdown (2h) Tests statistiques (2h) Visualisation des donn\u00e9es (2h) Classification et r\u00e9gressions logistiques (4h) Exercices pratiques* (10h) RStudio Partie II : Profilage du transcriptome* (20h) Profilage du transcriptome en cellule unique* (20h) * enseignements en autonomie sous format \u201ctutoriel ex\u00e9cutable\u201d ou \u201cexercice flash\u201d","title":"Curamus Project"},{"location":"Curamus_project/#titre-du-projet","text":"","title":"Titre du projet"},{"location":"Curamus_project/#startbio-un-programme-de-formation-a-lanalyse-bioinformatique-des-donnees-de-cancerologie-pour-les-chercheurs-du-siric-curamus","text":"","title":"STARTbio, un programme de formation \u00e0 l\u2019analyse bioinformatique des donn\u00e9es de canc\u00e9rologie pour les chercheurs du SIRIC CURAMUS"},{"location":"Curamus_project/#resume-scientifique-15-lignes-max","text":"Les nouvelles technologies de s\u00e9quen\u00e7age g\u00e9n\u00e8rent d\u2019\u00e9normes quantit\u00e9s de donn\u00e9es qui n\u00e9cessitent un traitement informatique complexe pour leur analyse. Ainsi, les \u00e9quipes de biologistes et de m\u00e9decins qui n\u2019ont pas de comp\u00e9tences solides en bioinformatique se trouvent aujourd\u2019hui limit\u00e9es dans leurs recherches. Pour aider \u00e0 d\u00e9passer cette limitation, la plateforme de bioinformatique ARTbio propose aux chercheurs du SIRIC Curamus le programme de formation STARTbio, S erial T raining with ARTbio . STARTbio \u00e9tablira les bases m\u00e9thodologiques de l\u2019analyse informatique et couvrira, graduellement, les principales approches analytiques en canc\u00e9rologie: analyse des variants g\u00e9n\u00e9tiques, profilage du transcriptome et des ARN non codants, profilage de cellules uniques et d\u00e9tection/assemblage des g\u00e9nomes viraux. La formation alliera \u00e9troitement th\u00e9ories et pratiques et s\u2019appuiera sur des m\u00e9thodes p\u00e9dagogiques innovantes: cahiers num\u00e9riques versionn\u00e9s, environnements d'entra\u00eenement Galaxy, Rstudio et Jupyter, pratique quotidienne \u201cflash\u201d, documentation ex\u00e9cutable et r\u00e9seau d\u2019entraide en ligne. STARTbio est ainsi une opportunit\u00e9 pour le SIRIC Curamus de mettre en place un programme de diffusion des connaissances et des nouvelles pratiques n\u00e9cessaires \u00e0 la bioinformatique des donn\u00e9es de canc\u00e9rologie.","title":"R\u00e9sum\u00e9 scientifique (15 lignes max)"},{"location":"Curamus_project/#projet-de-recherche-5-pages-max","text":"","title":"Projet de recherche (5 pages max)"},{"location":"Curamus_project/#etat-de-la-question","text":"La recherche en biologie est entr\u00e9e assez brusquement dans l\u2019\u00e8re du \u201cBig Data\u201d avec l\u2019av\u00e8nement des nouvelles technologies telles que le s\u00e9quen\u00e7age \u00e0 haut d\u00e9bit, les approches \u201cmulti-omics\u201d ou l\u2019imagerie haute-r\u00e9solution ou en continu. Le s\u00e9quen\u00e7age de cellules uniques qui \u00e9tait par exemple une approche confidentielle il y a seulement 5 ans, impacte aujourd\u2019hui tr\u00e8s fortement la biologie comme la m\u00e9decine, et notamment la canc\u00e9rologie. On peut citer aussi le s\u00e9quen\u00e7age haut-d\u00e9bit de tr\u00e8s longues mol\u00e9cules d\u2019ADN qui permet aujourd\u2019hui d\u2019assembler des g\u00e9nomes ou d\u2019identifier des liaisons g\u00e9n\u00e9tiques \u00e0 une vitesse et avec une pr\u00e9cision d\u00e9cupl\u00e9es. On peut anticiper enfin qu\u2019\u00e0 br\u00e8ve \u00e9ch\u00e9ance, les m\u00e9thodes d\u2019intelligence artificielles s\u2019imposeront aux biologistes et m\u00e9decins car elles permettent d\u2019extraire des informations de donn\u00e9es de grandes dimensions avec des performances in\u00e9gal\u00e9es. Ainsi, l\u2019informatique et les statistiques deviennent des domaines de comp\u00e9tence cl\u00e9s en biologie, simplement parce que cette science, comme d\u2019autres avant (Physique, Astronomie, etc.), devient une \u201cscience de donn\u00e9es\u201d. Or, les biologistes et les m\u00e9decins n\u2019ont g\u00e9n\u00e9ralement pas eu le temps d\u2019acqu\u00e9rir ces comp\u00e9tences durant leurs formations initiales et ces manques tendent \u00e0 se creuser avec l\u2019\u00e9volution rapide des m\u00e9thodes et des technologies. Dans ce contexte, les plateformes de bioinformatique rencontrent une forte demande, en particulier d\u2019\u00e9quipes qui n\u2019ont pas l\u2019opportunit\u00e9 d\u2019int\u00e9grer des bioinformaticiens dans leur effectif. Cependant, cette solution d\u2019analyses sur prestations n\u2019est pas satisfaisante, ni dans la pratique, ni sur le fond. Dans la pratique, \u00e0 moins de multiplier les plateformes d\u2019analyses, ce qui n\u2019est pas possible, celles-ci ne pourront satisfaire une demande en croissance et r\u00e9clamant des comp\u00e9tences toujours plus diverses et sp\u00e9cifiques; il est \u00e0 noter que pour les m\u00eame raisons de moyens, la solution consistant \u00e0 immerger, m\u00eame temporairement, un bioinformaticien dans toute \u00e9quipe de recherche le n\u00e9cessitant n\u2019est pas r\u00e9aliste. Sur le fond, si l\u2019analyse des donn\u00e9es de grands volumes ou de hautes dimensions traduit bien le changement de paradigme entam\u00e9 depuis quelques ann\u00e9es, les biologistes int\u00e9greront t\u00f4t ou tard l\u2019expertise requise dans leur profil et deviendront, en quelque sorte, leurs propres informaticiens ou statisticiens. C\u2019est en effet dans la nature de la recherche scientifique que d\u2019int\u00e9grer les concepts et m\u00e9thodes n\u00e9cessaires \u00e0 ses explorations. Pour acqu\u00e9rir des comp\u00e9tences solides en bioinformatique, les chercheurs peuvent se tourner vers des programmes propos\u00e9s par les universit\u00e9s, le CNRS, l\u2019Inserm, ou des plateformes de bioinformatiques qui maillent le territoire acad\u00e9mique et sont affili\u00e9es \u00e0 l\u2019Institut Fran\u00e7ais de Bioinformatiques (1) . A l\u2019exception de l\u2019offre de formation continue de l\u2019Universit\u00e9 Paris Diderot d\u00e9velopp\u00e9e en partenariat avec l\u2019IFB, ces formations en bioinformatique peuvent \u00eatre class\u00e9es en deux cat\u00e9gories : des introductions g\u00e9n\u00e9ralistes, comme par exemple une initiation au langage de programmation R. Ces formations sont n\u00e9cessaires et adapt\u00e9es au plus grand nombre mais ne permettent pas d\u2019acqu\u00e9rir un niveau suffisant pour aborder r\u00e9ellement des probl\u00e8mes de recherche. des formations avanc\u00e9es \u00e0 des m\u00e9thodologies pointues (par exemple \u201cDNA methylation and bisulfite treatment\u201d, \u201cPhylog\u00e9nie mol\u00e9culaire\u201d, etc), qui s\u2019adressent \u00e0 des chercheurs ayant d\u00e9j\u00e0 des bases solides en bioinformatique, bases que les formations g\u00e9n\u00e9ralistes \u00e9voqu\u00e9es ci-dessus peinent \u00e0 d\u00e9livrer. Ainsi, bien que les formations de qualit\u00e9 soient nombreuses, elles sont morcel\u00e9es th\u00e9matiquement et g\u00e9ographiquement. Pour doter les biologistes et les m\u00e9decins d\u2019une expertise de fond, de nature \u00e0 les rendre autonome dans la fouille de leurs donn\u00e9es, il nous semble important d\u2019 int\u00e9grer les diff\u00e9rentes \u00e9tapes n\u00e9cessaire \u00e0 l\u2019apprentissage de la bioinformatique, en partant des notions de bases jusqu\u2019au m\u00e9thodes sp\u00e9cifiques appropri\u00e9es \u00e0 leurs recherches, et de construire un savoir-faire durable en favorisant la pratique et les situations de recherche r\u00e9elles de fa\u00e7on r\u00e9currente et sur une dur\u00e9e significative.","title":"\u00c9tat de la question"},{"location":"Curamus_project/#justification-de-la-demande","text":"Bien que les Siric aient pour mission de mettre en place des programmes de diffusion des connaissances et des nouvelles pratiques, et ce dans le but d\u2019am\u00e9liorer la recherche translationnelle en canc\u00e9rologie, les biologistes et m\u00e9decins du SIRIC Curamus n\u2019ont pas \u00e0 leur disposition de programme de formations d\u00e9di\u00e9 pour se former \u00e0 l\u2019analyse bioinformatique des donn\u00e9es biologiques. La formation des chercheurs est l\u2019un des objectifs affich\u00e9s d\u00e8s sa cr\u00e9ation \u00e0 l\u2019IBPS en 2015. Pour aborder cet objectif, la plateforme s\u2019appuie sur plusieurs atouts. Son responsable, CA, a conduit auparavant une \u00e9quipe de recherche en g\u00e9n\u00e9tique et conna\u00eet bien les questions qui se posent aux chercheurs en situation d\u2019analyse de grand volume de donn\u00e9es. La plateforme est par ailleurs en contact permanent, \u00e0 travers ses prestations d\u2019analyse, avec les biologistes et m\u00e9decins, ce qui lui permet d\u2019avoir une vision pr\u00e9cise et pratique de leur besoins, de leurs difficult\u00e9s, et de leurs contraintes. ARTbio s\u2019est impliqu\u00e9e dans le SIRIC Curamus depuis sa candidature \u00e0 l\u2019INCA. Elle collabore \u00e9galement activement avec plusieurs \u00e9quipes affili\u00e9es aux SIRIC: L\u2019\u00e9quipe de F. Delhommeau (Leuc\u00e9mie Aigu\u00eb My\u00e9locytaire et ses pr\u00e9dispositions g\u00e9n\u00e9tiques), de M.P. Junnier H. Chneiweiss (Profilage du Glioblastome en cellule uniques (2) ) et de A.G. Marcellin et V. Calvez (recherche de loci de l\u2019herp\u00e8svirus impliqu\u00e9s dans les maladies de Kaposi et de Castleman). Elle conna\u00eet ainsi tr\u00e8s bien les axes th\u00e9matiques de Curamus ainsi que les besoins en bioinformatique qu\u2019ils suscitent. Finalement, ARTbio a d\u00e9j\u00e0 une exp\u00e9rience importante en mati\u00e8re de formation que ce soit dans le cadre de Masters de Sorbonne-Universit\u00e9 (Biologie des G\u00e9nomes et G\u00e9n\u00e9tique de la souris), celui du r\u00e9seau Elixir (analyse g\u00e9nomique sous Galaxy, Ljubljana 2018) ou \u00e0 l\u2019occasion de la journ\u00e9e \u201cGalaxy Spring Day\u201d (3) consacr\u00e9e \u00e0 l\u2019analyse RNAseq et financ\u00e9e par le SIRIC. De plus, la plateforme propose depuis 2019, une programme de compagnonnage, qui est un accompagnement didactique des chercheurs dans un projet de recherche, ayant pour but de r\u00e9aliser leurs analyses tout en leur apprenant les m\u00e9thodologies et proc\u00e9dures mises en oeuvre.","title":"Justification de la demande"},{"location":"Curamus_project/#lensemble-de-ces-points-motive-la-proposition-de-la-plateforme-artbio-pour-le-projet-serial-training-with-artbio-startbio-qui-vise-a-mettre-en-oeuvre-un-programme-de-formation-a-la-bioinformatique-des-chercheurs-du-siric-curamus","text":"adapt\u00e9 \u00e0 leurs besoins d\u2019analyse, bas\u00e9 sur l\u2019enseignement de notions th\u00e9oriques comme sur la pratique construit sur la longueur et la r\u00e9currence des cours pour un meilleurs ancrage de l\u2019apprentissage.","title":"L\u2019ensemble de ces points motive la proposition de la plateforme ARTbio pour le projet \u201cSerial Training with ARTbio\u201d (STARTbio), qui vise \u00e0 mettre en oeuvre un programme de formation \u00e0 la bioinformatique des chercheurs du SIRIC CURAMUS"},{"location":"Curamus_project/#positionnement-des-equipes-postulantes-par-rapport-a-la-competition-internationale","text":"Plusieurs offres de formations sont disponibles \u00e0 l\u2019\u00e9chelle internationale, \u00e9manants par exemple de l\u2019organisation europ\u00e9enne Elixir, ou du projet international Galaxy. Ces initiatives ne peuvent \u00eatre consid\u00e9r\u00e9es comme comp\u00e9titives, bien au contraire. Non seulement, ARTbio contribue directement \u00e0 leur organisation, acqu\u00e9rant ainsi un savoir-faire pr\u00e9cieux, mais elle peut aussi b\u00e9n\u00e9ficier en retour du mat\u00e9riel p\u00e9dagogique tr\u00e8s abondant (Cours et pr\u00e9sentations en ligne, tutoriaux ex\u00e9cutables, listes bibliographiques, r\u00e9seaux d\u2019experts) d\u00e9ploy\u00e9 par la communaut\u00e9 Galaxy. Cette communaut\u00e9 Galaxy offre par exemple, via le Galaxy Training Network (4) plus de 80 tutoriaux qui peuvent \u00eatre directement \u201cex\u00e9cut\u00e9s\u201d en ligne sur des serveurs Galaxy d\u00e9di\u00e9s. Notre serveur Galaxy Mississippi est d\u2019ailleurs l\u2019un des serveurs propos\u00e9s pour suivre certains de ces tutoriaux. Il est aussi \u00e0 noter que la plateforme ARTbio est reconnue au sein du Galaxyproject pour son savoir-faire en mati\u00e8re de d\u00e9ploiement de serveurs d\u2019analyse bioinformatique. Cette position lui permet d\u2019envisager sereinement l\u2019organisation des ressources informatiques n\u00e9cessaire \u00e0 la formation, d\u2019autant plus que ses liens avec la communaut\u00e9 offre l\u2019opportunit\u00e9 de pouvoir disposer d\u2019acc\u00e8s aux grappes de machines de l\u2019universit\u00e9 de Freiburg (usegalaxy.eu) si cela est n\u00e9cessaire. Finalement, la plateforme ARTbio vient d\u2019\u00eatre labellis\u00e9e par l\u2019Institut Fran\u00e7ais de Bioinformatique, l\u2019IFB, qui si elle n\u2019est pas une organisation internationale, contribue de fa\u00e7on majeure \u00e0 structurer les efforts acad\u00e9miques en mati\u00e8re de bioinformatique. Ceci est une opportunit\u00e9 de b\u00e9n\u00e9ficier du savoir-faire de l\u2019IFB en mati\u00e8re d\u2019organisation de formations, ainsi que d\u2019une aide de la communaut\u00e9 fran\u00e7aise des plateformes de bioinformatique.","title":"Positionnement des \u00e9quipes postulantes par rapport \u00e0 la comp\u00e9tition internationale"},{"location":"Curamus_project/#objectifs-et-methodologie-developpee","text":"","title":"Objectifs et m\u00e9thodologie d\u00e9velopp\u00e9e"},{"location":"Curamus_project/#lessentiel-du-programme","text":"","title":"L\u2019essentiel du programme"},{"location":"Curamus_project/#la-formation-couvrira-5-domaines-de-lanalyse-des-donnees-en-cancerologie","text":"Analyse des variants g\u00e9n\u00e9tiques Profilage du transcriptome Profilage des ARN non codants Profilage des cellules uniques M\u00e9tag\u00e9nomique virale. Ces 5 domaines seront abord\u00e9s apr\u00e8s l\u2019enseignement et la pratique d\u2019un socle de notions indispensables: Formation aux m\u00e9thodes d\u2019Int\u00e9gration Continue et de travail collaboratif en informatique Introduction au syst\u00e8me Unix et \u00e0 la manipulation des fichiers Formation au Cloud Computing (informatique sans machine) Introduction et utilisation des bases de donn\u00e9es biologiques, g\u00e9n\u00e9ralistes ou d\u00e9di\u00e9es aux cancers Utilisation de l\u2019environnement R pour les statistiques et la visualisation des donn\u00e9es Introduction \u00e0 l\u2019utilisation du langage de programmation Python Le contenu d\u00e9taill\u00e9 de la formation est pr\u00e9cis\u00e9 en Table 1. Analyse des variants - Responsable: Leonardo Panunzi La formation sera ax\u00e9e sur les m\u00e9thodes et outils pour caract\u00e9riser et annoter les variants g\u00e9nomiques: Single Nucleotide Polymorphism (SNP), Petites insertions ou d\u00e9l\u00e9tions (indels) et variants structurels issus de larges r\u00e9arrangements chromosomiques. Nous montrerons \u00e9galement comment r\u00e9aliser l'interpr\u00e9tation fonctionnelle et \u00e9valuer l\u2019impact des variants identifi\u00e9s \u00e0 l\u2019aide des bases de donn\u00e9es d\u00e9di\u00e9es (Table 1). Nous discuterons les avantages et les inconv\u00e9nients de chaque m\u00e9thode pr\u00e9sent\u00e9e. Les m\u00e9thodes et outils abord\u00e9s lors de la formation sont test\u00e9s et mis en place dans le cadre du projet national CONECT-AML (COllaborative NEtwork on research for Children and Teenagers with Acute Myeloid Leukemia) dans lequel nous sommes impliqu\u00e9s. Profilage du transcriptome - Responsable: Na\u00efra Naouar Nous exposerons aux participants les diff\u00e9rentes \u00e9tapes de l\u2019analyse, les outils \u00e0 utiliser et comment les param\u00e9trer. Nous pr\u00e9senterons les diff\u00e9rents outils d\u2019alignement avec leurs avantages et inconv\u00e9nients (Bowtie, Hisat2, STAR). Nous aborderons diff\u00e9rentes m\u00e9thodes de quantification d\u2019expression (Feature count, HTSeq-count) et leurs probl\u00e9matiques, en particulier avec les fichiers d\u2019annotation g\u00e9nomiques (GTF). Nous r\u00e9aliserons l\u2019analyse de l\u2019expression diff\u00e9rentielle et exposeront les limites statistiques \u00e0 l\u2019analyse : fitting de la variance, lois statistiques et probl\u00e8mes des tests multiples. Nous r\u00e9aliserons des analyses d\u2019enrichissement en se servant des ontologies, telle que la Gene Ontology, et argumenteront sur les limites de ce type d\u2019analyse. Nous caract\u00e9riserons les voies m\u00e9taboliques alt\u00e9r\u00e9es. Finalement, nous confronterons nos listes de g\u00e8nes d\u2019int\u00e9r\u00eat aux bases de donn\u00e9es d\u00e9di\u00e9es \u00e0 la canc\u00e9rologie (TCGA, The Human Protein Atlas, MutaGene, COSMIC, Genomic Data Commons Data Portal) de mani\u00e8re \u00e0 aller plus en avant dans l\u2019interpr\u00e9tation biologique des r\u00e9sultats avec les connaissances disponibles. Profilage des ARN non codants - Responsable: Christophe Antoniewski Nous utiliserons la suite d\u2019outils Mississippi qui a \u00e9t\u00e9 d\u00e9velopp\u00e9e \u00e0 cet effet. Nous apprendrons \u00e0 contr\u00f4ler et \u00e0 pr\u00e9parer des jeux de donn\u00e9es de s\u00e9quen\u00e7age des petits ARN (smallRNAseq), et, \u00e0 annoter les diff\u00e9rentes cat\u00e9gories de petits ARN d\u00e9tect\u00e9es apr\u00e8s alignement sur les g\u00e9nomes (miRNA, rRNA, siRNA, piRNA, snoRNA, snRNA, scaRNA, tRNA-tRF, \u2026). Nous montrerons comment visualiser les structures communes au sein de ces classes d\u2019ARN (tailles, compositions nucl\u00e9otidiques, structures secondaires, signatures g\u00e9nomiques, etc.) et comment regrouper les alignements pour identifier des loci producteurs de petits ARN. Nous aborderons les probl\u00e9matiques d\u2019expression diff\u00e9rentielle (miRNA profiling). Finalement, nous introduirons la probl\u00e9matique des petits ARN issus des s\u00e9quences g\u00e9nomiques r\u00e9p\u00e9t\u00e9es (profiling des transposons et r\u00e9trotransposons) Profilage des cellules uniques - Responsable: L\u00e9a Bellenger Nous nous attacherons aux m\u00e9thodes permettant de caract\u00e9riser l\u2019h\u00e9t\u00e9rog\u00e9n\u00e9it\u00e9 des transcriptomes de cellules canc\u00e9reuses. Pour cela, nous reprendrons les techniques permettant d\u2019effectuer les alignements de s\u00e9quences et comptages d\u2019un tr\u00e8s grand nombre d\u2019\u00e9chantillons gr\u00e2ce \u00e0 des environnements de calcul hautement parall\u00e9lis\u00e9es (Galaxy). Nous pr\u00e9senterons en d\u00e9tail les principales m\u00e9thodes statistiques pour analyser et visualiser les espaces de tr\u00e8s hautes dimensions et introduiront les m\u00e9thodes de r\u00e9duction dimensionnelles (PCA, t-SNE, UMAP) et de classifications (classification hi\u00e9rarchique, k-means, PAM, HCPC, r\u00e9gression logistique) des donn\u00e9es. Nous expliciterons les proc\u00e9dures \u00e0 suivre pour isoler des signatures g\u00e9niques permettant de diff\u00e9rencier les sous-populations cellulaires d\u2019une tumeur. Pour finir, nous aborderons les analyses diff\u00e9rentielles entre populations cellulaires multiples et l\u2019inf\u00e9rence de trajectoires entre ces populations. Metag\u00e9nomique virale - Responsable: Christophe Antoniewski Nous aborderons les m\u00e9thodes de metag\u00e9nomique pour aligner et analyser des s\u00e9quen\u00e7ages d\u2019\u00e9chantillons h\u00f4te/virus. Nous d\u00e9taillerons l\u2019utilisation des outils d\u2019assemblage de novo de s\u00e9quences et nous reprendrons en d\u00e9tail les alignements des s\u00e9quences nucl\u00e9iques et prot\u00e9iques avec les algorithmes blast (blastn, blastx, tblastx). la suite d\u2019outils Metavisitor (5, 6) d\u00e9velopp\u00e9e par ARTbio sera utilis\u00e9e. Cette suite disponible sur Galaxy permet de r\u00e9aliser une analyse compl\u00e8te de d\u00e9tection, d\u2019assemblage, de quantification et d\u2019annotation de l\u2019ensemble des g\u00e9nomes viraux.","title":"La formation couvrira 5 domaines de l\u2019analyse des donn\u00e9es en canc\u00e9rologie :"},{"location":"Curamus_project/#plan-experimental","text":"Le programme de formation STARTbio (Table 1) pourra d\u00e9marrer en mars 2020 et s\u2019\u00e9chelonnera sur environ une ann\u00e9e. Il alliera diff\u00e9rents types de formats p\u00e9dagogiques: Des sessions courtes de 2 \u00e0 4 heures seront anim\u00e9es par les membres d\u2019ARTbio avec l\u2019aide d\u2019intervenants invit\u00e9s affili\u00e9s \u00e0 Curamus et serviront \u00e0 enseigner les bases th\u00e9oriques et pratiques de l\u2019analyse bioinformatique (Table 1). Ces sessions seront r\u00e9alis\u00e9es au sein des campus de Sorbonne-Universit\u00e9, en favorisant la proximit\u00e9 avec les sites de travail des participants du SIRIC (nous serions favorable \u00e0 une localisation dans un campus hospitalier si cela est techniquement r\u00e9alisable). Les sessions courtes allieront des pr\u00e9sentations th\u00e9oriques et des mises en pratiques instantan\u00e9es et individuelles. En effet, chaque participant pour toute la dur\u00e9e du programme, un environnement d\u2019analyse personnel en ligne regroupant un serveur Rstudio, Jupyter et Galaxy. Cinq sessions longues de 2 ou 3 jours sont pr\u00e9vue pour couvrir les 5 th\u00e9matiques principales de la formation. Elles inclueront des expos\u00e9s d\u2019introduction th\u00e9orique et pratique et mettront les participants en situation individuelle d\u2019analyse de jeux de donn\u00e9es r\u00e9cents ayant fait l\u2019objet d\u2019une publication. Ces sessions pourraient \u00eatre film\u00e9es et mises en ligne de mani\u00e8re \u00e0 constituer une base documentaire de tutoriaux en vid\u00e9o. Des tutoriaux ex\u00e9cutables seront pr\u00e9par\u00e9s et mis en ligne afin que les participants puissent revisiter \u00e0 leur rythme des notions ou protocoles d\u2019analyses pr\u00e9sent\u00e9s pendant les sessions courtes ou longues. Ces tutoriaux seront d\u00e9velopp\u00e9s dans les notebooks R-studio et Jupyter, ou \u00e0 l\u2019aide des workflow et des fonctions interactive tours de Galaxy. De nombreux exemples de ce type de tutoriaux peuvent \u00eatre trouv\u00e9s sur le site du Galaxy Training Network. Des exercices flash , \u00e0 ex\u00e9cuter en moins de 10 min sur le mode \u201ccomment faire pour ?\u201d, seront mis \u00e0 disposition des participants dans une documentation en ligne au format \u201creadthedocs\u201d. Ils permettront de constituer une base documentaire de savoir-faire bioinformatique cibl\u00e9 sur les th\u00e9matiques de Curamus et offrirons aux participants \u00e0 la formation l\u2019occasion d\u2019une pratique courte mais quotidienne des techniques d\u2019analyse des donn\u00e9es, qui dans notre exp\u00e9rience est une approche performante pour acqu\u00e9rir un savoir-faire durable. Les principes du FAIR data ( findable, accessible, interoperable, reusable ) seront enseign\u00e9s aux participants et appliqu\u00e9s au mat\u00e9riel p\u00e9dagogique de programme STARTbio : toutes les documentations et \u00e9l\u00e9ments de formation seront librement accessibles sur des d\u00e9p\u00f4ts Github et des serveurs publics Galaxy. Les \u00e9changes entre formateurs et participants seront favoris\u00e9s par l\u2019utilisation d\u2019outils de travail collaboratifs : le gestionnaire de projet Trello, un canal de communication Gitter li\u00e9 aux d\u00e9p\u00f4ts Github de STARbio et des notebooks num\u00e9riques partag\u00e9s. Le programme STARTbio s\u2019adresse \u00e0 des scientifiques confirm\u00e9s, experts dans leurs domaines mais aussi souvent dans d\u2019autres disciplines. Nous favoriserons donc les situations de p\u00e9dagogie invers\u00e9e o\u00f9 les \u00e9l\u00e8ves partagent leur savoir et savoir-faire, et tirerons parti de cette richesse pour instaurer un esprit de travail collaboratif et d'entraide entre tous les participants. Des \u00e9valuations et enqu\u00eates de satisfaction seront r\u00e9alis\u00e9es \u00e0 toutes les \u00e9tapes de la formation pour suivre les progr\u00e8s des participants, identifier les difficult\u00e9s rencontr\u00e9es et prendre en compte les critiques pour adapter les enseignements. Finalement, nous allons prendre contact avec CAPSULE , le Centre d\u2019Accompagnement pour la P\u00e9dagogie et SUpport \u00e0 L\u2019Exp\u00e9rimentation de Sorbonne-Universit\u00e9 afin de pouvoir b\u00e9n\u00e9ficier de leur exp\u00e9rience et de leur soutien logistique.","title":"Plan exp\u00e9rimental"},{"location":"Curamus_project/#resultats-attendus","text":"Le projet STARTbio est une opportunit\u00e9 pour le SIRIC Curamus de mettre en place un programme de diffusion des connaissances et des nouvelles pratiques n\u00e9cessaires au traitement bioinformatique des donn\u00e9e de canc\u00e9rologie. Ax\u00e9 sur une volume d\u2019enseignement important et dot\u00e9 d\u2019outils p\u00e9dagogiques innovant, de former en profondeur les participants aux analyses dont ils ont besoins, tout en les sensibilisant aux principes FAIR adopt\u00e9s \u00e0 l\u2019\u00e9chelle europ\u00e9enne. Les mat\u00e9riaux p\u00e9dagogiques g\u00e9n\u00e9r\u00e9s et accessibles en ligne et la mise en place d\u2019un r\u00e9seau d'entraide faciliteront \u00e0 moyen terme l\u2019autoformation des personnels Les \u00e9valuations des connaissances et les enqu\u00eates effectu\u00e9es aupr\u00e8s des participants permettront \u00e9galement de dresser un typologie plus pr\u00e9cise des besoins en bioinformatique g\u00e9n\u00e9r\u00e9s par les programme de recherche du SIRIC Curamus.","title":"R\u00e9sultats attendus"},{"location":"Curamus_project/#bibliographie","text":"Liste des formations en Bioinformatique \\ https://docs.google.com/spreadsheets/d/1QUiakFq0xpjoVvIM1s92c_fWMgEB2h_Qx7WQ2ELNP4M/edit?usp=sharing . Saurty-Seerunghen,M.S., Bellenger,L., El-Habr,E.A., Delaunay,V., Garnier,D., Chneiweiss,H., Antoniewski,C., Morvan-Dubois,G. and Junier,M.-P. (2019) Capture at the single cell level of metabolic modules distinguishing aggressive and indolent glioblastoma cells. Acta Neuropathol Commun, 7, 155. Galaxy Spring Day. Training material \\ https://artbio.github.io/springday/#galaxy-spring-day-2019-reference-based-rnaseq-analysis . Galaxy Training https://galaxyproject.github.io/training-material/ Galaxy Training Network. Carissimo,G., van den Beek,M., Vernick,K.D. and Antoniewski,C. (2017) Metavisitor, a Suite of Galaxy Tools for Simple and Rapid Detection and Discovery of Viruses in Deep Sequence Data. PLoS One, 12, e0168397. Carissimo,G., Eiglmeier,K., Reveillaud,J., Holm,I., Diallo,M., Diallo,D., Vantaux,A., Kim,S., M\u00e9nard,D., Siv,S., et al. (2016) Identification and Characterization of Two Novel RNA Viruses from Anopheles gambiae Species Complex Mosquitoes. PLoS One, 11, e0153881.","title":"Bibliographie"},{"location":"Curamus_project/#equipes-participantes","text":"Personnes impliqu\u00e9es, statut, et pourcentage de temps consacr\u00e9 \u00e0 la recherche - \u00c9quipes participantes - Principales publications des 5 derni\u00e8res ann\u00e9es Plateforme ARTbio Nom 1\u00e8re ann\u00e9e 2nde ann\u00e9e Responsabilit\u00e9 Christophe Antoniewski DR, CNRS 20% 315h 10% 158h Profilage des ARNs non codants ; M\u00e9tag\u00e9nomique virale Int\u00e9gration Continue Cloud Galaxy Python Na\u00efra Naouar IR, SU 20% 315h 10% 158h Profilage du transcriptome Unix RStudio Pr\u00e9sentation et utilisation des bases de donn\u00e9es Logistique L\u00e9a Bellenger IE, Inserm 15% 236h 10% 158h Profilage des cellules uniques Leonardo G Panunzi, Post-Doc, CNRS 15% 236h 10% 158h Analyse des variants g\u00e9n\u00e9tiques Totaux horaires 1102h 632h Toutes les \u00e9quipes int\u00e9ress\u00e9es de Curamus.","title":"\u00c9quipes participantes"},{"location":"Curamus_project/#principales-publications-des-5-dernieres-annees","text":"Mirca S. Saurty-Seerunghen, L\u00e9a Bellenger , Elias A. El-Habr, Virgile Delaunay, Delphine Garnier, Herv\u00e9 Chneiweiss, Christophe Antoniewski , Ghislaine Morvan-Dubois, Marie-Pierre Junier. 2019. \u201cCapture at the single cell level of metabolic modules distinguishing aggressive and indolent glioblastoma cells\u201d, Acta Neuropathologica Communications. van den Beek M, da Silva B, Pouch J, Ali Chaouche MEA, Carr\u00e9 C, Antoniewski C . 2018. \"Dual-layer transposon repression in heads of Drosophila melanogaster.\" RNA (12):1749-1760 Castel, David, Meryem B. Baghdadi, S\u00e9bastien Mella, Barbara Gayraud-Morel, Virginie Marty, J\u00e9r\u00f4me Cavaill\u00e9, Christophe Antoniewski, and Shahragim Tajbakhsh. 2018. \u201cSmall-RNA Sequencing Identifies Dynamic microRNA Deregulation during Skeletal Muscle Lineage Progression.\u201d Scientific Reports 8 (1): 4208. Carissimo, Guillaume, Marius van den Beek, Kenneth D. Vernick, and Christophe Antoniewski. 2017. \u201cMetavisitor, a Suite of Galaxy Tools for Simple and Rapid Detection and Discovery of Viruses in Deep Sequence Data.\u201d PloS One 12 (1): e0168397. Carissimo, Guillaume, Karin Eiglmeier, Julie Reveillaud, Inge Holm, Mawlouth Diallo, Diawo Diallo, Am\u00e9lie Vantaux, Christophe Antoniewski and Kenneth Vernick. 2016. \u201cIdentification and Characterization of Two Novel RNA Viruses from Anopheles Gambiae Species Complex Mosquitoes.\u201d PloS One 11 (5): e0153881.","title":"Principales publications des 5 derni\u00e8res ann\u00e9es"},{"location":"Curamus_project/#budget","text":"pour le programme STARTbio limit\u00e9 \u00e0 16 participants par \u00e9dition: Co\u00fbt de pr\u00e9paration et de l'ex\u00e9cution de la formation STARTbio Premi\u00e8re ann\u00e9e - Mars 2020 \u00e0 F\u00e9vrier 2021 volume horaire par les membres d\u2019ARTbio : 1102 h co\u00fbt horaire ARTbio : 40 \u20ac Total 1\u00e8re ann\u00e9e : 44 080 \u20ac Deuxi\u00e8me ann\u00e9e - Mars 2021 \u00e0 F\u00e9vrier 2022 volume horaire par les membres d\u2019ARTbio: 632 h co\u00fbt horaire : 40 \u20ac Total 2nde ann\u00e9e: 25 280 \u20ac Total: 69 360 \u20ac Co\u00fbt support\u00e9 par 2 * 16 participants : 29 360 \u20ac soit 920 \u20ac par participant Montant demand\u00e9 au SIRIC Curamus : 40 000\u20ac Nous proposons que les co\u00fbts de la formation soit pris en charge en partie par les participants, soit 920 \u20ac pour une participation au programme, et en partie par le SIRIC, soit un financement de 40 000 \u20ac vers\u00e9s \u00e0 ARTbio.","title":"Budget"},{"location":"Curamus_project/#table-1-programme-detaille-de-la-formation","text":"Volume horaire en pr\u00e9sence d\u2019un encadrant Timeline Volume horaire r\u00e9alis\u00e9 de mani\u00e8re autonome* Int\u00e9gration Continue 8h Mars 10h Cloud 4h Mars 5h Unix 4h Avril 5h Pr\u00e9sentation et Utilisation des bases de donn\u00e9es 8h Avril - Mai 20h Th\u00e9matiques Principales 69h Avril - F\u00e9vrier 10h Partie I : Initiation et Galaxy Training Network 2h Avril - F\u00e9vrier 10h Partie II : Analyse de donn\u00e9es Profilage du transcriptome M\u00e9tag\u00e9nomique virale Profilage des ARN non codants Profilage des cellules uniques Analyse des variants g\u00e9n\u00e9tiques 67h 14h 14h 14h 18h 21h Avril - Janvier Avril Juin Septembre Novembre Janvier - Python 6h Sept. - F\u00e9vrier 20h Partie I : Initiation - Jupyter Notebook 6h Sept. 10h Partie II : Analyse de donn\u00e9es - Sept. - F\u00e9vrier 10h Rstudio 14h Oct. - F\u00e9vrier 50h Partie I : Initiation 14h Oct. - Nov. 10h Partie II : Analyse de donn\u00e9es - Nov. - F\u00e9vrier 40h Bilan 4h F\u00e9vrier - Totaux 117h Mars 19 - F\u00e9vrier 20 120h * Tutoriaux ex\u00e9cutables ou exercices \u201cflash\u201d Contenu d\u00e9taill\u00e9 : Int\u00e9gration Continue Introduction (1h) Partage des donn\u00e9es et transparence - Open access Open source - FAIR data (1h) GitHub : Introduction (2h) - Exercices pratiques* (5h) Travail en \u00e9quipe - Google Apps : Introduction (4h) - Exercices pratiques* (5h) Cloud Principes et outils (2h) Stockage Computing : Introduction (2h) - Exercices pratiques* (5h) Unix Introduction (2h) Expressions r\u00e9guli\u00e8res (2h) Exercices pratiques* (5h) Pr\u00e9sentation et Utilisation des bases de donn\u00e9es G\u00e9n\u00e9ralistes : Introduction (4h) - Exercices pratiques* (10h) Ensembl, NCBI, UCSC D\u00e9di\u00e9es au Cancer : Introduction (4h) - Exercices pratiques* (10h) TCGA, The Human Protein Atlas, MutaGene, COSMIC, Genomic Data Commons Data Portal Th\u00e9matiques principales Partie I : Initiation (2h) Galaxy Training Network* (10h) Th\u00e9matiques principales Partie II : Profilage du transcriptome (14h - Avril ) M\u00e9tag\u00e9nomique virale(14h - Juin ) Profilage des ARN non codants (14h - Septembre ) Profilage des cellules uniques (18h - Novembre ) Analyse des variants g\u00e9n\u00e9tiques (21h - Janvier ) Python Partie I : Initiation (2h) Jupyter Notebook : Introduction (4h) - Exercices pratiques* (10h) Python Partie II : Exercices pratiques* (20h) RStudio Partie I : Initiation (4h) Introduction \u00e0 R Markdown (2h) Tests statistiques (2h) Visualisation des donn\u00e9es (2h) Classification et r\u00e9gressions logistiques (4h) Exercices pratiques* (10h) RStudio Partie II : Profilage du transcriptome* (20h) Profilage du transcriptome en cellule unique* (20h) * enseignements en autonomie sous format \u201ctutoriel ex\u00e9cutable\u201d ou \u201cexercice flash\u201d","title":"Table 1. Programme d\u00e9taill\u00e9 de la formation"},{"location":"bash_snippets/","text":"diff differences of 2 stdout outputs 1 diff (egrep --color regexp pathtofile1 ) (egrep --color regexp pathtofile2 ) egrep 1 egrep --color regexp pathtofile Remove lines before and after some strings in a file remove before somestring, retaining somestring awk '/somestring/,0' pathtofile pathtodeleted_file remove after someotherstring, including someotherstring sed -n '/someotherstring/q;p' pathtofile pathtodeleted_file Check which ports are busy and which ports are free 1 2 3 4 5 sudo netstat -tulpn sudo netstat -antup sudo lsof -i -n -P # You can verify process using port /proc: ls -l /proc/ PID /exe List Open Files For Process First you need to find out PID of process. Simply use any one of the following command to obtain process id: ps aux | grep {program-name} OR ps -C {program-name} -o pid= To list open files for firefox process, enter: ls -l /proc/ PID /fd lsof lsof command list open files under all Linux distributions or UNIX like operating system. Type the following command to list open file for process ID 351: lsof -p 351 Empty the content of a file without closing it 1 truncate -s 0 filename Calculate total used disk space by files older than 180 days using find From stackoverflow , here an example for space occupied by files older than 5 years (1825 days) 1 find . -type f -mtime +1825 -printf %s\\n | awk {a+=$1;} END {printf %.1f GB\\n , a/2**30;} generate a private/public ssh key pair and share it for ssh connection","title":"Terminal Bash"},{"location":"bash_snippets/#diff","text":"","title":"diff"},{"location":"bash_snippets/#differences-of-2-stdout-outputs","text":"1 diff (egrep --color regexp pathtofile1 ) (egrep --color regexp pathtofile2 )","title":"differences of 2 stdout outputs"},{"location":"bash_snippets/#egrep","text":"1 egrep --color regexp pathtofile","title":"egrep"},{"location":"bash_snippets/#remove-lines-before-and-after-some-strings-in-a-file","text":"","title":"Remove lines before and after some strings in a file"},{"location":"bash_snippets/#remove-before-somestring-retaining-somestring","text":"awk '/somestring/,0' pathtofile pathtodeleted_file","title":"remove before somestring, retaining somestring"},{"location":"bash_snippets/#remove-after-someotherstring-including-someotherstring","text":"sed -n '/someotherstring/q;p' pathtofile pathtodeleted_file","title":"remove after someotherstring, including someotherstring"},{"location":"bash_snippets/#check-which-ports-are-busy-and-which-ports-are-free","text":"1 2 3 4 5 sudo netstat -tulpn sudo netstat -antup sudo lsof -i -n -P # You can verify process using port /proc: ls -l /proc/ PID /exe","title":"Check which ports are busy and which ports are free"},{"location":"bash_snippets/#list-open-files-for-process","text":"First you need to find out PID of process. Simply use any one of the following command to obtain process id: ps aux | grep {program-name} OR ps -C {program-name} -o pid= To list open files for firefox process, enter: ls -l /proc/ PID /fd","title":"List Open Files For Process"},{"location":"bash_snippets/#lsof","text":"lsof command list open files under all Linux distributions or UNIX like operating system. Type the following command to list open file for process ID 351: lsof -p 351","title":"lsof"},{"location":"bash_snippets/#empty-the-content-of-a-file-without-closing-it","text":"1 truncate -s 0 filename","title":"Empty the content of a file without closing it"},{"location":"bash_snippets/#calculate-total-used-disk-space-by-files-older-than-180-days-using-find","text":"From stackoverflow , here an example for space occupied by files older than 5 years (1825 days) 1 find . -type f -mtime +1825 -printf %s\\n | awk {a+=$1;} END {printf %.1f GB\\n , a/2**30;}","title":"Calculate total used disk space by files older than 180 days using find"},{"location":"bash_snippets/#generate-a-privatepublic-ssh-key-pair-and-share-it-for-ssh-connection","text":"","title":"generate a private/public ssh key pair and share it for ssh connection"},{"location":"links/","text":"Introduction to WEB ontology","title":"Links"},{"location":"links/#introduction-to-web-ontology","text":"","title":"Introduction to WEB ontology"},{"location":"ten2Regex_01/","text":"Credit: this tutorial is extracted from Regex tutorial \u2014 A quick cheatsheet by examples Regular expressions (regex or regexp) are extremely useful in extracting information from any text by searching for one or more matches of a specific search pattern (i.e. a specific sequence of ASCII or unicode characters). Fields of application range from validation to parsing/replacing strings, passing through translating data to other formats and web scraping. One of the most interesting features is that once you\u2019ve learned the syntax, you can actually use this tool in (almost) all programming languages \u200b\u200b(JavaScript, Java, VB, C #, C / C++, Python, Perl, Ruby, Delphi, R, Tcl, and many others) with the slightest distinctions about the support of the most advanced features and syntax versions supported by the engines). Let\u2019s start by looking at some examples and explanations. Basic topics Anchors \u2014 ^ and $ ^The matches any string that starts with The -> Try it! end$ matches a string that ends with end ^The end$ exact string match (starts and ends with The end ) roar matches any string that has the text roar in it Quantifiers \u2014 * + ? and {} abc * matches a string that has ab followed by zero or more c -> Try it! abc + matches a string that has ab followed by one or more c abc ? matches a string that has ab followed by zero or one c abc {2} matches a string that has ab followed by 2 c abc {2,} matches a string that has ab followed by 2 or more c abc {2,5} matches a string that has ab followed by 2 up to 5 c a (bc)* matches a string that has a followed by zero or more copies of the sequence bc a (bc){2,5} matches a string that has a followed by 2 up to 5 copies of the sequence bc OR operator \u2014 | or [ ] a(b|c) matches a string that has a followed by b or c -> Try it! a[bc] same as previous Character classes \u2014 \\d \\w \\s and . \\d matches a single character that is a digit -> Try it! \\w matches a word character (alphanumeric character plus underscore) -> Try it! \\s matches a whitespace character (includes tabs and line breaks) . matches any character -> Try it! Use the . operator carefully since often class or negated character class (which we\u2019ll cover next) are faster and more precise. \\d , \\w and \\s also present their negations with \\D , \\W and \\S respectively. For example, \\D will perform the inverse match with respect to that obtained with \\d . \\D matches a single non-digit character -> Try it! In order to be taken literally, you must escape the characters ^.[$()|*+?{\\ with a backslash \\ as they have special meaning. \\$\\d matches a string that has a $ before one digit -> Try it! \\$\\d Notice that you can match also non-printable characters like tabs \\t , new-lines \\n , carriage returns \\r . Flags We are learning how to construct a regex but forgetting a fundamental concept: flags . A regex usually comes within this form ** /abc/ , where the search pattern is delimited by two slash characters / . At the end we can specify a flag with these values (we can also combine them each other): g (global) does not return after the first match, restarting the subsequent searches from the end of the previous match m (multi-line) when enabled ^ and $ will match the start and end of a line, instead of the whole string i (insensitive) makes the whole expression case-insensitive (for instance ** /aBc/i would match AbC ) Intermediate topics Grouping and capturing \u2014 () a ( bc ) parentheses create a capturing group with value bc -> Try it! a (?: bc ) * using ?: we disable the capturing group -> Try it! a (? bc ) using ? we put a name to the group -> Try it! This operator is very useful when we need to extract information from strings or data using your preferred programming language. Any multiple occurrences captured by several groups will be exposed in the form of a classical array: we will access their values specifying using an index on the result of the match. If we choose to put a name to the groups (using ( ? ...) ) we will be able to retrieve the group values using the match result like a dictionary where the keys will be the name of each group. Bracket expressions\u200a\u2014\u200a[ ] [abc] matches a string that has either an a or a b or a c -> is the same as a|b|c -> Try it! [a-c] same as previous [a-fA-F0-9] a string that represents a single hexadecimal digit, case insensitively -> Try it! [0-9]% a string that has a character from 0 to 9 before a % sign [^a-zA-Z] a string that has not a letter from a to z or from A to Z. In this case the ^ is used as negation of the expression -> Try it! Remember that inside bracket expressions all special characters (including the backslash \\ ) lose their special powers: thus we will not apply the \u201cescape rule\u201d. Greedy and Lazy match The quantifiers ( * + {} ) are greedy operators, so they expand the match as far as they can through the provided text. For example, .+ matches div simple div /div in This is a strong div simple div /div /strong test . In order to catch only the div tag we can use a ? to make it lazy: .+? matches any character one or more times included inside and , expanding as needed Try it! Notice that a better solution should avoid the usage of . in favor of a more strict regex: [^ ]+ matches any character except or one or more times included inside and - Try it! Advanced topics Boundaries \u2014 \\b and \\B \\babc\\b performs a \"whole words only\" search -> Try it! \\b represents an anchor like caret (it is similar to $ and ^ ) matching positions where one side is a word character (like \\w ) and the other side is not a word character (for instance it may be the beginning of the string or a space character). It comes with its negation , \\B . This matches all positions where \\b doesn\u2019t match and could be if we want to find a search pattern fully surrounded by word characters. \\Babc\\B matches only if the pattern is fully surrounded by word characters -> Try it! Back-references \u2014 \\1 expression effect Try ([abc])\\1 using \\1 it matches the same text that was matched by the first capturing group Try it! ([abc])([de])\\2\\1 we can use \\2 (\\3, \\4, etc.) to identify the same text that was matched by the second (third, fourth, etc.) capturing group Try it! (? foo [abc])\\k foo we put the name foo to the group and we reference it later ( \\k ). The result is the same of the first regex Try it! Look-ahead and Look-behind \u2014 (?=) and (? =) d (?= r ) matches a d only if is followed by r , but r will not be part of the overall regex match -> Try it! (? < =r)d matches a d only if is preceded by an r , but r will not be part of the overall regex match -> Try it! You can use also the negation operator! expression effect Try d(?=r) matches a d only if is not followed by r , but r will not be part of the overall regex match Try it! (? !r)d matches a d only if is not preceded by an r, but r will not be part of the overall regex match Try it! Summary As you\u2019ve seen, the application fields of regex can be multiple and I\u2019m sure that you\u2019ve recognized at least one of these tasks among those seen in your developer career, here a quick list: data validation (for example check if a time string i well-formed) data scraping (especially web scraping, find all pages that contain a certain set of words eventually in a specific order) data wrangling (transform data from \u201craw\u201d to another format) string parsing (for example catch all URL GET parameters, capture text inside a set of parenthesis) string replacement (for example, even during a code session using a common IDE to translate a Java or C# class in the respective JSON object \u2014 replace \u201c;\u201d with \u201c,\u201d make it lowercase, avoid type declaration, etc.) syntax highlightning, file renaming, packet sniffing and many other applications involving strings (where data need not be textual) Have fun and do not forget to recommend the article if you liked it \ud83d\udc9a","title":"Grep and Regex (1)"},{"location":"ten2Regex_01/#credit-this-tutorial-is-extracted-from-regex-tutorial-a-quick-cheatsheet-by-examples","text":"Regular expressions (regex or regexp) are extremely useful in extracting information from any text by searching for one or more matches of a specific search pattern (i.e. a specific sequence of ASCII or unicode characters). Fields of application range from validation to parsing/replacing strings, passing through translating data to other formats and web scraping. One of the most interesting features is that once you\u2019ve learned the syntax, you can actually use this tool in (almost) all programming languages \u200b\u200b(JavaScript, Java, VB, C #, C / C++, Python, Perl, Ruby, Delphi, R, Tcl, and many others) with the slightest distinctions about the support of the most advanced features and syntax versions supported by the engines). Let\u2019s start by looking at some examples and explanations.","title":"Credit: this tutorial is extracted from Regex tutorial \u2014 A quick cheatsheet by examples"},{"location":"ten2Regex_01/#basic-topics","text":"","title":"Basic topics"},{"location":"ten2Regex_01/#anchors-and","text":"^The matches any string that starts with The -> Try it! end$ matches a string that ends with end ^The end$ exact string match (starts and ends with The end ) roar matches any string that has the text roar in it","title":"Anchors \u2014 ^ and $"},{"location":"ten2Regex_01/#quantifiers-and","text":"abc * matches a string that has ab followed by zero or more c -> Try it! abc + matches a string that has ab followed by one or more c abc ? matches a string that has ab followed by zero or one c abc {2} matches a string that has ab followed by 2 c abc {2,} matches a string that has ab followed by 2 or more c abc {2,5} matches a string that has ab followed by 2 up to 5 c a (bc)* matches a string that has a followed by zero or more copies of the sequence bc a (bc){2,5} matches a string that has a followed by 2 up to 5 copies of the sequence bc","title":"Quantifiers \u2014 * + ? and {}"},{"location":"ten2Regex_01/#or-operator-or","text":"a(b|c) matches a string that has a followed by b or c -> Try it! a[bc] same as previous","title":"OR operator \u2014 | or [ ]"},{"location":"ten2Regex_01/#character-classes-d-w-s-and","text":"\\d matches a single character that is a digit -> Try it! \\w matches a word character (alphanumeric character plus underscore) -> Try it! \\s matches a whitespace character (includes tabs and line breaks) . matches any character -> Try it! Use the . operator carefully since often class or negated character class (which we\u2019ll cover next) are faster and more precise. \\d , \\w and \\s also present their negations with \\D , \\W and \\S respectively. For example, \\D will perform the inverse match with respect to that obtained with \\d . \\D matches a single non-digit character -> Try it! In order to be taken literally, you must escape the characters ^.[$()|*+?{\\ with a backslash \\ as they have special meaning. \\$\\d matches a string that has a $ before one digit -> Try it! \\$\\d Notice that you can match also non-printable characters like tabs \\t , new-lines \\n , carriage returns \\r .","title":"Character classes \u2014 \\d \\w \\s and ."},{"location":"ten2Regex_01/#flags","text":"We are learning how to construct a regex but forgetting a fundamental concept: flags . A regex usually comes within this form ** /abc/ , where the search pattern is delimited by two slash characters / . At the end we can specify a flag with these values (we can also combine them each other): g (global) does not return after the first match, restarting the subsequent searches from the end of the previous match m (multi-line) when enabled ^ and $ will match the start and end of a line, instead of the whole string i (insensitive) makes the whole expression case-insensitive (for instance ** /aBc/i would match AbC )","title":"Flags"},{"location":"ten2Regex_01/#intermediate-topics","text":"","title":"Intermediate topics"},{"location":"ten2Regex_01/#grouping-and-capturing","text":"a ( bc ) parentheses create a capturing group with value bc -> Try it! a (?: bc ) * using ?: we disable the capturing group -> Try it! a (? bc ) using ? we put a name to the group -> Try it! This operator is very useful when we need to extract information from strings or data using your preferred programming language. Any multiple occurrences captured by several groups will be exposed in the form of a classical array: we will access their values specifying using an index on the result of the match. If we choose to put a name to the groups (using ( ? ...) ) we will be able to retrieve the group values using the match result like a dictionary where the keys will be the name of each group.","title":"Grouping and capturing \u2014 ()"},{"location":"ten2Regex_01/#bracket-expressions","text":"[abc] matches a string that has either an a or a b or a c -> is the same as a|b|c -> Try it! [a-c] same as previous [a-fA-F0-9] a string that represents a single hexadecimal digit, case insensitively -> Try it! [0-9]% a string that has a character from 0 to 9 before a % sign [^a-zA-Z] a string that has not a letter from a to z or from A to Z. In this case the ^ is used as negation of the expression -> Try it! Remember that inside bracket expressions all special characters (including the backslash \\ ) lose their special powers: thus we will not apply the \u201cescape rule\u201d.","title":"Bracket expressions\u200a\u2014\u200a[ ]"},{"location":"ten2Regex_01/#greedy-and-lazy-match","text":"The quantifiers ( * + {} ) are greedy operators, so they expand the match as far as they can through the provided text. For example, .+ matches div simple div /div in This is a strong div simple div /div /strong test . In order to catch only the div tag we can use a ? to make it lazy: .+? matches any character one or more times included inside and , expanding as needed Try it! Notice that a better solution should avoid the usage of . in favor of a more strict regex: [^ ]+ matches any character except or one or more times included inside and - Try it!","title":"Greedy and Lazy match"},{"location":"ten2Regex_01/#advanced-topics","text":"","title":"Advanced topics"},{"location":"ten2Regex_01/#boundaries-b-and-b","text":"\\babc\\b performs a \"whole words only\" search -> Try it! \\b represents an anchor like caret (it is similar to $ and ^ ) matching positions where one side is a word character (like \\w ) and the other side is not a word character (for instance it may be the beginning of the string or a space character). It comes with its negation , \\B . This matches all positions where \\b doesn\u2019t match and could be if we want to find a search pattern fully surrounded by word characters. \\Babc\\B matches only if the pattern is fully surrounded by word characters -> Try it!","title":"Boundaries \u2014 \\b and \\B"},{"location":"ten2Regex_01/#back-references-1","text":"expression effect Try ([abc])\\1 using \\1 it matches the same text that was matched by the first capturing group Try it! ([abc])([de])\\2\\1 we can use \\2 (\\3, \\4, etc.) to identify the same text that was matched by the second (third, fourth, etc.) capturing group Try it! (? foo [abc])\\k foo we put the name foo to the group and we reference it later ( \\k ). The result is the same of the first regex Try it!","title":"Back-references \u2014 \\1"},{"location":"ten2Regex_01/#look-ahead-and-look-behind-and","text":"d (?= r ) matches a d only if is followed by r , but r will not be part of the overall regex match -> Try it! (? < =r)d matches a d only if is preceded by an r , but r will not be part of the overall regex match -> Try it! You can use also the negation operator! expression effect Try d(?=r) matches a d only if is not followed by r , but r will not be part of the overall regex match Try it! (? !r)d matches a d only if is not preceded by an r, but r will not be part of the overall regex match Try it!","title":"Look-ahead and Look-behind \u2014 (?=) and (?&lt;=)"},{"location":"ten2Regex_01/#summary","text":"As you\u2019ve seen, the application fields of regex can be multiple and I\u2019m sure that you\u2019ve recognized at least one of these tasks among those seen in your developer career, here a quick list: data validation (for example check if a time string i well-formed) data scraping (especially web scraping, find all pages that contain a certain set of words eventually in a specific order) data wrangling (transform data from \u201craw\u201d to another format) string parsing (for example catch all URL GET parameters, capture text inside a set of parenthesis) string replacement (for example, even during a code session using a common IDE to translate a Java or C# class in the respective JSON object \u2014 replace \u201c;\u201d with \u201c,\u201d make it lowercase, avoid type declaration, etc.) syntax highlightning, file renaming, packet sniffing and many other applications involving strings (where data need not be textual) Have fun and do not forget to recommend the article if you liked it \ud83d\udc9a","title":"Summary"},{"location":"Run-Galaxy/","text":"This training documentation is coded in this GitHub repository Why Running Galaxy as an administrator ? You may be wondering: \"Why doing all this geeky IT stuff when I have access to Galaxy servers administrated by professional ?\" It is true that there is a lot of powerful Galaxy instances, and at first, the main Galaxy instance . The expanding list of public galaxy servers is available here . However, a number of issues can be successfully addressed if you are able to administrate your own Galaxy server, including: Storage/Disk Space. Most of Public Galaxy Servers provide their users with a quota that rarely exceed 200-300 Giga-bytes. Although this may seem a lot, it is not unfrequent that analyses that deal with numerous samples require 1 Tera-bytes or more. When you administrate your Galaxy server, you control your storage space. Of course, since nothing in free in this world, keep in mind that you will have to support for the cost of this storage. Isolation. If you control a Galaxy server for a given analysis project and only for this project, you can argue that you benefit from an analysis environment that is isolated. Accessibility and Reproducibility Whenever you need to give access to collaborators or reviewers to your work, giving access to your Galaxy server is enough to provide high-quality transparency and reproducibility. This is far better than just sharing public histories, since when you are not administrator, you do not have access to all computational details that are logged for Galaxy admins. Moreover, if you deploy you Galaxy server in a virtual environment (VM or docker containers) you can preserve the whole environment in an archive and redeploy this environment latter on in another infrastructure. Computational Resources. Galaxy public servers are generally hosted in high performance computing infrastructures whose resources are shared between users. For instance, the main Galaxy server is hosted by a network of US supercomputers . Nevertheless, the computational walltime for a user to execute standards analyses (BWA, bowtie, Tophat, Trinity, etc.) may exceed 5 or 6 hours. Likewise, some metagenomic or de novo assembly approaches may require a substantial amount of memory that is not necessarily provided by public Galaxy server. Full control on installed tools You may need a particular combination of tools for your analysis, and this combination may not be available in any public server. Although Galaxy admin are generally happy to install new tools for their users, other considerations that have to be taken into account in a public resources may limit installation of new tools: \"not considered as harmless for the server\", \"to much resource-demanding for the infrastructure\", \"unable to provide support to the users of this tool\", \"not in the policy of the thematic Galaxy server\", etc. When you administrate your Galaxy server, you can install any tool you need. You can even modify tools, or code your own tools and test these tools in live in your Galaxy instance. Last, but not least, when you are administrator, you have access to information on tool workflow runs you cannot access to when you are regular users (some metadata, including running times, command lines, etc.) Full Control on computational workflows. Galaxy workflows can be exchanged between researchers and between Galaxy instances. However, to be effective, this interoperability requires that the tools called by an imported workflow are installed in the new Galaxy instance. You can only do that if your are administrator of this Galaxy instance. Help your community. Galaxy server administration is a very useful expertise: you can greatly help your colleagues if you are able to run a Galaxy server for them !","title":"Introduction: Why Running Galaxy as an administrator?"},{"location":"Run-Galaxy/#why-running-galaxy-as-an-administrator","text":"You may be wondering: \"Why doing all this geeky IT stuff when I have access to Galaxy servers administrated by professional ?\" It is true that there is a lot of powerful Galaxy instances, and at first, the main Galaxy instance . The expanding list of public galaxy servers is available here . However, a number of issues can be successfully addressed if you are able to administrate your own Galaxy server, including: Storage/Disk Space. Most of Public Galaxy Servers provide their users with a quota that rarely exceed 200-300 Giga-bytes. Although this may seem a lot, it is not unfrequent that analyses that deal with numerous samples require 1 Tera-bytes or more. When you administrate your Galaxy server, you control your storage space. Of course, since nothing in free in this world, keep in mind that you will have to support for the cost of this storage. Isolation. If you control a Galaxy server for a given analysis project and only for this project, you can argue that you benefit from an analysis environment that is isolated. Accessibility and Reproducibility Whenever you need to give access to collaborators or reviewers to your work, giving access to your Galaxy server is enough to provide high-quality transparency and reproducibility. This is far better than just sharing public histories, since when you are not administrator, you do not have access to all computational details that are logged for Galaxy admins. Moreover, if you deploy you Galaxy server in a virtual environment (VM or docker containers) you can preserve the whole environment in an archive and redeploy this environment latter on in another infrastructure. Computational Resources. Galaxy public servers are generally hosted in high performance computing infrastructures whose resources are shared between users. For instance, the main Galaxy server is hosted by a network of US supercomputers . Nevertheless, the computational walltime for a user to execute standards analyses (BWA, bowtie, Tophat, Trinity, etc.) may exceed 5 or 6 hours. Likewise, some metagenomic or de novo assembly approaches may require a substantial amount of memory that is not necessarily provided by public Galaxy server. Full control on installed tools You may need a particular combination of tools for your analysis, and this combination may not be available in any public server. Although Galaxy admin are generally happy to install new tools for their users, other considerations that have to be taken into account in a public resources may limit installation of new tools: \"not considered as harmless for the server\", \"to much resource-demanding for the infrastructure\", \"unable to provide support to the users of this tool\", \"not in the policy of the thematic Galaxy server\", etc. When you administrate your Galaxy server, you can install any tool you need. You can even modify tools, or code your own tools and test these tools in live in your Galaxy instance. Last, but not least, when you are administrator, you have access to information on tool workflow runs you cannot access to when you are regular users (some metadata, including running times, command lines, etc.) Full Control on computational workflows. Galaxy workflows can be exchanged between researchers and between Galaxy instances. However, to be effective, this interoperability requires that the tools called by an imported workflow are installed in the new Galaxy instance. You can only do that if your are administrator of this Galaxy instance. Help your community. Galaxy server administration is a very useful expertise: you can greatly help your colleagues if you are able to run a Galaxy server for them !","title":"Why Running Galaxy as an administrator ?"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/","text":"Installation of a Galaxy server with Docker What is Docker ? Virtual machines Virtual machines (VMs) are an abstraction of physical hardware turning one server into many servers. The hypervisor allows multiple VMs to run on a single machine. Each VM includes a full copy of an operating system, one or more apps, necessary binaries and libraries - taking up tens of GBs. VMs can also be slow to boot. Containers Containers are an abstraction at the app layer that packages code and dependencies together. Multiple containers can run on the same machine and share the OS kernel with other containers, each running as isolated processes in user space. Containers take up less space than VMs (container images are typically tens of MBs in size), and start almost instantly. A GalaxyKickStart Docker Container for the Analyse des g\u00e9nomes Instead of using the GalaxyKickStart playbook in a VM, the playbook can be used to build a Docker container image that will be an almost exact mirror of the GalaxyKickStart VM you have just built. You are not going to do that today (although you should be able to do it by reading the instructions). Instead, you are going to Install the docker system pull the GalaxyKickStart docker container that is deposited in the Docker Hub run this docker container and connect to the deployed GalaxyKickStart server instance Deployment There is actually no need for a new VM, the ansible already installed the docker service in the VM used to deploy GalaxyKickStart. If not already, be connected to you VM as root user using the Google ssh console ( sudo -i ) download the script run_docker_analyse_genomes_2019.sh using the command 1 wget https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/deployment_scripts/run_docker_analyse_genomes_2019.sh run the script using the command 1 sh run_docker_analyse_genomes_2019.sh Connect to your docker-deployed \"GalaxyKickStart\" instance: Just click on the url displayed in your Google Cloud Engine Console and connect using the login:password admin@galaxy.org:admin Shutdown on the docker container and clear disk space go back to your console type: 1 docker ps copy the docker id or the docker container name type the following command while replacing with the copied content 1 docker stop id or name docker rm id or name remove the docker image with the command 1 docker rmi artbio/analyse_genomes:2019 - remove the exported folders by typing 1 rm -rf /galaxy_export /galaxy_tmp Info Following this procedure you will recover about 50 Go of free disk space This is significant ! The run_docker_analyse_genomes_2019.sh script explained run_docker_analyse_genomes_2019.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/usr/bin/env bash # run `bash run_docker_analyse_genomes_2019` set -e echo Now pulling the artbio/analyse_genomes:2019 docker image from DockerHub\\n supervisorctl stop all docker pull artbio/analyse_genomes:2019 echo Running artbio/analyse_genomes:2019 docker container\\n mkdir /galaxy_export /galaxy_tmp chown 1450 :1450 /galaxy_export /galaxy_tmp export DOCKER_INSTANCE = ` docker run -d -p 80 :80 -p 21 :21 -p 8800 :8800 \\ --privileged = true \\ -e GALAXY_CONFIG_ALLOW_USER_DATASET_PURGE = True \\ -e GALAXY_CONFIG_ALLOW_LIBRARY_PATH_PASTE = True \\ -e GALAXY_CONFIG_ENABLE_USER_DELETION = True \\ -e GALAXY_CONFIG_ENABLE_BETA_WORKFLOW_MODULES = True \\ -v /galaxy_tmp:/tmp \\ -v /galaxy_export:/export \\ artbio/analyse_genomes:2019 ` echo The analyse_genomes:2019 docker container is deploying...\\n echo Press Ctrl-C to interrupt this log and start using the container...\\n docker logs -f $DOCKER_INSTANCE The run_docker_analyse_genomes_2019.sh script explained The shebang line. Says that it is a script code and that the interpreter to execute the code is sh and can be found in the /usr/bin/env environment a commented line to explain the script usage set -e says to the bash interpreter to exit the run at first error (to avoid catastrophes) prompts \"Now pulling the galaxykickstart docker image from DockerHub\" stop the galaxy services (galaxy, postgresql, slurm, nginx,...) that were deployed before with ansible (to liberate ports) Pulls (Downloads) the Docker Image specified as parameter to the docker pull statement ( artbio/analyse_genomes:2019 ) reports this action to the terminal creates the /galaxy_export and /galaxy_tmp directory to export automatically data produced by the docker container docker image, and gives write rights to the container for these folders ( chown 1450:1450 ) the command invocation to run the docker container from the docker image artbio/analyse_genomes:2019 . Note the \\ at ends of lines 9 to 17: this character \\ specifies that the code line is continued without line break for the bash interpreter. line 9 starts with an export DOCKER_INSTANCE= instruction. This means that the result of the command between ` after the sign = will be put in the environmental variable DOCKER_INSTANCE , available system-wide. Now, the docker command (between `) itself: Still in line 9, we have docker run -d -p 80:80 -p 21:21 -p 8800:8800 . This means that a container will be run as a deamon ( -d option) and that the internal TCP/IP ports 80 (web interface) and 21 (ftp interface) of the docker instance will be mapped to the ports 80 and 21 of your machin (The VM in this case). Note that in the syntax -p 80:80 , the host port is specified to the left of the : and the docker port is specified to the right of the : . line 10 specifies that the docker container acquires the root privileges line 11 sets the environmental variable GALAXY_CONFIG_ALLOW_USER_DATASET_PURGE passed ( -e xported) to the docker container to the value True line 12 sets the environmental variable GALAXY_CONFIG_ALLOW_LIBRARY_PATH_PASTE to True line 13 sets the environmental variable GALAXY_CONFIG_ENABLE_USER_DELETION to True line 14 sets the environmental variable GALAXY_CONFIG_ENABLE_BETA_WORKFLOW_MODULES to True Note that all these exports in the docker command correspond to advanced boiling/tuning of the docker container. You are not obliged to understand the details to get the container properly running. line 15. -v stands for \"volume\". the -v option says to export the /tmp directory of the docker container to the /galaxy_tmp directory of the host. line 16. we also export the /export directory of the container (any docker container has or should have by default an /export directory) to an /galaxy_export directory of the host (your VM here). Note that if the /galaxy_export directory does not exists at docker run runtime, it will be created. So it is important to understand the -v magics: every directory specified by the -v option will be shared between the docker container filesystem and the host filesystem. It is a mapping operation, so that the same directory is accessible either from inside the docker container or from the host. If you stop and remove the docker container, all exported directory will persist in the host. If you don't do that, all operations performed with a container are lost when you stop this container. line 17. This is the end of the docker run command. The docker image to be instantiated is specified by $1 variable, the parameter passed to the script at runtime. reports to the terminal user wait 90 sec during the docker container deployment reports to the terminal user Now that the docker container is launched, you can access its logs with the command docker logs followed by the identification number of the docker container. We have put this ID in the variable DOCKER_INSTANCE and we access to the content of this variable by prefixing the variable with a $ : docker logs $DOCKER_INSTANCE","title":"Install a Galaxy server with Docker"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#installation-of-a-galaxy-server-with-docker","text":"","title":"Installation of a Galaxy server with Docker"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#what-is-docker","text":"","title":"What is Docker ?"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#virtual-machines","text":"Virtual machines (VMs) are an abstraction of physical hardware turning one server into many servers. The hypervisor allows multiple VMs to run on a single machine. Each VM includes a full copy of an operating system, one or more apps, necessary binaries and libraries - taking up tens of GBs. VMs can also be slow to boot.","title":"Virtual machines"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#containers","text":"Containers are an abstraction at the app layer that packages code and dependencies together. Multiple containers can run on the same machine and share the OS kernel with other containers, each running as isolated processes in user space. Containers take up less space than VMs (container images are typically tens of MBs in size), and start almost instantly.","title":"Containers"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#a-galaxykickstart-docker-container-for-the-analyse-des-genomes","text":"Instead of using the GalaxyKickStart playbook in a VM, the playbook can be used to build a Docker container image that will be an almost exact mirror of the GalaxyKickStart VM you have just built. You are not going to do that today (although you should be able to do it by reading the instructions). Instead, you are going to Install the docker system pull the GalaxyKickStart docker container that is deposited in the Docker Hub run this docker container and connect to the deployed GalaxyKickStart server instance","title":"A GalaxyKickStart Docker Container for the Analyse des g\u00e9nomes"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#deployment","text":"There is actually no need for a new VM, the ansible already installed the docker service in the VM used to deploy GalaxyKickStart. If not already, be connected to you VM as root user using the Google ssh console ( sudo -i ) download the script run_docker_analyse_genomes_2019.sh using the command 1 wget https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/deployment_scripts/run_docker_analyse_genomes_2019.sh run the script using the command 1 sh run_docker_analyse_genomes_2019.sh Connect to your docker-deployed \"GalaxyKickStart\" instance: Just click on the url displayed in your Google Cloud Engine Console and connect using the login:password admin@galaxy.org:admin","title":"Deployment"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#shutdown-on-the-docker-container-and-clear-disk-space","text":"go back to your console type: 1 docker ps copy the docker id or the docker container name type the following command while replacing with the copied content 1 docker stop id or name docker rm id or name remove the docker image with the command 1 docker rmi artbio/analyse_genomes:2019 - remove the exported folders by typing 1 rm -rf /galaxy_export /galaxy_tmp Info Following this procedure you will recover about 50 Go of free disk space This is significant !","title":"Shutdown on the docker container and clear disk space"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#the-run_docker_analyse_genomes_2019sh-script-explained","text":"run_docker_analyse_genomes_2019.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/usr/bin/env bash # run `bash run_docker_analyse_genomes_2019` set -e echo Now pulling the artbio/analyse_genomes:2019 docker image from DockerHub\\n supervisorctl stop all docker pull artbio/analyse_genomes:2019 echo Running artbio/analyse_genomes:2019 docker container\\n mkdir /galaxy_export /galaxy_tmp chown 1450 :1450 /galaxy_export /galaxy_tmp export DOCKER_INSTANCE = ` docker run -d -p 80 :80 -p 21 :21 -p 8800 :8800 \\ --privileged = true \\ -e GALAXY_CONFIG_ALLOW_USER_DATASET_PURGE = True \\ -e GALAXY_CONFIG_ALLOW_LIBRARY_PATH_PASTE = True \\ -e GALAXY_CONFIG_ENABLE_USER_DELETION = True \\ -e GALAXY_CONFIG_ENABLE_BETA_WORKFLOW_MODULES = True \\ -v /galaxy_tmp:/tmp \\ -v /galaxy_export:/export \\ artbio/analyse_genomes:2019 ` echo The analyse_genomes:2019 docker container is deploying...\\n echo Press Ctrl-C to interrupt this log and start using the container...\\n docker logs -f $DOCKER_INSTANCE The run_docker_analyse_genomes_2019.sh script explained The shebang line. Says that it is a script code and that the interpreter to execute the code is sh and can be found in the /usr/bin/env environment a commented line to explain the script usage set -e says to the bash interpreter to exit the run at first error (to avoid catastrophes) prompts \"Now pulling the galaxykickstart docker image from DockerHub\" stop the galaxy services (galaxy, postgresql, slurm, nginx,...) that were deployed before with ansible (to liberate ports) Pulls (Downloads) the Docker Image specified as parameter to the docker pull statement ( artbio/analyse_genomes:2019 ) reports this action to the terminal creates the /galaxy_export and /galaxy_tmp directory to export automatically data produced by the docker container docker image, and gives write rights to the container for these folders ( chown 1450:1450 ) the command invocation to run the docker container from the docker image artbio/analyse_genomes:2019 . Note the \\ at ends of lines 9 to 17: this character \\ specifies that the code line is continued without line break for the bash interpreter. line 9 starts with an export DOCKER_INSTANCE= instruction. This means that the result of the command between ` after the sign = will be put in the environmental variable DOCKER_INSTANCE , available system-wide. Now, the docker command (between `) itself: Still in line 9, we have docker run -d -p 80:80 -p 21:21 -p 8800:8800 . This means that a container will be run as a deamon ( -d option) and that the internal TCP/IP ports 80 (web interface) and 21 (ftp interface) of the docker instance will be mapped to the ports 80 and 21 of your machin (The VM in this case). Note that in the syntax -p 80:80 , the host port is specified to the left of the : and the docker port is specified to the right of the : . line 10 specifies that the docker container acquires the root privileges line 11 sets the environmental variable GALAXY_CONFIG_ALLOW_USER_DATASET_PURGE passed ( -e xported) to the docker container to the value True line 12 sets the environmental variable GALAXY_CONFIG_ALLOW_LIBRARY_PATH_PASTE to True line 13 sets the environmental variable GALAXY_CONFIG_ENABLE_USER_DELETION to True line 14 sets the environmental variable GALAXY_CONFIG_ENABLE_BETA_WORKFLOW_MODULES to True Note that all these exports in the docker command correspond to advanced boiling/tuning of the docker container. You are not obliged to understand the details to get the container properly running. line 15. -v stands for \"volume\". the -v option says to export the /tmp directory of the docker container to the /galaxy_tmp directory of the host. line 16. we also export the /export directory of the container (any docker container has or should have by default an /export directory) to an /galaxy_export directory of the host (your VM here). Note that if the /galaxy_export directory does not exists at docker run runtime, it will be created. So it is important to understand the -v magics: every directory specified by the -v option will be shared between the docker container filesystem and the host filesystem. It is a mapping operation, so that the same directory is accessible either from inside the docker container or from the host. If you stop and remove the docker container, all exported directory will persist in the host. If you don't do that, all operations performed with a container are lost when you stop this container. line 17. This is the end of the docker run command. The docker image to be instantiated is specified by $1 variable, the parameter passed to the script at runtime. reports to the terminal user wait 90 sec during the docker container deployment reports to the terminal user Now that the docker container is launched, you can access its logs with the command docker logs followed by the identification number of the docker container. We have put this ID in the variable DOCKER_INSTANCE and we access to the content of this variable by prefixing the variable with a $ : docker logs $DOCKER_INSTANCE","title":"The run_docker_analyse_genomes_2019.sh script explained"},{"location":"Run-Galaxy/GCE_TP_Galaxy/","text":"Spin off a virtual Machine 1. Go to the Google Cloud Dashboard and select \"Compute Engine\" on the left hand menu bar 2. Select the submenu \"Instances de VM\" 3. Click on the top bar menu the \"CREER UNE INSTANCE\" panel 4. Put name my-galaxy-server , Zone europe-west1-b (or c) , Type de machine 8 vCPU + 30 Go de m\u00e9moire. 5. Disque de D\u00e9marrage: Click on Modifier 6. Select the top menu images personnalis\u00e9es ( custom images ) 7. Click on the rolling menu Afficher les images de and select the My Project - main-sunset-133416 What is important here is the identifier main-sunset-133416 8. Check the button to select galaxy-image-pasteur 9. At the bottom of the same form, choose 100 Go for the Disk Size (Taille). Note that this size should be already selected. Click the S\u00e9lectionner button to leave the selection Disque persistant standard / Standard persistant drive 10. Back to the main form, Click Authorize HTTP traffic / Autoriser le traffic HTTP 11. Click Cr\u00e9er / Create 12. After ~1 minute or so, the VM turns on \"green\" and an ssh menu becomes selectable 13. Click on the http link provided in the Adress IP externe column You should now be able to access to your own Galaxy server instance, but not that this phase can take an additional minute or so, this is the time to start all the galaxy services in the new server instance. 14. Immediately Log in to your server as the administrator And log in with admin@galaxy.org : admin YOU ARE READY TO USE GALAXY !","title":"Appendix 2: Run your personal TP Galaxy server"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#spin-off-a-virtual-machine","text":"","title":"Spin off a virtual Machine"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#1-go-to-the-google-cloud-dashboard-and-select-compute-engine-on-the-left-hand-menu-bar","text":"","title":"1. Go to the Google Cloud Dashboard and select \"Compute Engine\" on the left hand menu bar"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#2-select-the-submenu-instances-de-vm","text":"","title":"2. Select the submenu \"Instances de VM\""},{"location":"Run-Galaxy/GCE_TP_Galaxy/#3-click-on-the-top-bar-menu-the-creer-une-instance-panel","text":"","title":"3. Click on the top bar menu the \"CREER UNE INSTANCE\" panel"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#4-put-name-my-galaxy-server-zone-europe-west1-b-or-c-type-de-machine-8-vcpu-30-go-de-memoire","text":"","title":"4. Put name my-galaxy-server, Zone europe-west1-b (or c), Type de machine 8 vCPU + 30 Go de m\u00e9moire."},{"location":"Run-Galaxy/GCE_TP_Galaxy/#5-disque-de-demarrage-click-on-modifier","text":"","title":"5. Disque de D\u00e9marrage: Click on Modifier"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#6-select-the-top-menu-images-personnalisees-custom-images","text":"","title":"6. Select the top menu images personnalis\u00e9es (custom images)"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#7-click-on-the-rolling-menu-afficher-les-images-de-and-select-the-my-project-main-sunset-133416","text":"What is important here is the identifier main-sunset-133416","title":"7. Click on the rolling menu Afficher les images de and select the My Project - main-sunset-133416"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#8-check-the-button-to-select-galaxy-image-pasteur","text":"","title":"8. Check the button to select galaxy-image-pasteur"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#9-at-the-bottom-of-the-same-form-choose-100-go-for-the-disk-size-taille-note-that-this-size-should-be-already-selected","text":"Click the S\u00e9lectionner button to leave the selection Disque persistant standard / Standard persistant drive","title":"9. At the bottom of the same form, choose 100 Go for the Disk Size (Taille). Note that this size should be already selected."},{"location":"Run-Galaxy/GCE_TP_Galaxy/#10-back-to-the-main-form-click-authorize-http-traffic-autoriser-le-traffic-http","text":"","title":"10. Back to the main form, Click Authorize HTTP traffic / Autoriser le traffic HTTP"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#11-click-creer-create","text":"","title":"11. Click Cr\u00e9er / Create"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#12-after-1-minute-or-so-the-vm-turns-on-green-and-an-ssh-menu-becomes-selectable","text":"","title":"12. After ~1 minute or so, the VM turns on \"green\" and an ssh menu becomes selectable"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#13-click-on-the-http-link-provided-in-the-adress-ip-externe-column","text":"You should now be able to access to your own Galaxy server instance, but not that this phase can take an additional minute or so, this is the time to start all the galaxy services in the new server instance.","title":"13. Click on the http link provided in the Adress IP externe column"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#14-immediately-log-in-to-your-server-as-the-administrator","text":"And log in with admin@galaxy.org : admin","title":"14. Immediately Log in to your server as the administrator"},{"location":"Run-Galaxy/GCE_TP_Galaxy/#you-are-ready-to-use-galaxy","text":"","title":"YOU ARE READY TO USE GALAXY !"},{"location":"Run-Galaxy/GalaxyKickStart/","text":"Installation of a Galaxy server with Ansible and the GalaxyKickStart playbook What is Ansible ? Ansible is an automation engine that automates configuration management and application deployment. Ansible reads instructions (Tasks) from a playbook and performs the indicated tasks on target machines (Hosts), through an ssh connection. There is no magics: everything an \"administrator\" can do using command lines of a linux OS, can be automated with ansible that \"wraps\" these command lines. The power of Ansible (and similar orchestration software, ie Puppet, Chief, etc.) comes from the abstraction of complex suite of commands in the Ansible syntax. Moreover, automation allows to reproduce exactly the desired configuration. Finally, Ansible is idempotent : whatever the initial configuration, it brings the target to the exact same final state. This is useful to repair a broken configuration. Ansible playbook - GalaxyKickStart The Ansible \"language\" (Striclty speaking, Ansible language is not a programming language) is structured. Thus a playbook is not necessarily a single flat file. Multiple tasks can be gathered in a file, a \"role\" is the execution of a set of tasks, and a playbook can execute multiple roles. GalaxyKickStart is an Ansible playbook that will install basic dependencies needed for Galaxy Create and manage all the linux users involved in the deployment of Galaxy Install and configure the services required for Galaxy: postgresql (database engine) nginx (web server) docker (containers) proftpd (ftp server) slurm (job manager) supervisor (service manager) Configure Galaxy for using these services Install tools and workflows using the bioblend API. The code of the GalaxyKickStart playbook is freely available at the ARTbio GitHub Repository https://github.com/ARTbio/GalaxyKickStart . Deployment start a GCE VM Google Instance S\u00e9rie N2 n2-standard-16 (16 processeurs virtuels, 64 Go de m\u00e9moire) Disque de d\u00e9marrage Ubuntu 16.04 LTS Taille (Go) 200 Pare-feu Autoriser le trafic HTTP connect to you VM using the Google ssh console start an interactive session as root using the command 1 sudo -i download the script run_ansible_analyse_genomes_2019.sh using the command 1 wget https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/deployment_scripts/run_ansible_analyse_genomes_2019.sh We are now ready to run this script. However this year there is bonus ! All trainees will participate to Pasteur 2019 Ansible Racing . In order to participate, you'll just have to put the time command just before the script invokation, as follows: 1 time sh run_ansible_analyse_genomes_2019.sh analyseGenomes_2019 The Ultimate Pasteur 2019 Ansible Racing Please copy the time info returned by your console at the end of the deploymment. It shoud look like this: 1 2 3 real 10m27.142s user 8m22.941s sys 1m16.409s Then Paste this time as a comment in this GitHub issue When the deployment is finished, connect to your ansible-deployed \"GalaxyKickStart\" instance: Just click on the url displayed in your Google Cloud Engine Console. Connect to your server as an admin: This time, ansible and the GalaxyKickStart playbook already programmatically registered an admin user. Just use the admin@galaxy.org:admin as credentials (user:password) When logged in, see that required tools as well as workflows are already installed ! Warning admin is not really a decent password, please c h a n g e . y o u r . p a s s w o r d to avoid your Galaxy server getting hacked before the end of the course. Galaxy administration tasks Transfert input data to you newly deployed Galaxy instance that is : - a data set with reference sequences - a data set with small RNAseq files - a data set with RNAseq files Click the main menu User Saved Histories Press the top right button (above history list) Import from file copy this url : 1 https://galaxy.pasteur.fr/history/export_archive?id=4c5da5ad7355ff42 repeat the same operation with: 1 https://galaxy.pasteur.fr/history/export_archive?id=eb4c1d5564c9f78c and 1 https://galaxy.pasteur.fr/history/export_archive?id=69a1b70d1c4a6bdb In case of emergency 1 https://00e9e64bacc86f2151a14558fe5cde860787db0c2c54b36c1a-apidata.googleusercontent.com/download/storage/v1/b/artbio_genomic_analysis/o/export_archive%3Fid=4c5da5ad7355ff42?qk=AD5uMEtvNjCQjALPCHx2V2xEEgOELrj8N7H0HMXCrsm1eeKXAkG8UIO61njE8WdWHo7XzcDTL3vCm0buCY9AUa4rIF-a4Sc4FlWVafXUjLoHiJj1YGmWoRFPdaf-p3R0pCF_VROCXjKV7ymBlPJsfZFA8LVxriefgb2-2l9221SXmzBD2lUsgOM5hZ2peTXHGg3DPTFYJGg7eEAIaVJJ4EjjIMsbXXAh6g0gSQuw5qqO36HnZUE2hLnrajHhyVrT4JqXOobUYEZ1oeC2DD3Jv_1eaW3C58lTdtQPv9EZn5boJMzjL41Q0pKkOvIYRPWb6ymoKz5INAWExtWcAKD4hmy00MhQMha0paPbmtIxxwh4lJWrkBgbg51r5AZhpJeQBVES8QXxboUlongYUO8VsNr4rsZLtcG6q4jduyRG2VmzoYkXRoPVZYKnZPp9ZUk8xjk8BhnKP0hBsO9MM8LKtqM4ds-4xF58q734nd1zQl4jKeRYk2ERwnC61Z4G5Rc-WQpgQ7NdsJqjC0tA5evX4eMonQH3xW1Ij2M-o05miErydA7dy8XzDu_tcvjFGxVNk-2JeDdp5UK7FqnZ271ojL_H_tfN40NtDAxX1yDyZ3RIAGM9Z937qssTqCjjj0OFLJK9LSntR_fImQCSAeRk1YKnVJKMs01gMIf_6Gz8tfj9tS6qjdD9H1HQ3_tGTE1cj_egeSKnVE36BqB4Nj54Hb27VIj0-EvZhx4yNGnXVhHGLNqOH5UbyAA2INr44i7nRUMSkaRFQwnBfdn7_fLPJdWgN-_b3wE3eBDG_CUN3oAV7Mjx0P6xxUeBsqAD4ugVUERHh-McR98_-0FBezzEE3EIJSLWUp5GOg 1 https://00e9e64baca628f8986257c92fd678925eff3fb11ed25ba2b3-apidata.googleusercontent.com/download/storage/v1/b/artbio_genomic_analysis/o/export_archive%3Fid=eb4c1d5564c9f78c?qk=AD5uMEspDJlepCT36Fk5XUY2i-BUI0uhBg7mqTL5kNfw23Po2uQRUsJFuLXd5JTCaDxmioB3mph-QvZJO9CrW9mdHW3Yu-yym0I6ZkNxiprIiVwg3samXY9EGm-NwFG9St0c16Fdu9tnjzck3RPqq3uKpFgKPT62hD4r0e7qsDjNt2Quy0Wy0hKdJhOTstdvV0jBZ91Ve6-0ZaP9uY_-vXnMXK0-CYkoUnyJ_i8Nr8reo7UBQ6ywpVyOqLSrfTTe-p7UK1v1OV-TZkq21bKm0BtskJy5ZF_PtaDV5FrXzlVuxJJsjusR5k0j2nWtfX8wQIOhFdsPHkLR3U9u4-93d50K1gAx2HUz7C4ZoS3cnZvdIjvHRWWq7TPIzXhPDVEVY2ETA9Qlte7qcbrVwxGMGnpQIcoocdHWE8MBQd3KKuFP5CU0LH4wR_psJiDUWeeUQp7l91Aptb8rDlWXzMBYzibxqG5nfuVFY6w_3WnGnWzjAtF81fAYApgShNG5dHqrulYITHSzVmoW9-bGRU9S2TfFILM0eGgcck7ryY1WCYaf5D0DScmLi2Ry-7vnqAzBryVVQ6ibereAOl88ZsIb-e7MOZvJISxma3QDA42eY5zK39RbI-9hzzd75g8c7m3n_vHnjao7wsBq80ky5LcJlzdKmYoAf4iG3yHzKH5pAb_J4YlKu14r5coPANk_kludWrqNo8nd-yGAvrxh2psfdw4atGbbBzB9CUZmRmX2-xYSL4ZSSsTovsqORJCQT5SXafeMTWn5n9wR4-JJrZ_FmW_db9v1AZJLmdzCyAgen6bzuFfYIUVapLvupfUXNzzJ5ybPOh0Zk5sl8Aa_EN3JZdAUZNc-yeTy6g 1 https://00e9e64bac1f24fd127c07b2755b873390c106ba537ea32aed-apidata.googleusercontent.com/download/storage/v1/b/artbio_genomic_analysis/o/export_archive%3Fid=69a1b70d1c4a6bdb?qk=AD5uMEvpKg6l79WadDI7pXbIp7WoH3Sh8LVqu1s7IAhvofa5sf2M_Ab5XXO2zb1eyiXREekqHvIJnnhjPmkbos1HespE82fUoJlBND5192zO_nXVtNTKIa_QxcVh4R9fdAVp-wq8jGffS-rYSiFU1qtqJJnAabXZqJw3jmENXDSo-h4LUckbFVUQPypSDt7P8U1V4bLHozaiiJGrEo88Y5LHXykoMWsouMGch-bUBJb-AsU_2u215VALH0qFIG3VByfljeF-FyVGkoeBCYgHk1VX0CR9AlwIm9pvhZ6Jq70nQNUHPmNH5lKC3Lu3VCWly4rjyIeEtPU1mbyaz-US0GrHVOr7twI0bCL9gH-PleKCTSrbciqOyD6zxa42TcnZakmPGdaRwB6JjqHw1ZwLakVrerLkupAaB6He1_R_RvWQkgZYY51EKMgFraobdNC5mTpzCblq2NuxGjzCwhhWZBbJrbt1OltXE5sS2B5wU93fX9VTnuYLS9L02fcoTrMP_YVfM2opW1l23A0vu8QnlkV0zndjk0cYDFrKxTMTM6ziCmzNbbcnwKCWiJJYVMTQNGn6dyCiE4T5y5ZmsqvEoARJ8-77BLd6-Nc2_cVSv0FYszDP4AK3lencR5fkuP185lxv2MuAXeI4gZDJlUbdKNS6RYAAJ06bZ-hHyJvcT_Oh-c6-SNceDQmYh4Ugx_VhTiYYrodUPQ2Or9ldWIdzUW2v-ICF4Qc28joHZpz6vYNsEQsRertuQiHSD7yhV-XstmkE6sA2eL8LpziLQ-QXgl5N1CtepqOdf6ARKm8O4botzX3IO4HPZAtWFvFe3usqss-ccrn_Bh3Xb7hdPnBcZ6A0Vt3ZVLwh0Q Content of the run_ansible_analyse_genomes_2019.sh script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/usr/bin/env bash set -e apt update -y apt install -y python-pip python-dev python-setuptools git htop echo Upgrading pip pip install -U pip /usr/local/bin/pip --version /usr/local/bin/pip install ansible == 2 .7.4 ansible --version git clone https://github.com/ARTbio/GalaxyKickStart.git -b $1 cd GalaxyKickStart/ ansible-galaxy install -r requirements_roles.yml -p roles/ -f cp scripts/8cpu_job_conf.xml roles/galaxyprojectdotorg.galaxy-extras/templates/job_conf.xml.j2 cp scripts/configure_slurm.py.j2 roles/galaxyprojectdotorg.galaxy-extras/templates/configure_slurm.py.j2 ansible-playbook -i inventory_files/analyseGenomes galaxy.yml echo end of deployment\\n the run_ansible_analyse_genomes_2019.sh script explained The shebang line ( #! ) says that it is a script code that has to be executed by the shell bash which can be found in the /usr/bin/env environment set -e says to the bash interpreter to exit the run at first error (to avoid catastrophes) update apt package database installs python-pip , python-dev , python-setuptools (these 3 packages are required to install pip), git (to clone and manage GitHub repositories) and htop (a monitoring tool) using the package installer apt Is just a command to inform the user about run state. This will prompt \"Upgrading pip version\" in the console does what is echoed before by the previous line : this is the command to upgrade the pip program that was installed with installation of python-pip . pip is a recursive acronym that can stand for either \"Pip Installs Packages\" or \"Pip Installs Python\". will prompt the version of pip in the console install ansible , version 2.7.4, using pip will prompt the version of ansible in the console clone the GalaxyKickStart Repository available at https://github.com/ARTbio/GalaxyKickStart.git , creating locally the GalaxyKickStart folder. The repository branch that is cloned is indicated as a parameter in the command line (the $1 ). Changes directory, i.e. goes to /root/GalaxyKickStart Says to ansible to install additional roles (collection of files to control ansible) which are not the the GalaxyKickStart repository but whose address is stated in the file requirements_roles.yml . These roles will be installed in the subdirectory /root/GalaxyKickStart/roles/ . NB: ansible-galaxy has nothing to do with Galaxy, the name of this ansible command is serendipitous. triggers the play of the playbook galaxy.yml by ansible. The target host of the playbook is defined in the file inventory_files/analyseGenomes , as well as how ansible will interact with the target. Here, we play the playbook on the same computer (localhost). Prompts the end of the deployment","title":"Install a Galaxy server with Ansible and GalaxyKickStart"},{"location":"Run-Galaxy/GalaxyKickStart/#installation-of-a-galaxy-server-with-ansible-and-the-galaxykickstart-playbook","text":"","title":"Installation of a Galaxy server with Ansible and the GalaxyKickStart playbook"},{"location":"Run-Galaxy/GalaxyKickStart/#what-is-ansible","text":"Ansible is an automation engine that automates configuration management and application deployment. Ansible reads instructions (Tasks) from a playbook and performs the indicated tasks on target machines (Hosts), through an ssh connection. There is no magics: everything an \"administrator\" can do using command lines of a linux OS, can be automated with ansible that \"wraps\" these command lines. The power of Ansible (and similar orchestration software, ie Puppet, Chief, etc.) comes from the abstraction of complex suite of commands in the Ansible syntax. Moreover, automation allows to reproduce exactly the desired configuration. Finally, Ansible is idempotent : whatever the initial configuration, it brings the target to the exact same final state. This is useful to repair a broken configuration.","title":"What is Ansible ?"},{"location":"Run-Galaxy/GalaxyKickStart/#ansible-playbook-galaxykickstart","text":"The Ansible \"language\" (Striclty speaking, Ansible language is not a programming language) is structured. Thus a playbook is not necessarily a single flat file. Multiple tasks can be gathered in a file, a \"role\" is the execution of a set of tasks, and a playbook can execute multiple roles. GalaxyKickStart is an Ansible playbook that will install basic dependencies needed for Galaxy Create and manage all the linux users involved in the deployment of Galaxy Install and configure the services required for Galaxy: postgresql (database engine) nginx (web server) docker (containers) proftpd (ftp server) slurm (job manager) supervisor (service manager) Configure Galaxy for using these services Install tools and workflows using the bioblend API. The code of the GalaxyKickStart playbook is freely available at the ARTbio GitHub Repository https://github.com/ARTbio/GalaxyKickStart .","title":"Ansible playbook - GalaxyKickStart"},{"location":"Run-Galaxy/GalaxyKickStart/#deployment","text":"start a GCE VM Google Instance S\u00e9rie N2 n2-standard-16 (16 processeurs virtuels, 64 Go de m\u00e9moire) Disque de d\u00e9marrage Ubuntu 16.04 LTS Taille (Go) 200 Pare-feu Autoriser le trafic HTTP connect to you VM using the Google ssh console start an interactive session as root using the command 1 sudo -i download the script run_ansible_analyse_genomes_2019.sh using the command 1 wget https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/deployment_scripts/run_ansible_analyse_genomes_2019.sh We are now ready to run this script. However this year there is bonus ! All trainees will participate to Pasteur 2019 Ansible Racing . In order to participate, you'll just have to put the time command just before the script invokation, as follows: 1 time sh run_ansible_analyse_genomes_2019.sh analyseGenomes_2019 The Ultimate Pasteur 2019 Ansible Racing Please copy the time info returned by your console at the end of the deploymment. It shoud look like this: 1 2 3 real 10m27.142s user 8m22.941s sys 1m16.409s Then Paste this time as a comment in this GitHub issue When the deployment is finished, connect to your ansible-deployed \"GalaxyKickStart\" instance: Just click on the url displayed in your Google Cloud Engine Console. Connect to your server as an admin: This time, ansible and the GalaxyKickStart playbook already programmatically registered an admin user. Just use the admin@galaxy.org:admin as credentials (user:password) When logged in, see that required tools as well as workflows are already installed ! Warning admin is not really a decent password, please c h a n g e . y o u r . p a s s w o r d to avoid your Galaxy server getting hacked before the end of the course.","title":"Deployment"},{"location":"Run-Galaxy/GalaxyKickStart/#galaxy-administration-tasks","text":"","title":"Galaxy administration tasks"},{"location":"Run-Galaxy/GalaxyKickStart/#transfert-input-data-to-you-newly-deployed-galaxy-instance","text":"that is : - a data set with reference sequences - a data set with small RNAseq files - a data set with RNAseq files Click the main menu User Saved Histories Press the top right button (above history list) Import from file copy this url : 1 https://galaxy.pasteur.fr/history/export_archive?id=4c5da5ad7355ff42 repeat the same operation with: 1 https://galaxy.pasteur.fr/history/export_archive?id=eb4c1d5564c9f78c and 1 https://galaxy.pasteur.fr/history/export_archive?id=69a1b70d1c4a6bdb In case of emergency 1 https://00e9e64bacc86f2151a14558fe5cde860787db0c2c54b36c1a-apidata.googleusercontent.com/download/storage/v1/b/artbio_genomic_analysis/o/export_archive%3Fid=4c5da5ad7355ff42?qk=AD5uMEtvNjCQjALPCHx2V2xEEgOELrj8N7H0HMXCrsm1eeKXAkG8UIO61njE8WdWHo7XzcDTL3vCm0buCY9AUa4rIF-a4Sc4FlWVafXUjLoHiJj1YGmWoRFPdaf-p3R0pCF_VROCXjKV7ymBlPJsfZFA8LVxriefgb2-2l9221SXmzBD2lUsgOM5hZ2peTXHGg3DPTFYJGg7eEAIaVJJ4EjjIMsbXXAh6g0gSQuw5qqO36HnZUE2hLnrajHhyVrT4JqXOobUYEZ1oeC2DD3Jv_1eaW3C58lTdtQPv9EZn5boJMzjL41Q0pKkOvIYRPWb6ymoKz5INAWExtWcAKD4hmy00MhQMha0paPbmtIxxwh4lJWrkBgbg51r5AZhpJeQBVES8QXxboUlongYUO8VsNr4rsZLtcG6q4jduyRG2VmzoYkXRoPVZYKnZPp9ZUk8xjk8BhnKP0hBsO9MM8LKtqM4ds-4xF58q734nd1zQl4jKeRYk2ERwnC61Z4G5Rc-WQpgQ7NdsJqjC0tA5evX4eMonQH3xW1Ij2M-o05miErydA7dy8XzDu_tcvjFGxVNk-2JeDdp5UK7FqnZ271ojL_H_tfN40NtDAxX1yDyZ3RIAGM9Z937qssTqCjjj0OFLJK9LSntR_fImQCSAeRk1YKnVJKMs01gMIf_6Gz8tfj9tS6qjdD9H1HQ3_tGTE1cj_egeSKnVE36BqB4Nj54Hb27VIj0-EvZhx4yNGnXVhHGLNqOH5UbyAA2INr44i7nRUMSkaRFQwnBfdn7_fLPJdWgN-_b3wE3eBDG_CUN3oAV7Mjx0P6xxUeBsqAD4ugVUERHh-McR98_-0FBezzEE3EIJSLWUp5GOg 1 https://00e9e64baca628f8986257c92fd678925eff3fb11ed25ba2b3-apidata.googleusercontent.com/download/storage/v1/b/artbio_genomic_analysis/o/export_archive%3Fid=eb4c1d5564c9f78c?qk=AD5uMEspDJlepCT36Fk5XUY2i-BUI0uhBg7mqTL5kNfw23Po2uQRUsJFuLXd5JTCaDxmioB3mph-QvZJO9CrW9mdHW3Yu-yym0I6ZkNxiprIiVwg3samXY9EGm-NwFG9St0c16Fdu9tnjzck3RPqq3uKpFgKPT62hD4r0e7qsDjNt2Quy0Wy0hKdJhOTstdvV0jBZ91Ve6-0ZaP9uY_-vXnMXK0-CYkoUnyJ_i8Nr8reo7UBQ6ywpVyOqLSrfTTe-p7UK1v1OV-TZkq21bKm0BtskJy5ZF_PtaDV5FrXzlVuxJJsjusR5k0j2nWtfX8wQIOhFdsPHkLR3U9u4-93d50K1gAx2HUz7C4ZoS3cnZvdIjvHRWWq7TPIzXhPDVEVY2ETA9Qlte7qcbrVwxGMGnpQIcoocdHWE8MBQd3KKuFP5CU0LH4wR_psJiDUWeeUQp7l91Aptb8rDlWXzMBYzibxqG5nfuVFY6w_3WnGnWzjAtF81fAYApgShNG5dHqrulYITHSzVmoW9-bGRU9S2TfFILM0eGgcck7ryY1WCYaf5D0DScmLi2Ry-7vnqAzBryVVQ6ibereAOl88ZsIb-e7MOZvJISxma3QDA42eY5zK39RbI-9hzzd75g8c7m3n_vHnjao7wsBq80ky5LcJlzdKmYoAf4iG3yHzKH5pAb_J4YlKu14r5coPANk_kludWrqNo8nd-yGAvrxh2psfdw4atGbbBzB9CUZmRmX2-xYSL4ZSSsTovsqORJCQT5SXafeMTWn5n9wR4-JJrZ_FmW_db9v1AZJLmdzCyAgen6bzuFfYIUVapLvupfUXNzzJ5ybPOh0Zk5sl8Aa_EN3JZdAUZNc-yeTy6g 1 https://00e9e64bac1f24fd127c07b2755b873390c106ba537ea32aed-apidata.googleusercontent.com/download/storage/v1/b/artbio_genomic_analysis/o/export_archive%3Fid=69a1b70d1c4a6bdb?qk=AD5uMEvpKg6l79WadDI7pXbIp7WoH3Sh8LVqu1s7IAhvofa5sf2M_Ab5XXO2zb1eyiXREekqHvIJnnhjPmkbos1HespE82fUoJlBND5192zO_nXVtNTKIa_QxcVh4R9fdAVp-wq8jGffS-rYSiFU1qtqJJnAabXZqJw3jmENXDSo-h4LUckbFVUQPypSDt7P8U1V4bLHozaiiJGrEo88Y5LHXykoMWsouMGch-bUBJb-AsU_2u215VALH0qFIG3VByfljeF-FyVGkoeBCYgHk1VX0CR9AlwIm9pvhZ6Jq70nQNUHPmNH5lKC3Lu3VCWly4rjyIeEtPU1mbyaz-US0GrHVOr7twI0bCL9gH-PleKCTSrbciqOyD6zxa42TcnZakmPGdaRwB6JjqHw1ZwLakVrerLkupAaB6He1_R_RvWQkgZYY51EKMgFraobdNC5mTpzCblq2NuxGjzCwhhWZBbJrbt1OltXE5sS2B5wU93fX9VTnuYLS9L02fcoTrMP_YVfM2opW1l23A0vu8QnlkV0zndjk0cYDFrKxTMTM6ziCmzNbbcnwKCWiJJYVMTQNGn6dyCiE4T5y5ZmsqvEoARJ8-77BLd6-Nc2_cVSv0FYszDP4AK3lencR5fkuP185lxv2MuAXeI4gZDJlUbdKNS6RYAAJ06bZ-hHyJvcT_Oh-c6-SNceDQmYh4Ugx_VhTiYYrodUPQ2Or9ldWIdzUW2v-ICF4Qc28joHZpz6vYNsEQsRertuQiHSD7yhV-XstmkE6sA2eL8LpziLQ-QXgl5N1CtepqOdf6ARKm8O4botzX3IO4HPZAtWFvFe3usqss-ccrn_Bh3Xb7hdPnBcZ6A0Vt3ZVLwh0Q","title":"Transfert input data to you newly deployed Galaxy instance"},{"location":"Run-Galaxy/GalaxyKickStart/#content-of-the-run_ansible_analyse_genomes_2019sh-script","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/usr/bin/env bash set -e apt update -y apt install -y python-pip python-dev python-setuptools git htop echo Upgrading pip pip install -U pip /usr/local/bin/pip --version /usr/local/bin/pip install ansible == 2 .7.4 ansible --version git clone https://github.com/ARTbio/GalaxyKickStart.git -b $1 cd GalaxyKickStart/ ansible-galaxy install -r requirements_roles.yml -p roles/ -f cp scripts/8cpu_job_conf.xml roles/galaxyprojectdotorg.galaxy-extras/templates/job_conf.xml.j2 cp scripts/configure_slurm.py.j2 roles/galaxyprojectdotorg.galaxy-extras/templates/configure_slurm.py.j2 ansible-playbook -i inventory_files/analyseGenomes galaxy.yml echo end of deployment\\n the run_ansible_analyse_genomes_2019.sh script explained The shebang line ( #! ) says that it is a script code that has to be executed by the shell bash which can be found in the /usr/bin/env environment set -e says to the bash interpreter to exit the run at first error (to avoid catastrophes) update apt package database installs python-pip , python-dev , python-setuptools (these 3 packages are required to install pip), git (to clone and manage GitHub repositories) and htop (a monitoring tool) using the package installer apt Is just a command to inform the user about run state. This will prompt \"Upgrading pip version\" in the console does what is echoed before by the previous line : this is the command to upgrade the pip program that was installed with installation of python-pip . pip is a recursive acronym that can stand for either \"Pip Installs Packages\" or \"Pip Installs Python\". will prompt the version of pip in the console install ansible , version 2.7.4, using pip will prompt the version of ansible in the console clone the GalaxyKickStart Repository available at https://github.com/ARTbio/GalaxyKickStart.git , creating locally the GalaxyKickStart folder. The repository branch that is cloned is indicated as a parameter in the command line (the $1 ). Changes directory, i.e. goes to /root/GalaxyKickStart Says to ansible to install additional roles (collection of files to control ansible) which are not the the GalaxyKickStart repository but whose address is stated in the file requirements_roles.yml . These roles will be installed in the subdirectory /root/GalaxyKickStart/roles/ . NB: ansible-galaxy has nothing to do with Galaxy, the name of this ansible command is serendipitous. triggers the play of the playbook galaxy.yml by ansible. The target host of the playbook is defined in the file inventory_files/analyseGenomes , as well as how ansible will interact with the target. Here, we play the playbook on the same computer (localhost). Prompts the end of the deployment","title":"Content of the run_ansible_analyse_genomes_2019.sh script"},{"location":"Run-Galaxy/Galaxy_architecture/","text":"Galaxy software built","title":"Appendix 3: Galaxy software architecture"},{"location":"Run-Galaxy/Galaxy_architecture/#galaxy-software-built","text":"","title":"Galaxy software built"},{"location":"Run-Galaxy/Google_cloud_Account/","text":"Run-Galaxy Run Galaxy training course Google Cloud Engine Prerequisite: a Google account / Gmail account Connect to Google Cloud Engine Click on Essai Gratuit / Free Trial Enter your Gmail mail address and password Review conditions and accept Inscription Form: Entreprise / Company : put anything like \"Perso\" or \"foo/bar\" Ajouter une carte de paiement / Add credit Card or optionally Ajouter un compte bancaire / Bank account You are in for a free trial of 12 months / 300 $ go to your Google Cloud Console to control your spin off / control your Virtual Machines","title":"Appendix 1: Getting a Google Cloud Engine Account"},{"location":"Run-Galaxy/Google_cloud_Account/#run-galaxy","text":"","title":"Run-Galaxy"},{"location":"Run-Galaxy/Google_cloud_Account/#run-galaxy-training-course","text":"","title":"Run Galaxy training course"},{"location":"Run-Galaxy/Google_cloud_Account/#google-cloud-engine","text":"Prerequisite: a Google account / Gmail account Connect to Google Cloud Engine Click on Essai Gratuit / Free Trial Enter your Gmail mail address and password Review conditions and accept Inscription Form: Entreprise / Company : put anything like \"Perso\" or \"foo/bar\" Ajouter une carte de paiement / Add credit Card or optionally Ajouter un compte bancaire / Bank account You are in for a free trial of 12 months / 300 $ go to your Google Cloud Console to control your spin off / control your Virtual Machines","title":"Google Cloud Engine"},{"location":"Run-Galaxy/Run_workflow/","text":"Running a workflow in Galaxy In this use case, we are going to Upload 3 workflow description files in the Galaxy server instance Visualise these workflows and see that tools to execute the workflows are missing since you are administrating the instance , install the missing tools Eventually run the workflows on input data obtained from a remote public repository. 1. Upload workflow description file (.ga) Ensure you are connected to your Galaxy server as an admin (the email you have entered in the galaxy.yml configuration file and the password to you've entered for this login when you registered for the first time) Click the workflow menu Click the \"Upload or import workflow\" button at the top right In the Galaxy workflow URL: field, paste the url of the workflow file: 1 https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/workflows/Galaxy-Workflow-canonical_transposons.gtf_from_transposon_sequence_set.txt.ga Note that this file is in the Run-Galaxy repository where a part of the material for this training is hosted Click on the Import button repeat the same operation with the second workflow 1 https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/workflows/Galaxy-Workflow-Extract_canonical_transposons_fasta.ga repeat the same operation with the third workflow 1 https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/workflows/Galaxy-Workflow-workflow_of_workflows.ga Note Alternatively, you could upload the workflow files from you computer instead of uploading them by URL the 'Workflow` menu is now a list of 3 workflows that should look like : Click the workflow canonical_transposons.gtf from transposon_sequence_set.txt (imported from uploaded file) and the Edit option Observe the warning window that should look like: Issues loading this workflow Please review the following issues, possibly resulting from tool upgrades or changes. Step 3: toolshed.g2.bx.psu.edu/repos/kellrott/regex_replace/regex_replace/1.0.0 Tool is not installed Step 4: toolshed.g2.bx.psu.edu/repos/kellrott/regex_replace/regex_replace/1.0.0 Tool is not installed Step 5: toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.0.1 Tool is not installed Step 6: toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.0.1 Tool is not installed Step 7: toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.0.1 Tool is not installed Step 8: toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.0.1 Tool is not installed Step 9: toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.0.1 Tool is not installed Step 10: toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.0.1 Tool is not installed Step 12: toolshed.g2.bx.psu.edu/repos/kellrott/regex_replace/regex_replace/1.0.0 Tool is not installed Step 13: toolshed.g2.bx.psu.edu/repos/kellrott/regex_replace/regex_replace/1.0.0 Tool is not installed When you read the warnings, you will see that the workflow was indeed successfully imported. However, some tools are missing, namely: 1 2 3 toolshed.g2.bx.psu.edu/repos/kellrott/regex_replace/regex_replace/1.0.0, version 1.0.0 toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.1.0, version 0.1.0 toolshed.g2.bx.psu.edu/repos/jjohnson/regex_find_replace/regex_find_replace/0.1.0, version 0.1.0 The other lines are redundant, because the workflow is using the same tools at different steps. broken workflow We are going to fix this. Click on the Continue button and then the upper \"wheel\" icon and select Close , we will come back to the workflow editor when the missing tools are installed in the server. 2. Installing (missing) tools The missing tools are reported in the tools.yml file in yaml format in the Run-Galaxy repository, as well as just bellow. Details of missing tools Thus, we have to install the following three tools in our Galaxy instance: tools: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 - name : regex_replace owner : kellrott revisions : - 9a77d5fca67c tool_panel_section_label : Analyse des genomes tool_shed_url : https://toolshed.g2.bx.psu.edu - name : column_regex_substitution owner : blankenberg revisions : - 12b740c4cbc1 tool_panel_section_label : Analyse des genomes tool_shed_url : https://toolshed.g2.bx.psu.edu - name : regex_find_replace owner : jjohnson revisions : - 9ea374bb0350 tool_panel_section_label : Analyse des genomes tool_shed_url : https://toolshed.g2.bx.psu.edu Click on the Admin top menu In the left bar click on Manage tools Check that there is actually no installed tools ! Now, click the Install new tools menu (again in the left bar) Press the Galaxy Main Tool Shed button In the search field, copy and paste 1 regex_replace and press the Enter key. One tool will show up, owned by kellrott . Click this tool, and select preview and install (No other solution anyway) Click the Install to Galaxy button at the top of the screen In the Select existing tool panel section: menu, select Text Manipulation . Thus, the tools will appears in the section Text Manipulation of the Galaxy tools. Click Install After a few seconds, you will notice the Monitor installing tools... in the screen. And rapidly enough, the Installation status should turn to green. Click again the Manage tools menu in the left bar, and look at the newly installed tool regex_find_replace in the list. Repeat the same operations for the tool 1 regex_find_replace owned by jjohnson (version 1.1.0 ). Do not take the tool with the same name but owned by galaxyp Repeat the same operations for the tool 1 column_regex_substitution owned by blankenberg (version 0.0.1 ) For this last installation, you will see a different panel after clicking Install to Galaxy : If you scroll down a little bit, you should see a list of uninstalled tool dependencies like this: This is a software package required to get the tool column_regex_substitution working properly. The required package (python 2.7) will be installed by the package manager conda . You can further check this by clicking the Display Details button bellow the Dependency list. Do not forget to select the tool panel section Text Manipulation , and finally click the Install button. This time, the Monitor installing tool shed repositories will display new steps (in yellow/orange), including the Installing tool dependencies step. The whole process may take longer, but not too long in this specific case. Finally go back for a last time to the Manage tools panel: There you'll see all three tools needed to properly run the imported workflow. 3. Check that the imported workflows now display correctly If you click the workflow top menu, you should now be able to edit the imported workflows, and see that everything is displaying correctly. For the workflow canonical_transposons.gtf from transposon_sequence_set.txt : We can go through the various steps of the workflows and figure out what they are doing. This first workflow performs a suite of find-and-replace text manipulations, starting from input data that has been tagged transposon_set_embl.txt and producing a new text dataset that is renamed canonical_transposons.gtf . The second workflow uses the same input data file transposon_set_embl.txt to generate a fasta file of canonical_transposon sequences The third workflow is a workflow of the two previous workflows ! We will come back to all these steps after the workflows execution. However, we need to retrieve the input data set before running the workflows on these data. Retrieve the transposon_set_embl.txt dataset Create a new history and name it transposon_set_embl.txt manipulation import the dataset using the Paste/Fetch data mode of the upload manager (the small bottom-top arrow icone at the top left of the Galaxy interface). Copy the URL 1 https://github.com/cbergman/transposons/raw/master/current/transposon_sequence_set.embl.txt in the open field and click the Start button. have a close look at the file Run the workflow Click on the workflow menu Click on the first workflow and select the Run option Leave the Send results to a new history menu to the No option for the moment. Just Click the Run workflow button to run the workflow, and look at datasets in the history turning from grey to yellow to green. Note: often you don't see the dataset in the \"yellow\" state (running). You just need to refresh the history with the 2-curved-arrows icon of the local history menu. repeat the same operation (from the input history) for the second workflow Extract canonical transposons fasta (imported from uploaded file) Discussion on workflows and on workflow of workflows","title":"After deployment use case"},{"location":"Run-Galaxy/Run_workflow/#running-a-workflow-in-galaxy","text":"","title":"Running a workflow in Galaxy"},{"location":"Run-Galaxy/Run_workflow/#in-this-use-case-we-are-going-to","text":"Upload 3 workflow description files in the Galaxy server instance Visualise these workflows and see that tools to execute the workflows are missing since you are administrating the instance , install the missing tools Eventually run the workflows on input data obtained from a remote public repository.","title":"In this use case, we are going to"},{"location":"Run-Galaxy/Run_workflow/#1-upload-workflow-description-file-ga","text":"Ensure you are connected to your Galaxy server as an admin (the email you have entered in the galaxy.yml configuration file and the password to you've entered for this login when you registered for the first time) Click the workflow menu Click the \"Upload or import workflow\" button at the top right In the Galaxy workflow URL: field, paste the url of the workflow file: 1 https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/workflows/Galaxy-Workflow-canonical_transposons.gtf_from_transposon_sequence_set.txt.ga Note that this file is in the Run-Galaxy repository where a part of the material for this training is hosted Click on the Import button repeat the same operation with the second workflow 1 https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/workflows/Galaxy-Workflow-Extract_canonical_transposons_fasta.ga repeat the same operation with the third workflow 1 https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/workflows/Galaxy-Workflow-workflow_of_workflows.ga Note Alternatively, you could upload the workflow files from you computer instead of uploading them by URL the 'Workflow` menu is now a list of 3 workflows that should look like : Click the workflow canonical_transposons.gtf from transposon_sequence_set.txt (imported from uploaded file) and the Edit option Observe the warning window that should look like: Issues loading this workflow Please review the following issues, possibly resulting from tool upgrades or changes. Step 3: toolshed.g2.bx.psu.edu/repos/kellrott/regex_replace/regex_replace/1.0.0 Tool is not installed Step 4: toolshed.g2.bx.psu.edu/repos/kellrott/regex_replace/regex_replace/1.0.0 Tool is not installed Step 5: toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.0.1 Tool is not installed Step 6: toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.0.1 Tool is not installed Step 7: toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.0.1 Tool is not installed Step 8: toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.0.1 Tool is not installed Step 9: toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.0.1 Tool is not installed Step 10: toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.0.1 Tool is not installed Step 12: toolshed.g2.bx.psu.edu/repos/kellrott/regex_replace/regex_replace/1.0.0 Tool is not installed Step 13: toolshed.g2.bx.psu.edu/repos/kellrott/regex_replace/regex_replace/1.0.0 Tool is not installed When you read the warnings, you will see that the workflow was indeed successfully imported. However, some tools are missing, namely: 1 2 3 toolshed.g2.bx.psu.edu/repos/kellrott/regex_replace/regex_replace/1.0.0, version 1.0.0 toolshed.g2.bx.psu.edu/repos/blankenberg/column_regex_substitution/column_regex_substitution/0.1.0, version 0.1.0 toolshed.g2.bx.psu.edu/repos/jjohnson/regex_find_replace/regex_find_replace/0.1.0, version 0.1.0 The other lines are redundant, because the workflow is using the same tools at different steps. broken workflow We are going to fix this. Click on the Continue button and then the upper \"wheel\" icon and select Close , we will come back to the workflow editor when the missing tools are installed in the server.","title":"1. Upload workflow description file (.ga)"},{"location":"Run-Galaxy/Run_workflow/#2-installing-missing-tools","text":"The missing tools are reported in the tools.yml file in yaml format in the Run-Galaxy repository, as well as just bellow. Details of missing tools Thus, we have to install the following three tools in our Galaxy instance: tools: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 - name : regex_replace owner : kellrott revisions : - 9a77d5fca67c tool_panel_section_label : Analyse des genomes tool_shed_url : https://toolshed.g2.bx.psu.edu - name : column_regex_substitution owner : blankenberg revisions : - 12b740c4cbc1 tool_panel_section_label : Analyse des genomes tool_shed_url : https://toolshed.g2.bx.psu.edu - name : regex_find_replace owner : jjohnson revisions : - 9ea374bb0350 tool_panel_section_label : Analyse des genomes tool_shed_url : https://toolshed.g2.bx.psu.edu Click on the Admin top menu In the left bar click on Manage tools Check that there is actually no installed tools ! Now, click the Install new tools menu (again in the left bar) Press the Galaxy Main Tool Shed button In the search field, copy and paste 1 regex_replace and press the Enter key. One tool will show up, owned by kellrott . Click this tool, and select preview and install (No other solution anyway) Click the Install to Galaxy button at the top of the screen In the Select existing tool panel section: menu, select Text Manipulation . Thus, the tools will appears in the section Text Manipulation of the Galaxy tools. Click Install After a few seconds, you will notice the Monitor installing tools... in the screen. And rapidly enough, the Installation status should turn to green. Click again the Manage tools menu in the left bar, and look at the newly installed tool regex_find_replace in the list. Repeat the same operations for the tool 1 regex_find_replace owned by jjohnson (version 1.1.0 ). Do not take the tool with the same name but owned by galaxyp Repeat the same operations for the tool 1 column_regex_substitution owned by blankenberg (version 0.0.1 ) For this last installation, you will see a different panel after clicking Install to Galaxy : If you scroll down a little bit, you should see a list of uninstalled tool dependencies like this: This is a software package required to get the tool column_regex_substitution working properly. The required package (python 2.7) will be installed by the package manager conda . You can further check this by clicking the Display Details button bellow the Dependency list. Do not forget to select the tool panel section Text Manipulation , and finally click the Install button. This time, the Monitor installing tool shed repositories will display new steps (in yellow/orange), including the Installing tool dependencies step. The whole process may take longer, but not too long in this specific case. Finally go back for a last time to the Manage tools panel: There you'll see all three tools needed to properly run the imported workflow.","title":"2. Installing (missing) tools"},{"location":"Run-Galaxy/Run_workflow/#3-check-that-the-imported-workflows-now-display-correctly","text":"If you click the workflow top menu, you should now be able to edit the imported workflows, and see that everything is displaying correctly. For the workflow canonical_transposons.gtf from transposon_sequence_set.txt : We can go through the various steps of the workflows and figure out what they are doing. This first workflow performs a suite of find-and-replace text manipulations, starting from input data that has been tagged transposon_set_embl.txt and producing a new text dataset that is renamed canonical_transposons.gtf . The second workflow uses the same input data file transposon_set_embl.txt to generate a fasta file of canonical_transposon sequences The third workflow is a workflow of the two previous workflows ! We will come back to all these steps after the workflows execution. However, we need to retrieve the input data set before running the workflows on these data.","title":"3. Check that the imported workflows now display correctly"},{"location":"Run-Galaxy/Run_workflow/#retrieve-the-transposon_set_embltxt-dataset","text":"Create a new history and name it transposon_set_embl.txt manipulation import the dataset using the Paste/Fetch data mode of the upload manager (the small bottom-top arrow icone at the top left of the Galaxy interface). Copy the URL 1 https://github.com/cbergman/transposons/raw/master/current/transposon_sequence_set.embl.txt in the open field and click the Start button. have a close look at the file","title":"Retrieve the transposon_set_embl.txt dataset"},{"location":"Run-Galaxy/Run_workflow/#run-the-workflow","text":"Click on the workflow menu Click on the first workflow and select the Run option Leave the Send results to a new history menu to the No option for the moment. Just Click the Run workflow button to run the workflow, and look at datasets in the history turning from grey to yellow to green. Note: often you don't see the dataset in the \"yellow\" state (running). You just need to refresh the history with the 2-curved-arrows icon of the local history menu. repeat the same operation (from the input history) for the second workflow Extract canonical transposons fasta (imported from uploaded file)","title":"Run the workflow"},{"location":"Run-Galaxy/Run_workflow/#discussion-on-workflows-and-on-workflow-of-workflows","text":"","title":"Discussion on workflows and on workflow of workflows"},{"location":"Run-Galaxy/Training_menu/","text":"Training Menu Three methods of Galaxy server deployment will be explained in this tutorial, which can be used with personal computers, clusters of machines or virtual machines in cloud computing environment. All you need is an ssh access and the control of tcp/ip ports of the target machine. These two conditions are far more easily fulfilled with virtual machines in clouds. This is the reason why we are going to use virtual machines in the Google Cloud Engine . Outline of the training session 1. Deployment of a Galaxy server in a VM using git 2. Deployment of a Galaxy server using Ansible 3. Use case of Galaxy administration: Install a computational workflow Install tools for its proper execution Running the workflow. 4. Deployment of a Galaxy server using Docker 5. Packaging and distribution of a virtual machine.","title":"Training Menu"},{"location":"Run-Galaxy/Training_menu/#training-menu","text":"Three methods of Galaxy server deployment will be explained in this tutorial, which can be used with personal computers, clusters of machines or virtual machines in cloud computing environment. All you need is an ssh access and the control of tcp/ip ports of the target machine. These two conditions are far more easily fulfilled with virtual machines in clouds. This is the reason why we are going to use virtual machines in the Google Cloud Engine .","title":"Training Menu"},{"location":"Run-Galaxy/Training_menu/#outline-of-the-training-session","text":"","title":"Outline of the training session"},{"location":"Run-Galaxy/Training_menu/#1-deployment-of-a-galaxy-server-in-a-vm-using-git","text":"","title":"1. Deployment of a Galaxy server in a VM using git"},{"location":"Run-Galaxy/Training_menu/#2-deployment-of-a-galaxy-server-using-ansible","text":"","title":"2. Deployment of a Galaxy server using Ansible"},{"location":"Run-Galaxy/Training_menu/#3-use-case-of-galaxy-administration","text":"Install a computational workflow Install tools for its proper execution Running the workflow.","title":"3. Use case of Galaxy administration:"},{"location":"Run-Galaxy/Training_menu/#4-deployment-of-a-galaxy-server-using-docker","text":"","title":"4. Deployment of a Galaxy server using Docker"},{"location":"Run-Galaxy/Training_menu/#5-packaging-and-distribution-of-a-virtual-machine","text":"","title":"5. Packaging and distribution of a virtual machine."},{"location":"Run-Galaxy/bare-galaxy/","text":"Install a minimal galaxy server with git Spin off a virtual Machine bare-galaxy You may have already done this in the previous section . If not, refer to this section We are going to use a GCE Virtual Machine with: Google Instance Ubuntu 16.04 LTS as OS 2 processors 7.5 Gb RAM a 50 Gb Volume Connect to the VM as explained in this section using the ssh web console Installation of dependencies To install Galaxy, we only need a few dependencies (i.e. pre-installed programs which Galaxy needs) and a limited number of command lines. We are going to execute these instruction as the root unix user. This is easier because installation of new programs as well as manipulations of network interfaces is generally permitted only to users with administration rights. So let's do this, step by step: 1. 1 sudo -i This command open a new \"shell\" where you are root. You can check this by typing pwd that should return /root/ , meaning that you are now working in the directory of the root user. 2. 1 apt update -y This command will just trigger an update of the installed programs database in the Ubuntu OS. 3. 1 apt install -y python-dev python-pip nano git This command install some python programs (python-dev and python-pip) that are intensively used by Galaxy, nano , a simple text editor we need, and the git program which is the software to fetch or and update Galaxy (i.e. a sort of \"installer\" program). The -y option specifies to the apt package installer that no manual confirmation is needed for this command. 4. 1 git clone https://github.com/galaxyproject/galaxy.git -b release_19.05 This command says to use git to clone the code repository located at https://github.com/galaxyproject/galaxy.git . In addition the -b release_19.05 option specifies that only the version release_19.05 will be cloned locally in your virtual machine. You may try to visualize the URL https://github.com/galaxyproject/galaxy.git in your web browser. You will, literally, see the code of Galaxy. It is Open Source, as you can notice. 5. 1 cd galaxy This command shift you in the galaxy directory that was created by git and the git clone command in 4. 6. 1 cp config/galaxy.yml.sample config/galaxy.yml This command makes a copie of the galaxy.yml.sample file into galaxy.yml - in the directory config that is in the galaxy directory. 7. 1 nano config/galaxy.yml Using this command, we are going to edit some important settings that are required to run our Galaxy fresh instance. Find the line 1 http: 127.0.0.1:8080 (you can use the editor command Ctrl + W , paste the previous line and press enter) and edit it the to 1 http: 0.0.0.0:80 By doing this, we ensure that we will be able to reach the galaxy web server on our virtual machine using the usual web port 80 . Find the line 1 #admin_users: delete the # character and type your email address between the two single quotes. Any email address is ok ( if you want). It is just used here as an admin identifier. save your changes by pressing the key combination Ctrl + O quit nano by pressing the key combination Ctrl + X 8. Ready for deploying Galaxy ? Then type sh run.sh and press the enter key ! You should see an abundant log scrolling down. Don't worry ! All Galaxy dependencies required for the Galaxy server instance are being downloaded and installed The Galaxy computing environment is automatically set up the Galaxy web server is installed and static pages are built The Galaxy database is automatically upgraded to its latest structure/model Various tools are upgraded. After 5-10 minutes, you should see the log stopping with: 1 2 Starting server in PID 3813. serving on http://127.0.0.1:80 8. Connect to your living Galaxy instance If so, this is all good, and you can now access to you Galaxy instance in a you web browser window: Go back to your Google Cloud Engine control panel. Find the External IP address / Adresse IP externe in the 7 th column of the Dashbord (to the left of the ssh menu that you used before. Click on the hyperlink. 9. Connect as an admin of your Galaxy server instance Register to your instance using the email address you put in the galaxy.yml at step 7 (menu \"Authentification et enregistrement Enregistrement) Now that you are registered, you can log in using the same login and password you have chosen. After login, you will see the admin tab in the top menu of the Galaxy interface. You are connected to Galaxy as an admin !","title":"Install a minimal standalone galaxy server"},{"location":"Run-Galaxy/bare-galaxy/#install-a-minimal-galaxy-server-with-git","text":"","title":"Install a minimal galaxy server with git"},{"location":"Run-Galaxy/bare-galaxy/#spin-off-a-virtual-machine-bare-galaxy","text":"You may have already done this in the previous section . If not, refer to this section We are going to use a GCE Virtual Machine with: Google Instance Ubuntu 16.04 LTS as OS 2 processors 7.5 Gb RAM a 50 Gb Volume","title":"Spin off a virtual Machine bare-galaxy"},{"location":"Run-Galaxy/bare-galaxy/#connect-to-the-vm-as-explained-in-this-section-using-the-ssh-web-console","text":"","title":"Connect to the VM as explained in this section using the ssh web console"},{"location":"Run-Galaxy/bare-galaxy/#installation-of-dependencies","text":"To install Galaxy, we only need a few dependencies (i.e. pre-installed programs which Galaxy needs) and a limited number of command lines. We are going to execute these instruction as the root unix user. This is easier because installation of new programs as well as manipulations of network interfaces is generally permitted only to users with administration rights. So let's do this, step by step:","title":"Installation of dependencies"},{"location":"Run-Galaxy/bare-galaxy/#1","text":"1 sudo -i This command open a new \"shell\" where you are root. You can check this by typing pwd that should return /root/ , meaning that you are now working in the directory of the root user.","title":"1."},{"location":"Run-Galaxy/bare-galaxy/#2","text":"1 apt update -y This command will just trigger an update of the installed programs database in the Ubuntu OS.","title":"2."},{"location":"Run-Galaxy/bare-galaxy/#3","text":"1 apt install -y python-dev python-pip nano git This command install some python programs (python-dev and python-pip) that are intensively used by Galaxy, nano , a simple text editor we need, and the git program which is the software to fetch or and update Galaxy (i.e. a sort of \"installer\" program). The -y option specifies to the apt package installer that no manual confirmation is needed for this command.","title":"3."},{"location":"Run-Galaxy/bare-galaxy/#4","text":"1 git clone https://github.com/galaxyproject/galaxy.git -b release_19.05 This command says to use git to clone the code repository located at https://github.com/galaxyproject/galaxy.git . In addition the -b release_19.05 option specifies that only the version release_19.05 will be cloned locally in your virtual machine. You may try to visualize the URL https://github.com/galaxyproject/galaxy.git in your web browser. You will, literally, see the code of Galaxy. It is Open Source, as you can notice.","title":"4."},{"location":"Run-Galaxy/bare-galaxy/#5","text":"1 cd galaxy This command shift you in the galaxy directory that was created by git and the git clone command in 4.","title":"5."},{"location":"Run-Galaxy/bare-galaxy/#6","text":"1 cp config/galaxy.yml.sample config/galaxy.yml This command makes a copie of the galaxy.yml.sample file into galaxy.yml - in the directory config that is in the galaxy directory.","title":"6."},{"location":"Run-Galaxy/bare-galaxy/#7","text":"1 nano config/galaxy.yml Using this command, we are going to edit some important settings that are required to run our Galaxy fresh instance. Find the line 1 http: 127.0.0.1:8080 (you can use the editor command Ctrl + W , paste the previous line and press enter) and edit it the to 1 http: 0.0.0.0:80 By doing this, we ensure that we will be able to reach the galaxy web server on our virtual machine using the usual web port 80 . Find the line 1 #admin_users: delete the # character and type your email address between the two single quotes. Any email address is ok ( if you want). It is just used here as an admin identifier. save your changes by pressing the key combination Ctrl + O quit nano by pressing the key combination Ctrl + X","title":"7."},{"location":"Run-Galaxy/bare-galaxy/#8-ready-for-deploying-galaxy","text":"Then type sh run.sh and press the enter key ! You should see an abundant log scrolling down. Don't worry ! All Galaxy dependencies required for the Galaxy server instance are being downloaded and installed The Galaxy computing environment is automatically set up the Galaxy web server is installed and static pages are built The Galaxy database is automatically upgraded to its latest structure/model Various tools are upgraded. After 5-10 minutes, you should see the log stopping with: 1 2 Starting server in PID 3813. serving on http://127.0.0.1:80","title":"8. Ready for deploying Galaxy ?"},{"location":"Run-Galaxy/bare-galaxy/#8-connect-to-your-living-galaxy-instance","text":"If so, this is all good, and you can now access to you Galaxy instance in a you web browser window: Go back to your Google Cloud Engine control panel. Find the External IP address / Adresse IP externe in the 7 th column of the Dashbord (to the left of the ssh menu that you used before. Click on the hyperlink.","title":"8. Connect to your living Galaxy instance"},{"location":"Run-Galaxy/bare-galaxy/#9-connect-as-an-admin-of-your-galaxy-server-instance","text":"Register to your instance using the email address you put in the galaxy.yml at step 7 (menu \"Authentification et enregistrement Enregistrement) Now that you are registered, you can log in using the same login and password you have chosen. After login, you will see the admin tab in the top menu of the Galaxy interface.","title":"9. Connect as an admin of your Galaxy server instance"},{"location":"Run-Galaxy/bare-galaxy/#you-are-connected-to-galaxy-as-an-admin","text":"","title":"You are connected to Galaxy as an admin !"},{"location":"Run-Galaxy/spin_off_VM/","text":"Spin off a virtual Machine Go to the Google Cloud Dashboard and select \"Compute Engine\" on the left hand menu bar Select the submenu \"Instances de VM\" Click on the top bar menu the \"CREER UNE INSTANCE\" panel Put name \"bare-galaxy\" Choose a Zone (suggestion: europe-west1-c ) Type de machine: choose 2 vCPU with 7.5 Memory Disque de D\u00e9marrage: Click on Modifier Select Ubuntu 16.04 LTS At the bottom of the form, put 100 Go for the Disk Size (Taille) Leave the selection Disque persistant standard / Standard persistant drive Click Select / S\u00e9lectionner Click Authorize HTTP traffic / Autoriser le traffic HTTP Click Cr\u00e9er / Create Connect to the started virtual Machine After a few seconds, the VM turns on \"green\" and an ssh menu becomes selectable Roll down this ssh menu and select the first option Ouvrir dans la fen\u00eatre du navigateur A shell console pop out and you should now be ready to control your VM with linux command lines Enter the sudo -i command at the prompt yourlogin@bare-galaxy:~$ and hit the return key. The unix prompt become root@bare-galaxy:~# : you are now controling your VM as a root administrator. [Optional] Here, if you do not have to work with the VM, you can turn off the VM and even trash it: in one shot, go back to your VM control panel in the web browser, ensure that the running VM is checked, and press the Trash button in the top menu. Confirm that you want to trash the VM and loose everything. after a few seconds the VM disappears from the Dashboard.","title":"Appendix 1: Spin off a Virtual Machine"},{"location":"Run-Galaxy/spin_off_VM/#spin-off-a-virtual-machine","text":"Go to the Google Cloud Dashboard and select \"Compute Engine\" on the left hand menu bar Select the submenu \"Instances de VM\" Click on the top bar menu the \"CREER UNE INSTANCE\" panel Put name \"bare-galaxy\" Choose a Zone (suggestion: europe-west1-c ) Type de machine: choose 2 vCPU with 7.5 Memory Disque de D\u00e9marrage: Click on Modifier Select Ubuntu 16.04 LTS At the bottom of the form, put 100 Go for the Disk Size (Taille) Leave the selection Disque persistant standard / Standard persistant drive Click Select / S\u00e9lectionner Click Authorize HTTP traffic / Autoriser le traffic HTTP Click Cr\u00e9er / Create","title":"Spin off a virtual Machine"},{"location":"Run-Galaxy/spin_off_VM/#connect-to-the-started-virtual-machine","text":"After a few seconds, the VM turns on \"green\" and an ssh menu becomes selectable Roll down this ssh menu and select the first option Ouvrir dans la fen\u00eatre du navigateur A shell console pop out and you should now be ready to control your VM with linux command lines Enter the sudo -i command at the prompt yourlogin@bare-galaxy:~$ and hit the return key. The unix prompt become root@bare-galaxy:~# : you are now controling your VM as a root administrator. [Optional] Here, if you do not have to work with the VM, you can turn off the VM and even trash it: in one shot, go back to your VM control panel in the web browser, ensure that the running VM is checked, and press the Trash button in the top menu. Confirm that you want to trash the VM and loose everything. after a few seconds the VM disappears from the Dashboard.","title":"Connect to the started virtual Machine"},{"location":"metavisitor/","text":"Metavisitor is a user-friendly and adaptable software to provide biologists, clinical researchers and possibly diagnostic clinicians with the ability to robustly detect and reconstruct viral genomes from complex deep sequence datasets. A set of modular bioinformatic tools and workflows was implemented as the Metavisitor package in the Galaxy framework. Using the graphical Galaxy workflow editor, users with minimal computational skills can use existing Metavisitor workflows or adapt them to suit specific needs by adding or modifying analysis modules. Reference viral database Metavisitor's workflows use a home-made reference viral database vir2 . This database was made using Galaxy-Workflow-Metavisitor__Workflow_for_nucleic_vir2_generation and Galaxy-Workflow-Metavisitor__Workflow_for_proteic_vir2_generation , that can both be found in Metavisitor's Github . How was nucleic vir2 generated? Downloading every viral sequence from NCBI's nuccore database with the following queries (2018/03/21): txid10239[Organism] NOT txid131567[Organism] NOT phage[All Fields] NOT patent[All Fields] NOT chimeric[Title] NOT vector[Title] NOT method[Title] NOT X174[All Fields] AND 301:10000[Sequence length] txid10239[Organism] NOT txid131567[Organism] NOT phage[All Fields] NOT patent[All Fields] NOT chimeric[Title] NOT vector[Title] NOT method[Title] NOT X174[All Fields] AND 10001:1300000[Sequence length] Clustering sequences with 95% identity and shorter than 10 001 bp using vclust. vir2 is available for download in Figshare Quick Start Users who want to use Metavisitor on the Galaxy Mississippi Server , or got already the Metavisitor suite of tools installed in their own Galaxy server, can jump to the next chapter Prepare input data histories . Availability of Metavisitor tools and workflows Metavisitor has been developed at the ARTbio platform . Its tools are primarily available in GitHub . Its workflows are primarily available in the metavisitor repository Metavisitor tools and workflows are also available in the toolshed Metavisitor tools developed by ARTbio in the ARTbio GitHub blast_to_scaffold blastx_to_scaffold blastparser_and_hits blast_unmatched cap3 cherry_pick_fasta cat_multi_datasets fetch_fasta_from_ncbi oases sequence_format_converter small_rna_maps sr_bowtie yac_clipper Tools from other developers are used in the suite metavisitor-2. These tools are available from the main Galaxy toolshed : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 name= bowtie2 owner= devteam name= data_manager_bowtie2_index_builder owner= devteam name= data_manager_fetch_genome_dbkeys_all_fasta owner= devteam name= fasta_compute_length owner= devteam name= fasta_filter_by_length owner= devteam name= fastx_trimmer owner= devteam name= ncbi_blast_plus owner= devteam name= data_manager_bowtie_index_builder owner= iuc name= khmer_normalize_by_median owner= iuc name= sra_tools owner= iuc name= trinity owner= iuc name= vsearch owner= iuc name= regex_find_replace owner= galaxyp name= spades owner= nml Availability of Metavisitor tools and workflows for Galaxy instance administrators All metavisitor tools are available from the suite_metavisitor_2 Galaxy Admin can just install this suite of tools by using the Install new tools menu in their Admin panel, searching for \"metavisitor\", and installing the suite_metavisitor_2 tool suite. Galaxy Admins can install the workflows from the metavisitor_workflows repository in the main Galaxy toolshed , which will install in addition all tools needed for Metavisitor. Availability of Metavisitors workflows for any Galaxy instance user. We have deposited the Metavisitors workflows in the myexperiment server , where they are searchable with \"metavisitor\" keyword and can be downloaded and reuploaded to the Galaxy instance. Starting a Metavisitor Galaxy server from scratch In the last section of this documentation, we provide instructions to set up Galaxy server instances from scratch with pre-installed Metavisitor tools and workflows: Based on Ansible: see Metavisitor with GalaxyKickstart (Ansible) Based on Docker: see Metavitor with Docker","title":"Metavisitor"},{"location":"metavisitor/#reference-viral-database","text":"Metavisitor's workflows use a home-made reference viral database vir2 . This database was made using Galaxy-Workflow-Metavisitor__Workflow_for_nucleic_vir2_generation and Galaxy-Workflow-Metavisitor__Workflow_for_proteic_vir2_generation , that can both be found in Metavisitor's Github .","title":"Reference viral database"},{"location":"metavisitor/#how-was-nucleic-vir2-generated","text":"Downloading every viral sequence from NCBI's nuccore database with the following queries (2018/03/21): txid10239[Organism] NOT txid131567[Organism] NOT phage[All Fields] NOT patent[All Fields] NOT chimeric[Title] NOT vector[Title] NOT method[Title] NOT X174[All Fields] AND 301:10000[Sequence length] txid10239[Organism] NOT txid131567[Organism] NOT phage[All Fields] NOT patent[All Fields] NOT chimeric[Title] NOT vector[Title] NOT method[Title] NOT X174[All Fields] AND 10001:1300000[Sequence length] Clustering sequences with 95% identity and shorter than 10 001 bp using vclust. vir2 is available for download in Figshare","title":"How was nucleic vir2 generated?"},{"location":"metavisitor/#quick-start","text":"Users who want to use Metavisitor on the Galaxy Mississippi Server , or got already the Metavisitor suite of tools installed in their own Galaxy server, can jump to the next chapter Prepare input data histories .","title":"Quick Start"},{"location":"metavisitor/#availability-of-metavisitor-tools-and-workflows","text":"Metavisitor has been developed at the ARTbio platform . Its tools are primarily available in GitHub . Its workflows are primarily available in the metavisitor repository Metavisitor tools and workflows are also available in the toolshed","title":"Availability of Metavisitor tools and workflows"},{"location":"metavisitor/#metavisitor-tools-developed-by-artbio-in-the-artbio-github","text":"blast_to_scaffold blastx_to_scaffold blastparser_and_hits blast_unmatched cap3 cherry_pick_fasta cat_multi_datasets fetch_fasta_from_ncbi oases sequence_format_converter small_rna_maps sr_bowtie yac_clipper Tools from other developers are used in the suite metavisitor-2. These tools are available from the main Galaxy toolshed : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 name= bowtie2 owner= devteam name= data_manager_bowtie2_index_builder owner= devteam name= data_manager_fetch_genome_dbkeys_all_fasta owner= devteam name= fasta_compute_length owner= devteam name= fasta_filter_by_length owner= devteam name= fastx_trimmer owner= devteam name= ncbi_blast_plus owner= devteam name= data_manager_bowtie_index_builder owner= iuc name= khmer_normalize_by_median owner= iuc name= sra_tools owner= iuc name= trinity owner= iuc name= vsearch owner= iuc name= regex_find_replace owner= galaxyp name= spades owner= nml","title":"Metavisitor tools developed by ARTbio in the ARTbio GitHub"},{"location":"metavisitor/#availability-of-metavisitor-tools-and-workflows-for-galaxy-instance-administrators","text":"All metavisitor tools are available from the suite_metavisitor_2 Galaxy Admin can just install this suite of tools by using the Install new tools menu in their Admin panel, searching for \"metavisitor\", and installing the suite_metavisitor_2 tool suite. Galaxy Admins can install the workflows from the metavisitor_workflows repository in the main Galaxy toolshed , which will install in addition all tools needed for Metavisitor.","title":"Availability of Metavisitor tools and workflows for Galaxy instance administrators"},{"location":"metavisitor/#availability-of-metavisitors-workflows-for-any-galaxy-instance-user","text":"We have deposited the Metavisitors workflows in the myexperiment server , where they are searchable with \"metavisitor\" keyword and can be downloaded and reuploaded to the Galaxy instance.","title":"Availability of Metavisitors workflows for any Galaxy instance user."},{"location":"metavisitor/#starting-a-metavisitor-galaxy-server-from-scratch","text":"In the last section of this documentation, we provide instructions to set up Galaxy server instances from scratch with pre-installed Metavisitor tools and workflows: Based on Ansible: see Metavisitor with GalaxyKickstart (Ansible) Based on Docker: see Metavitor with Docker","title":"Starting a Metavisitor Galaxy server from scratch"},{"location":"metavisitor/install_metavisitor/","text":"In the next three chapters, we provide documentation on two methods to set up a Galaxy server instances from scratch with pre-installed Metavisitor tools and workflows: Based on GalaxyKickstarter: see Metavisitor with GalaxyKickstarter (Ansible) Based on Docker: see Metavitor with Docker For more detailed information on GalaxyKickStart see its documentation","title":"Intro"},{"location":"metavisitor/metavisitor_access_control/","text":"When you are done with the installation of your own Metavisitor Galaxy instance installation using either GalaxyKickStart or docker , there are a few basic things to know for web access and basic server admin operations 1. Connect web frontpage of your Metavisitor Galaxy We assume that you know the IP address to reach the Metavisitor Galaxy webserver: if you used GalaxyKickStart , you had to indicate this host IP in your hosts inventory file. if you used docker , you had to connect to the host machine with the appropriate IP address. Thus, to access Metavisitor Galaxy webserver, just type this IP address in your web browser. If you did not installed yourself the Metavisitor Galaxy instance, ask the IP address to the person who did it. In case you decided to get the Metavisitor Galaxy served on a subdirectory do not forget to append this /subdirectory in your url which then looks like http:// IP /subdirectory 2. Log to the Galaxy server using the admin credentials: If everything goes well, you should now see the Galaxy Metavisitor home page in your web browser. You have to log as the admin. To do that, go to the User menu and click login This is your first login, thus the admin login is admin@galaxy.org and your admin password is admin However for security, immediately change the admin password. To do this, go again in the Users menu, Preferences And click the Change your password item in the User preferences. Basic admin operation: restart the Metavisitor Galaxy instance As we will see in the next chapter , installations of reference genomes or additional tools in the Galaxy Metavisitor instance imply a Galaxy restart for completion. Here is how to do it. restart Metavisitor Galaxy instance deployed with GalaxyKickStart Connect to the server where the Galaxy instance has been installed either through the ssh connection you have used with GalaxyKickStart in your terminal, type sudo supervisorctl restart galaxy: If everything went fine you should see in your terminal 1 2 3 4 5 6 7 # supervisorctl restart galaxy: galaxy_web: stopped handler0: stopped handler1: stopped handler0: started handler1: started galaxy_web: started That's it, the Galaxy instance has restarted. restart Metavisitor Galaxy instance deployed with docker Connect to the server where the Galaxy instance has been installed either through the ssh connection you have used with GalaxyKickStart connect to you docker host using ssh type docker ps . You should see your Metavisitor docker container running and the name of the container in the NAMES column enter into your container by typing: docker exec -it name_of_the_container bash - in the docker session you can now type sudo supervisorctl restart galaxy: and see also, within the container: 1 2 3 4 5 6 7 # supervisorctl restart galaxy: galaxy_web: stopped handler0: stopped handler1: stopped handler0: started handler1: started galaxy_web: started That's it, the Galaxy instance has restarted. you can leave the container by typing exit","title":"Access and Control Metavisitor Galaxy instance"},{"location":"metavisitor/metavisitor_access_control/#1-connect-web-frontpage-of-your-metavisitor-galaxy","text":"We assume that you know the IP address to reach the Metavisitor Galaxy webserver: if you used GalaxyKickStart , you had to indicate this host IP in your hosts inventory file. if you used docker , you had to connect to the host machine with the appropriate IP address. Thus, to access Metavisitor Galaxy webserver, just type this IP address in your web browser. If you did not installed yourself the Metavisitor Galaxy instance, ask the IP address to the person who did it. In case you decided to get the Metavisitor Galaxy served on a subdirectory do not forget to append this /subdirectory in your url which then looks like http:// IP /subdirectory","title":"1. Connect web frontpage of your Metavisitor Galaxy"},{"location":"metavisitor/metavisitor_access_control/#2-log-to-the-galaxy-server-using-the-admin-credentials","text":"If everything goes well, you should now see the Galaxy Metavisitor home page in your web browser. You have to log as the admin. To do that, go to the User menu and click login This is your first login, thus the admin login is admin@galaxy.org and your admin password is admin However for security, immediately change the admin password. To do this, go again in the Users menu, Preferences And click the Change your password item in the User preferences.","title":"2. Log to the Galaxy server using the admin credentials:"},{"location":"metavisitor/metavisitor_access_control/#basic-admin-operation-restart-the-metavisitor-galaxy-instance","text":"As we will see in the next chapter , installations of reference genomes or additional tools in the Galaxy Metavisitor instance imply a Galaxy restart for completion. Here is how to do it.","title":"Basic admin operation: restart the Metavisitor Galaxy instance"},{"location":"metavisitor/metavisitor_access_control/#restart-metavisitor-galaxy-instance-deployed-with-galaxykickstart","text":"Connect to the server where the Galaxy instance has been installed either through the ssh connection you have used with GalaxyKickStart in your terminal, type sudo supervisorctl restart galaxy: If everything went fine you should see in your terminal 1 2 3 4 5 6 7 # supervisorctl restart galaxy: galaxy_web: stopped handler0: stopped handler1: stopped handler0: started handler1: started galaxy_web: started That's it, the Galaxy instance has restarted.","title":"restart Metavisitor Galaxy instance deployed with GalaxyKickStart"},{"location":"metavisitor/metavisitor_access_control/#restart-metavisitor-galaxy-instance-deployed-with-docker","text":"Connect to the server where the Galaxy instance has been installed either through the ssh connection you have used with GalaxyKickStart connect to you docker host using ssh type docker ps . You should see your Metavisitor docker container running and the name of the container in the NAMES column enter into your container by typing: docker exec -it name_of_the_container bash - in the docker session you can now type sudo supervisorctl restart galaxy: and see also, within the container: 1 2 3 4 5 6 7 # supervisorctl restart galaxy: galaxy_web: stopped handler0: stopped handler1: stopped handler0: started handler1: started galaxy_web: started That's it, the Galaxy instance has restarted. you can leave the container by typing exit","title":"restart Metavisitor Galaxy instance deployed with docker"},{"location":"metavisitor/metavisitor_ansible/","text":"Installing Metavisitor with GalaxyKickStart and Ansible Here, a Deployment Machine will install a Metavisitor Galaxy server on Target Machine . Note that Deployment Machine and Target Machine can both be local or remote machines, and that they can be the same machine. Requirements On the Deployment Machine , git and ansible need to be installed. The Target Machine has to be accessible through ssh connection by the user (you) with root privileges. This implies that a correct ssh private key file is available on your Deployment Machine , for instance ~/.ssh/id_rsa . This key will be used for secure transactions managed by ansible between the Deployment Machine and the Target Machine . see the GalaxyKickStart manual for more detailed informations on how to install appropriate version of Ansible. Getting the ansible playbook This is done on the Deployment Machine by cloning the GalaxyKickStart (GalaxyKickStart) repository hosted by the ARTbio organization : In your terminal, type: 1 git clone https://github.com/ARTbio/GalaxyKickStart.git and navigate in the GalaxyKickStart folder: 1 cd GalaxyKickStart GalaxyKickStart makes use of other Ansible roles -- referenced in the requirements_roles.yml file -- that need to be downloaded as part of the installation step: 1 ansible-galaxy install -r requirements_roles.yml -p roles This command installs additional roles in the roles folder. Adapting the GalaxyKickStart folder to your deployment There are only few things to change in the GalaxyKickStart folder before running ansible. Adapt the ansible inventory file In the GalaxyKickStart/inventory_files folder, there is a file called the metavisitor . For deploying Metavisitor, you need to edit this file so that it just contains 1 2 3 [ metavisitor ] ip address ansible_ssh_user = root ansible_ssh_private_key_file = path/to/the/ssh/private/key The ip address is the address of the Target Machine . The path/to/the/ssh/private/key is the path on the Deployment Machine to your ssh key, to be recognized by the Target Machine . Thus, a practical exemple of the final content on the inventory file metavisitor is: 1 2 3 [ metavisitor ] 192.54.201.126 ansible_ssh_user = root ansible_ssh_private_key_file = ~/.ssh/id_rsa where 192.54.201.126 is the ip address of the Target machine and ~/.ssh/id_rsa the path to the private ssh key. Note that you can also install locally Metavisitor by letting the metavisitor inventory file as is: 1 2 3 [ metavisitor ] localhost ansible_connection = local Adapt the ansible inventory file to an Amazon Web Service (AWS) virtual machine In this specific case, add in the hosts inventory file: 1 2 3 4 5 [metavisitor] 192.54.201.126 ansible_ssh_user = ubuntu ansible_ssh_private_key_file= ~/.ssh/aws_private_key.pem [aws] 192.54.201.126 In that case aws_private_key.pem is the private ssh key for interacting with aws instances, and the [aws] section will trigger additional actions for accessing the Metavisitor Galaxy instance in the Amazon cloud. Note that in addition the settings of the security group associated to the AWS instance should be as follows: 1 2 3 4 5 6 Type |Protocole| Port Range | Source | #comment __________________________________________________________________________________________ HTTP | TCP | 80 | 0.0.0.0/0 | for Galaxy web access SSH | TCP | 22 | 0.0.0.0/0 | for ssh access to the AWS instance Custom TCP Rule | TCP | 21 | 0.0.0.0/0 | for FTP upload to Galaxy Custom TCP Rule | TCP | 49152 - 65534 | 0.0.0.0/0 | for FTP upload to Galaxy The ports 21 and 49152 - 65534 should be open for FTP uploads to the AWS instance, and port 80 should be open for accessing galaxy. Adapt the group_vars/all file for persisting data, if needed. In cases where your Target machine has volumes where you wish the Galaxy data to be persisted in, you have to edit the GalaxyKickStart/group_vars/all file, to indicate the path to this volume on the Target machine . If you don't understand the previous statement, no worries, just don't do anything and skip this step. For others, find the lines 1 2 #persistent data galaxy_persistent_directory: /export # for IFB it s /root/mydisk, by default, /export in the GalaxyKickStart/group_vars/all file, and change /export to the path of your persistent volume. Note that if /export is not changed, nothing will happen and the deployed galaxy server and all associated data files will be in the /home/galaxy/galaxy folder of the Target Machine . Deploying Metavisitor Galaxy on the Target Machine You are almost done. Navigate with your terminal to your GalaxyKickStart folder and type the following command to run the ansible playbook for deploying metavisitor Galaxy on the Target Machine : 1 ansible-playbook --inventory-file=hosts galaxy.yml If everything is ok, you may be asked to authorize the access to the Target Machine by typing yes in the terminal, and you will see ansible orchestrating the serveur deployment on the Target Machine in this terminal. When the process is finished, you should be able to access the Target Machine by typing its IP address in your web browser. By default the admin login/password is admin@galaxy.org / admin . You should change the password for safety. Re-deploying Metavisitor Galaxy on the Target Machine If you are experimented in using ansible, you may customize your Metavisitor Galaxy instance deployed with GalaxyKickStart by editing the content of GalaxyKickStart . In that case, when your changes are done, just run again the command 1 ansible-playbook --inventory-file=hosts galaxy.yml When you run the playbook a second time, the process will be much faster, since steps that have already been executed are skipped. Whenever you change a variable (see customizations ), you need to run the playbook again. You can put multiple machines in your inventory: a simple way to do this is just copying the line the required number of times with the appropriate ip addresses: 1 2 3 4 [metavisitor] 192.54.201.126 ansible_ssh_user = root ansible_ssh_private_key_file= ~/.ssh/id_rsa 192.54.201.127 ansible_ssh_user = root ansible_ssh_private_key_file= ~/.ssh/id_rsa 192.54.201.128 ansible_ssh_user = root ansible_ssh_private_key_file= ~/.ssh/id_rsa","title":"GalaxyKickStart"},{"location":"metavisitor/metavisitor_ansible/#installing-metavisitor-with-galaxykickstart-and-ansible","text":"Here, a Deployment Machine will install a Metavisitor Galaxy server on Target Machine . Note that Deployment Machine and Target Machine can both be local or remote machines, and that they can be the same machine.","title":"Installing Metavisitor with GalaxyKickStart and Ansible"},{"location":"metavisitor/metavisitor_ansible/#requirements","text":"On the Deployment Machine , git and ansible need to be installed. The Target Machine has to be accessible through ssh connection by the user (you) with root privileges. This implies that a correct ssh private key file is available on your Deployment Machine , for instance ~/.ssh/id_rsa . This key will be used for secure transactions managed by ansible between the Deployment Machine and the Target Machine . see the GalaxyKickStart manual for more detailed informations on how to install appropriate version of Ansible.","title":"Requirements"},{"location":"metavisitor/metavisitor_ansible/#getting-the-ansible-playbook","text":"This is done on the Deployment Machine by cloning the GalaxyKickStart (GalaxyKickStart) repository hosted by the ARTbio organization : In your terminal, type: 1 git clone https://github.com/ARTbio/GalaxyKickStart.git and navigate in the GalaxyKickStart folder: 1 cd GalaxyKickStart GalaxyKickStart makes use of other Ansible roles -- referenced in the requirements_roles.yml file -- that need to be downloaded as part of the installation step: 1 ansible-galaxy install -r requirements_roles.yml -p roles This command installs additional roles in the roles folder.","title":"Getting the ansible playbook"},{"location":"metavisitor/metavisitor_ansible/#adapting-the-galaxykickstart-folder-to-your-deployment","text":"There are only few things to change in the GalaxyKickStart folder before running ansible.","title":"Adapting the GalaxyKickStart folder to your deployment"},{"location":"metavisitor/metavisitor_ansible/#adapt-the-ansible-inventory-file","text":"In the GalaxyKickStart/inventory_files folder, there is a file called the metavisitor . For deploying Metavisitor, you need to edit this file so that it just contains 1 2 3 [ metavisitor ] ip address ansible_ssh_user = root ansible_ssh_private_key_file = path/to/the/ssh/private/key The ip address is the address of the Target Machine . The path/to/the/ssh/private/key is the path on the Deployment Machine to your ssh key, to be recognized by the Target Machine . Thus, a practical exemple of the final content on the inventory file metavisitor is: 1 2 3 [ metavisitor ] 192.54.201.126 ansible_ssh_user = root ansible_ssh_private_key_file = ~/.ssh/id_rsa where 192.54.201.126 is the ip address of the Target machine and ~/.ssh/id_rsa the path to the private ssh key.","title":"Adapt the ansible inventory file"},{"location":"metavisitor/metavisitor_ansible/#note-that-you-can-also-install-locally-metavisitor-by-letting-the-metavisitor-inventory-file-as-is","text":"1 2 3 [ metavisitor ] localhost ansible_connection = local","title":"Note that you can also install locally Metavisitor by letting the metavisitor inventory file as is:"},{"location":"metavisitor/metavisitor_ansible/#adapt-the-ansible-inventory-file-to-an-amazon-web-service-aws-virtual-machine","text":"In this specific case, add in the hosts inventory file: 1 2 3 4 5 [metavisitor] 192.54.201.126 ansible_ssh_user = ubuntu ansible_ssh_private_key_file= ~/.ssh/aws_private_key.pem [aws] 192.54.201.126 In that case aws_private_key.pem is the private ssh key for interacting with aws instances, and the [aws] section will trigger additional actions for accessing the Metavisitor Galaxy instance in the Amazon cloud. Note that in addition the settings of the security group associated to the AWS instance should be as follows: 1 2 3 4 5 6 Type |Protocole| Port Range | Source | #comment __________________________________________________________________________________________ HTTP | TCP | 80 | 0.0.0.0/0 | for Galaxy web access SSH | TCP | 22 | 0.0.0.0/0 | for ssh access to the AWS instance Custom TCP Rule | TCP | 21 | 0.0.0.0/0 | for FTP upload to Galaxy Custom TCP Rule | TCP | 49152 - 65534 | 0.0.0.0/0 | for FTP upload to Galaxy The ports 21 and 49152 - 65534 should be open for FTP uploads to the AWS instance, and port 80 should be open for accessing galaxy.","title":"Adapt the ansible inventory file to an Amazon Web Service (AWS) virtual machine"},{"location":"metavisitor/metavisitor_ansible/#adapt-the-group_varsall-file-for-persisting-data-if-needed","text":"In cases where your Target machine has volumes where you wish the Galaxy data to be persisted in, you have to edit the GalaxyKickStart/group_vars/all file, to indicate the path to this volume on the Target machine . If you don't understand the previous statement, no worries, just don't do anything and skip this step. For others, find the lines 1 2 #persistent data galaxy_persistent_directory: /export # for IFB it s /root/mydisk, by default, /export in the GalaxyKickStart/group_vars/all file, and change /export to the path of your persistent volume. Note that if /export is not changed, nothing will happen and the deployed galaxy server and all associated data files will be in the /home/galaxy/galaxy folder of the Target Machine .","title":"Adapt the group_vars/all file for persisting data, if needed."},{"location":"metavisitor/metavisitor_ansible/#deploying-metavisitor-galaxy-on-the-target-machine","text":"You are almost done. Navigate with your terminal to your GalaxyKickStart folder and type the following command to run the ansible playbook for deploying metavisitor Galaxy on the Target Machine : 1 ansible-playbook --inventory-file=hosts galaxy.yml If everything is ok, you may be asked to authorize the access to the Target Machine by typing yes in the terminal, and you will see ansible orchestrating the serveur deployment on the Target Machine in this terminal. When the process is finished, you should be able to access the Target Machine by typing its IP address in your web browser. By default the admin login/password is admin@galaxy.org / admin . You should change the password for safety.","title":"Deploying Metavisitor Galaxy on the Target Machine"},{"location":"metavisitor/metavisitor_ansible/#re-deploying-metavisitor-galaxy-on-the-target-machine","text":"If you are experimented in using ansible, you may customize your Metavisitor Galaxy instance deployed with GalaxyKickStart by editing the content of GalaxyKickStart . In that case, when your changes are done, just run again the command 1 ansible-playbook --inventory-file=hosts galaxy.yml When you run the playbook a second time, the process will be much faster, since steps that have already been executed are skipped. Whenever you change a variable (see customizations ), you need to run the playbook again. You can put multiple machines in your inventory: a simple way to do this is just copying the line the required number of times with the appropriate ip addresses: 1 2 3 4 [metavisitor] 192.54.201.126 ansible_ssh_user = root ansible_ssh_private_key_file= ~/.ssh/id_rsa 192.54.201.127 ansible_ssh_user = root ansible_ssh_private_key_file= ~/.ssh/id_rsa 192.54.201.128 ansible_ssh_user = root ansible_ssh_private_key_file= ~/.ssh/id_rsa","title":"Re-deploying Metavisitor Galaxy on the Target Machine"},{"location":"metavisitor/metavisitor_configure_references/","text":"Once you know how to access to your Metavisitor Galaxy instance with a web browser and are able to perform basic start/stop/restart operations, there is still some work needed to import and configure reference data (reference genomes and databases) so that they are directly available to all Galaxy users for running tools and workflows. Here we provide the step-by-step description of what we did to prepare our Metavisitor instance before performing the analyses described here . 1. Connect to your Metavisitor Galaxy admin account with your web browser 2. Import reference data in an history \"References\" At first, you need to import and prepare the reference datasets you will need for most of the Metavisitor analyses. As a Galaxy admin you will make latter some of these references directly accessible to the Galaxy tools, and/or accessible to any other users by putting them in a Galaxy public library. a. Preliminary actions Click on the Analyze Data menu rename the Unnamed history to References b. Upload nucleotide vir2 fasta file Click on the button on top of the tool bar (left handside of the Galaxy interface) In the open window, click on the Rule-based tab Make sure \"Upload data as:\" is set to datasets and \"Load tabular data from:\" is set to Pasted Table Copy - paste the following table (not including the header) Name URL nucleotide vir2 https://ndownloader.figshare.com/files/11005121 protein vir2 https://ndownloader.figshare.com/files/11005124 dm6 ftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r6.10_FB2016_02/fasta/dmel-all-chromosome-r6.10.fasta.gz AgamP4 https://www.vectorbase.org/sites/default/files/ftp/downloads/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa.gz P. berghei ftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz hg38 ftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz Click on the Build button on the bottom right Click on the + Rules button on the bottom left Select Add / Modify Column Definitions from the list Click the + Add Definition button, then select Name from the list and set column \"A\" as Name column Click the + Add Definition button, select URL and set the \"B\" column as URL column Click the Apply button. Finally click the Upload button in the bottom right The reference genomes should be uploaded shortly to Galaxy. 3. Prepare Blast databases Use the tool NCBI BLAST+ makeblastdb What to set in each form field for nucleotide vir2 protein vir2 Molecule type of input nucleotide protein Input FASTA files(s) dataset 1 (nucleotide vir2) dataset 2 (protein vir2) Title for BLAST database nucleotide vir2 blastdb protein vir2 blastdb Leave the rest of the form unchanged and click the Execute button Rename the generated datasets \" nucleotide vir2 blast database \" and \" protein vir2 blast database \" for clarity 4. Creating Galaxy dbkey and fasta references accessible to tools for every user Be sure that the References history is selected in the background, otherwise the uploaded genomes will not be available. Go to the admin panel Click Local data in the left menu Select the Create DBKey and Reference Genome in the \" Data Managers \" table What to set in each form field for nucleotide vir2 dm6 AgamP4 hg38 Use existing dbkey or create a new one New New New New dbkey vir2 dm6 AgamP4 hg38 Choose the source for the reference genome History History History History FASTA file nucleotide vir2 dm6 AgamP4 hg38 Leave the rest of the fields empty and click the Execute button Tip: Once you have run the first job. You can expand the new dataset that appeared in your history and click on the button, instead of going back to the admin panel. 5. Creating Galaxy bowtie indexes accessible to tools for every user Now we are going to generate the bowtie indexes using another data manager tool. Go to the admin panel Click Local data in the left menu Select the Bowtie index builder in the \" Data Managers \" table Select \" vir2 \" in the \"Source FASTA Sequence\" Leave the other options empty and click the Execute button Expand the \" bowtie index \" dataset that appeared in your history and click the button Repeat the previous 3 steps for \" dm6 \", \" AgamP4 \" and \" hg38 \" Note that the preparation of bowtie indexes can be long (several hours for the vir2 bowtie index for instance) 6. Creating Galaxy bowtie2 indexes accessible to tools for every user Finally, we are going to generate the bowtie2 indexes using another data manager tool. Go to the admin panel Click Local data in the left menu Select the Bowtie2 index builder in the \" Data Managers \" table Select \" vir2 \" in the \"Source FASTA Sequence\" Leave the other options empty and click the Execute button Expand the \" bowtie index \" dataset that appeared in your history and click the button Repeat the previous 3 steps for \" AgamP4 \" and \" hg38 \" Note that the preparation of bowtie2 indexes can be long too (several hours for the vir2 bowtie2 index for instance)","title":"Prepare Metavisitor Galaxy instance for analyses"},{"location":"metavisitor/metavisitor_configure_references/#1-connect-to-your-metavisitor-galaxy-admin-account-with-your-web-browser","text":"","title":"1. Connect to your Metavisitor Galaxy admin account with your web browser"},{"location":"metavisitor/metavisitor_configure_references/#2-import-reference-data-in-an-history-references","text":"At first, you need to import and prepare the reference datasets you will need for most of the Metavisitor analyses. As a Galaxy admin you will make latter some of these references directly accessible to the Galaxy tools, and/or accessible to any other users by putting them in a Galaxy public library.","title":"2. Import reference data in an history \"References\""},{"location":"metavisitor/metavisitor_configure_references/#a-preliminary-actions","text":"Click on the Analyze Data menu rename the Unnamed history to References","title":"a. Preliminary actions"},{"location":"metavisitor/metavisitor_configure_references/#b-upload-nucleotide-vir2-fasta-file","text":"Click on the button on top of the tool bar (left handside of the Galaxy interface) In the open window, click on the Rule-based tab Make sure \"Upload data as:\" is set to datasets and \"Load tabular data from:\" is set to Pasted Table Copy - paste the following table (not including the header) Name URL nucleotide vir2 https://ndownloader.figshare.com/files/11005121 protein vir2 https://ndownloader.figshare.com/files/11005124 dm6 ftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r6.10_FB2016_02/fasta/dmel-all-chromosome-r6.10.fasta.gz AgamP4 https://www.vectorbase.org/sites/default/files/ftp/downloads/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa.gz P. berghei ftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz hg38 ftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz Click on the Build button on the bottom right Click on the + Rules button on the bottom left Select Add / Modify Column Definitions from the list Click the + Add Definition button, then select Name from the list and set column \"A\" as Name column Click the + Add Definition button, select URL and set the \"B\" column as URL column Click the Apply button. Finally click the Upload button in the bottom right The reference genomes should be uploaded shortly to Galaxy.","title":"b. Upload nucleotide vir2 fasta file"},{"location":"metavisitor/metavisitor_configure_references/#3-prepare-blast-databases","text":"Use the tool NCBI BLAST+ makeblastdb What to set in each form field for nucleotide vir2 protein vir2 Molecule type of input nucleotide protein Input FASTA files(s) dataset 1 (nucleotide vir2) dataset 2 (protein vir2) Title for BLAST database nucleotide vir2 blastdb protein vir2 blastdb Leave the rest of the form unchanged and click the Execute button Rename the generated datasets \" nucleotide vir2 blast database \" and \" protein vir2 blast database \" for clarity","title":"3. Prepare Blast databases"},{"location":"metavisitor/metavisitor_configure_references/#4-creating-galaxy-dbkey-and-fasta-references-accessible-to-tools-for-every-user","text":"Be sure that the References history is selected in the background, otherwise the uploaded genomes will not be available. Go to the admin panel Click Local data in the left menu Select the Create DBKey and Reference Genome in the \" Data Managers \" table What to set in each form field for nucleotide vir2 dm6 AgamP4 hg38 Use existing dbkey or create a new one New New New New dbkey vir2 dm6 AgamP4 hg38 Choose the source for the reference genome History History History History FASTA file nucleotide vir2 dm6 AgamP4 hg38 Leave the rest of the fields empty and click the Execute button Tip: Once you have run the first job. You can expand the new dataset that appeared in your history and click on the button, instead of going back to the admin panel.","title":"4. Creating Galaxy dbkey and fasta references accessible to tools for every user"},{"location":"metavisitor/metavisitor_configure_references/#5-creating-galaxy-bowtie-indexes-accessible-to-tools-for-every-user","text":"Now we are going to generate the bowtie indexes using another data manager tool. Go to the admin panel Click Local data in the left menu Select the Bowtie index builder in the \" Data Managers \" table Select \" vir2 \" in the \"Source FASTA Sequence\" Leave the other options empty and click the Execute button Expand the \" bowtie index \" dataset that appeared in your history and click the button Repeat the previous 3 steps for \" dm6 \", \" AgamP4 \" and \" hg38 \" Note that the preparation of bowtie indexes can be long (several hours for the vir2 bowtie index for instance)","title":"5. Creating Galaxy bowtie indexes accessible to tools for every user"},{"location":"metavisitor/metavisitor_configure_references/#6-creating-galaxy-bowtie2-indexes-accessible-to-tools-for-every-user","text":"Finally, we are going to generate the bowtie2 indexes using another data manager tool. Go to the admin panel Click Local data in the left menu Select the Bowtie2 index builder in the \" Data Managers \" table Select \" vir2 \" in the \"Source FASTA Sequence\" Leave the other options empty and click the Execute button Expand the \" bowtie index \" dataset that appeared in your history and click the button Repeat the previous 3 steps for \" AgamP4 \" and \" hg38 \" Note that the preparation of bowtie2 indexes can be long too (several hours for the vir2 bowtie2 index for instance)","title":"6. Creating Galaxy bowtie2 indexes accessible to tools for every user"},{"location":"metavisitor/metavisitor_docker/","text":"Installing Metavisitor with Docker We distribute a docker image of Metavisitor, which can thus be used to run a Metavisitor docker container. For a quick start, go directly to the last section \"Persisting to disk\". Requirements You need to have docker installed and configured for your user. Running images from the dockerhub You can search for pre-built docker images from the dockerhub by typing in the terminal of the machine where you want to run the docker container: 1 docker search metavisitor Then, to get the docker image, type: 1 docker pull artbio/metavisitor-2 In this documentation, we recommend to use the artbio/metavisitor-2 which better corresponds to the environment described in our Metavisitor preprint When this pull is done (may take a few minutes depending on your connection speed to the dockerhub), you can start the container by typing: 1 docker run -d -p 80:80 artbio/metavisitor-2 This command starts a container in daemon mode ( -d ) from the image and serve it on port 80 of the local machine in the standard docker way. -p 80:80 forwards requests to nginx inside the container running on port 80. If you want to access the machine hosting the running container through another port (for instance 8080), just change -p 80:80 to -p 8080:80 Runtime changes to pre-built docker images If you wish to reach the container on a subdirectory, add -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" to the docker call. For instance, 1 docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory -p 80:80 artbio/metavisitor-2 will get the metavisitor docker container serving at http://127.0.0.1:80/my-subdirectory . We recommend also changing the default admin user as well, so the command becomes: 1 docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 80:80 artbio/galaxy-kickstart-base Note that if you do not make this latest change, the admin login for the metavisitor container is by default admin@galaxy.org and the password is admin . Persisting to disk All changes made to a docker container are by default ephemeral; if you remove the container, the changes are gone. To persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into the containers /export folder. Due to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container. Thus, assuming you would like to mount your local /my/data folder and persist you Galaxy data in this folder, run 1 docker run -d --privileged -v /my/data:/export -p 80:80 artbio/metavisitor-2 This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /my/data). From the new location the files are being bind-mounted back into their original location.","title":"Docker"},{"location":"metavisitor/metavisitor_docker/#installing-metavisitor-with-docker","text":"We distribute a docker image of Metavisitor, which can thus be used to run a Metavisitor docker container. For a quick start, go directly to the last section \"Persisting to disk\".","title":"Installing Metavisitor with Docker"},{"location":"metavisitor/metavisitor_docker/#requirements","text":"You need to have docker installed and configured for your user.","title":"Requirements"},{"location":"metavisitor/metavisitor_docker/#running-images-from-the-dockerhub","text":"You can search for pre-built docker images from the dockerhub by typing in the terminal of the machine where you want to run the docker container: 1 docker search metavisitor Then, to get the docker image, type: 1 docker pull artbio/metavisitor-2 In this documentation, we recommend to use the artbio/metavisitor-2 which better corresponds to the environment described in our Metavisitor preprint When this pull is done (may take a few minutes depending on your connection speed to the dockerhub), you can start the container by typing: 1 docker run -d -p 80:80 artbio/metavisitor-2 This command starts a container in daemon mode ( -d ) from the image and serve it on port 80 of the local machine in the standard docker way. -p 80:80 forwards requests to nginx inside the container running on port 80. If you want to access the machine hosting the running container through another port (for instance 8080), just change -p 80:80 to -p 8080:80","title":"Running images from the dockerhub"},{"location":"metavisitor/metavisitor_docker/#runtime-changes-to-pre-built-docker-images","text":"If you wish to reach the container on a subdirectory, add -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" to the docker call. For instance, 1 docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory -p 80:80 artbio/metavisitor-2 will get the metavisitor docker container serving at http://127.0.0.1:80/my-subdirectory . We recommend also changing the default admin user as well, so the command becomes: 1 docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 80:80 artbio/galaxy-kickstart-base Note that if you do not make this latest change, the admin login for the metavisitor container is by default admin@galaxy.org and the password is admin .","title":"Runtime changes to pre-built docker images"},{"location":"metavisitor/metavisitor_docker/#persisting-to-disk","text":"All changes made to a docker container are by default ephemeral; if you remove the container, the changes are gone. To persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into the containers /export folder. Due to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container. Thus, assuming you would like to mount your local /my/data folder and persist you Galaxy data in this folder, run 1 docker run -d --privileged -v /my/data:/export -p 80:80 artbio/metavisitor-2 This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /my/data). From the new location the files are being bind-mounted back into their original location.","title":"Persisting to disk"},{"location":"metavisitor/use_case_1/","text":"Histories for Use Cases 1-1, 1-2, 1-3 and 1-4 As you will see, Histories 1-1, 1-2 and 1-3 are generated in the same way, using their corresponding workflows. These workflows are available in your Galaxy top menu. An important thing to remember is that you will always start from the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history, run the appropriate workflow, sending the outputs of the workflow in a new history named accordingly. History for Use Case 1-1. 1. As aforementioned, ensure that you are in the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history. You can always control this by using the top menu Users Saved History and selecting the desired history. If you don't see the History right bar, just click in addition the top menu Analyze Data 2. Select the appropriate workflow Click now on the Workflow top menu Select the workflow \"Metavisitor: Workflow for Use Case 1-1\" and to see the workflow, select the submenu \"Edit\" Now that you see the workflow, you can directly execute it by clicking the top right wheel icon and selecting \"Run\" 3. Select the appropriate parameters before running the workflow You now see a page with all the workflow steps, whose top part looks like: As pointed by the first red arrow, a parameter has to be provided at runtime of the workflow: the ncbi_guide_ID . In this Use Case as in the other 1-2 and 1-3 Use Cases, you will paste in the ncbi_guide_ID field the NC_007919.3_ value. This is the NCBI identifier for the Nora virus genome sequence which will be retrieved from Genbank during the workflow and used as a guide for the final reconstruction of the Nora virus genome sequence that is \"present\" in the analyzed small RNA sequencing datasets. You have to select an Input dataset collection for Step 1 (second red arrow). However, as there is only one dataset collection in the input history (the one we have prepared in the previous chapter ), there is no other option in the menu than \" SRP013822 \". You have to select the viral nucleotide Blast database for Step 2. Here again there is indeed nothing else to select than the \" nucleotide vir2 blast database \", just because there is only one dataset in the input history with the \"blast database\" type. You can review the other steps of the workflow. But there is no other selection to perform before running the workflow. 4. Running the workflow sending the outputs in a new history We are almost ready, but before clicking the \" Run Workflow \" button there is an important thing to do: - Check the \" Send results to a new history \" checkbox as shown Here And edit the field to \"History for Use Case 1-1\" You can now click the \"Run workflow\" button. This trigger the workflow run. After a few seconds (may be take a while for complex workflows), you will see an alert that the workflow is started, and a link to navigate to the newly created history. When the workflow has finished, if you navigate to the created \"History for Use Case 1-1\", you should see: Note that 30 datasets have been hidden by the workflow for clarity. You just have to click on the \"hidden\" link to unhide these datasets Histories for Use Cases 1-2 and 1-3 Histories for Uses Cases 1-2 and 1-3 are produced in almost the same way as History for Use Case 1-1. Do exactly as described for Use Case 1-1 and - Remember to go back to the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history and be sure you are going to run the workflow from that history. - Select the appropriate workflow ! - Remember to Check the \" Send results to a new history \" checkbox, rename the new history appropriately before pressing the \" Run workflow \" button History for remapping in Use Cases 1-1,2,3 Before running the workflow for remapping in Use Cases 1-1,2,3, we need to collect datasets generated in the histories for Use Case 1-1, 1-2 and 1-3 and send them in our Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history. Update the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history This is because the purpose of the workflow for Use Case 1-4 is to remap the raw read sequencing datasets to the viral genomes generated in the previous histories as well as to 2 different Nora virus genomes deposited in Genbank (NC_007919.3 and JX220408). Thus, go back to the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history and Use the Retrieve FASTA from NCBI Metavisitor tool to retrieve the NC_007919.3 sequence. Use the Regex Find And Replace tool on the Retrieve FASTA from NCBI (Nucleotide) with queryString 'NC_007919.3' dataset as an input, and put .+ as Find Regex parameter and NC_007919.3 as Replacement parameter. This is just to change the header of the FASTA file and make it more readable. Rename the generated dataset NC_007919.3 for clarity. Use the Retrieve FASTA from NCBI Metavisitor tool to retrieve the JX220408 sequence. Use the Regex Find And Replace tool on the Retrieve FASTA from NCBI (Nucleotide) with queryString 'JX220408' dataset as an input, and put .+ as Find Regex parameter and JX220408.1 as Replacement parameter. Rename the generated dataset JX220408.1 for clarity. Click on the top wheel history icon, select Copy Datasets ; select \"History for Use Case 1-1\" as a Source History, click on the last dataset of the history (Nora_MV_NC_007919.3_guided), select \"Input data for Use Cases 1-1, 1-2...\" as Destination History, and click \"Copy History Items\". Repeat the previous operation for History for Use Case 1-2, selecting the last \"Nora_raw_reads_NC_007919.3_guided\" dataset. And Repeat the previous operation for History for Use Case 1-3, selecting the last \"Nora_Median-Norm-reads_NC_007919.3_guided\" dataset. You may have to refresh your Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history to reveal the three copied datasets The last step is to create a dataset collection with the 5 Nora virus genomes that you have now in your input data history: just click on the checkbox icon at the top of the history bar, select the 5 corresponding data sets (NC_007919.3, JX220408.1, Nora_MV_NC_007919.3_guided, Nora_raw_reads_NC_007919.3_guided and Nora_Median-Norm-reads_NC_007919.3_guided), click on the For all selected... button, select Build Dataset List , name this list \"Nora virus genomes\", and press the Create list button. We are done with the input data history update ! Generate the History for remapping in Use Cases 1-1,2,3 In the workflow top menu of Galaxy, select the Metavisitor: Workflow for remapping in Use Cases 1-1,2,3 workflow and directly select the run option (you may also look at the workflow before by selection the edit option). Specify \"SRP013822\" for the step 1 option Specify \"Nora virus genomes\" for the step 2 option (you see now why we had to create a dataset collection) Click at the bottom the checkbox Send results to a new history Edit the field that shows up by typing in it: \" History for remapping in Use Cases 1-1,2,3 \" Execute the workflow by clicking the Run workflow button. After few seconds, you may follow the link to the new history running ! History for remapping in Use Case 1-4 This is a simple history to generate because basically, it is similar to the History for Use Case 1-1, but with a slightly modified (and simplified workflow). Navigate back again to your \"base\" history Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 Now the sequence of operations to be performed should be more familiar to you: Top menu Workflow Select Metavisitor: Workflow for Use Case 1-4 and the run option Step 1 (Input Dataset Collection), select the SRP013822 option Step 2 (viral nucleotide BLAST database), select nucleotide vir2 blast database (forced option if everything went well - only one blast database is available in this input history) Click at the bottom the checkbox Send results to a new history Edit the field that shows up by typing in it: \" History for Use Case 1-4 \" Execute the workflow by clicking the Run workflow button. After few seconds, you may follow the link to the new \" History for Use Case 1-4 \" running !","title":"Use Cases 1-1 to 1-4"},{"location":"metavisitor/use_case_1/#histories-for-use-cases-1-1-1-2-1-3-and-1-4","text":"As you will see, Histories 1-1, 1-2 and 1-3 are generated in the same way, using their corresponding workflows. These workflows are available in your Galaxy top menu. An important thing to remember is that you will always start from the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history, run the appropriate workflow, sending the outputs of the workflow in a new history named accordingly.","title":"Histories for Use Cases 1-1, 1-2, 1-3 and 1-4"},{"location":"metavisitor/use_case_1/#history-for-use-case-1-1","text":"","title":"History for Use Case 1-1."},{"location":"metavisitor/use_case_1/#1-as-aforementioned-ensure-that-you-are-in-the-input-data-for-use-cases-1-1-1-2-1-3-and-1-4-history","text":"You can always control this by using the top menu Users Saved History and selecting the desired history. If you don't see the History right bar, just click in addition the top menu Analyze Data","title":"1. As aforementioned, ensure that you are in the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history."},{"location":"metavisitor/use_case_1/#2-select-the-appropriate-workflow","text":"Click now on the Workflow top menu Select the workflow \"Metavisitor: Workflow for Use Case 1-1\" and to see the workflow, select the submenu \"Edit\" Now that you see the workflow, you can directly execute it by clicking the top right wheel icon and selecting \"Run\"","title":"2. Select the appropriate workflow"},{"location":"metavisitor/use_case_1/#3-select-the-appropriate-parameters-before-running-the-workflow","text":"You now see a page with all the workflow steps, whose top part looks like: As pointed by the first red arrow, a parameter has to be provided at runtime of the workflow: the ncbi_guide_ID . In this Use Case as in the other 1-2 and 1-3 Use Cases, you will paste in the ncbi_guide_ID field the NC_007919.3_ value. This is the NCBI identifier for the Nora virus genome sequence which will be retrieved from Genbank during the workflow and used as a guide for the final reconstruction of the Nora virus genome sequence that is \"present\" in the analyzed small RNA sequencing datasets. You have to select an Input dataset collection for Step 1 (second red arrow). However, as there is only one dataset collection in the input history (the one we have prepared in the previous chapter ), there is no other option in the menu than \" SRP013822 \". You have to select the viral nucleotide Blast database for Step 2. Here again there is indeed nothing else to select than the \" nucleotide vir2 blast database \", just because there is only one dataset in the input history with the \"blast database\" type. You can review the other steps of the workflow. But there is no other selection to perform before running the workflow.","title":"3. Select the appropriate parameters before running the workflow"},{"location":"metavisitor/use_case_1/#4-running-the-workflow-sending-the-outputs-in-a-new-history","text":"We are almost ready, but before clicking the \" Run Workflow \" button there is an important thing to do: - Check the \" Send results to a new history \" checkbox as shown Here And edit the field to \"History for Use Case 1-1\" You can now click the \"Run workflow\" button. This trigger the workflow run. After a few seconds (may be take a while for complex workflows), you will see an alert that the workflow is started, and a link to navigate to the newly created history. When the workflow has finished, if you navigate to the created \"History for Use Case 1-1\", you should see: Note that 30 datasets have been hidden by the workflow for clarity. You just have to click on the \"hidden\" link to unhide these datasets","title":"4. Running the workflow sending the outputs in a new history"},{"location":"metavisitor/use_case_1/#histories-for-use-cases-1-2-and-1-3","text":"Histories for Uses Cases 1-2 and 1-3 are produced in almost the same way as History for Use Case 1-1. Do exactly as described for Use Case 1-1 and - Remember to go back to the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history and be sure you are going to run the workflow from that history. - Select the appropriate workflow ! - Remember to Check the \" Send results to a new history \" checkbox, rename the new history appropriately before pressing the \" Run workflow \" button","title":"Histories for Use Cases 1-2 and 1-3"},{"location":"metavisitor/use_case_1/#history-for-remapping-in-use-cases-1-123","text":"Before running the workflow for remapping in Use Cases 1-1,2,3, we need to collect datasets generated in the histories for Use Case 1-1, 1-2 and 1-3 and send them in our Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history.","title":"History for remapping in Use Cases 1-1,2,3"},{"location":"metavisitor/use_case_1/#update-the-input-data-for-use-cases-1-1-1-2-1-3-and-1-4-history","text":"This is because the purpose of the workflow for Use Case 1-4 is to remap the raw read sequencing datasets to the viral genomes generated in the previous histories as well as to 2 different Nora virus genomes deposited in Genbank (NC_007919.3 and JX220408). Thus, go back to the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history and Use the Retrieve FASTA from NCBI Metavisitor tool to retrieve the NC_007919.3 sequence. Use the Regex Find And Replace tool on the Retrieve FASTA from NCBI (Nucleotide) with queryString 'NC_007919.3' dataset as an input, and put .+ as Find Regex parameter and NC_007919.3 as Replacement parameter. This is just to change the header of the FASTA file and make it more readable. Rename the generated dataset NC_007919.3 for clarity. Use the Retrieve FASTA from NCBI Metavisitor tool to retrieve the JX220408 sequence. Use the Regex Find And Replace tool on the Retrieve FASTA from NCBI (Nucleotide) with queryString 'JX220408' dataset as an input, and put .+ as Find Regex parameter and JX220408.1 as Replacement parameter. Rename the generated dataset JX220408.1 for clarity. Click on the top wheel history icon, select Copy Datasets ; select \"History for Use Case 1-1\" as a Source History, click on the last dataset of the history (Nora_MV_NC_007919.3_guided), select \"Input data for Use Cases 1-1, 1-2...\" as Destination History, and click \"Copy History Items\". Repeat the previous operation for History for Use Case 1-2, selecting the last \"Nora_raw_reads_NC_007919.3_guided\" dataset. And Repeat the previous operation for History for Use Case 1-3, selecting the last \"Nora_Median-Norm-reads_NC_007919.3_guided\" dataset. You may have to refresh your Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history to reveal the three copied datasets The last step is to create a dataset collection with the 5 Nora virus genomes that you have now in your input data history: just click on the checkbox icon at the top of the history bar, select the 5 corresponding data sets (NC_007919.3, JX220408.1, Nora_MV_NC_007919.3_guided, Nora_raw_reads_NC_007919.3_guided and Nora_Median-Norm-reads_NC_007919.3_guided), click on the For all selected... button, select Build Dataset List , name this list \"Nora virus genomes\", and press the Create list button. We are done with the input data history update !","title":"Update the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history"},{"location":"metavisitor/use_case_1/#generate-the-history-for-remapping-in-use-cases-1-123","text":"In the workflow top menu of Galaxy, select the Metavisitor: Workflow for remapping in Use Cases 1-1,2,3 workflow and directly select the run option (you may also look at the workflow before by selection the edit option). Specify \"SRP013822\" for the step 1 option Specify \"Nora virus genomes\" for the step 2 option (you see now why we had to create a dataset collection) Click at the bottom the checkbox Send results to a new history Edit the field that shows up by typing in it: \" History for remapping in Use Cases 1-1,2,3 \" Execute the workflow by clicking the Run workflow button. After few seconds, you may follow the link to the new history running !","title":"Generate the History for remapping in Use Cases 1-1,2,3"},{"location":"metavisitor/use_case_1/#history-for-remapping-in-use-case-1-4","text":"This is a simple history to generate because basically, it is similar to the History for Use Case 1-1, but with a slightly modified (and simplified workflow).","title":"History for remapping in Use Case 1-4"},{"location":"metavisitor/use_case_1/#navigate-back-again-to-your-base-history-input-data-for-use-cases-1-1-1-2-1-3-and-1-4","text":"Now the sequence of operations to be performed should be more familiar to you: Top menu Workflow Select Metavisitor: Workflow for Use Case 1-4 and the run option Step 1 (Input Dataset Collection), select the SRP013822 option Step 2 (viral nucleotide BLAST database), select nucleotide vir2 blast database (forced option if everything went well - only one blast database is available in this input history) Click at the bottom the checkbox Send results to a new history Edit the field that shows up by typing in it: \" History for Use Case 1-4 \" Execute the workflow by clicking the Run workflow button. After few seconds, you may follow the link to the new \" History for Use Case 1-4 \" running !","title":"Navigate back again to your \"base\" history Input data for Use Cases 1-1, 1-2, 1-3 and 1-4"},{"location":"metavisitor/use_case_2/","text":"Histories for Use Cases 2-1, 2-2 Now that are more familiar with manipulations in Galaxy with the Use Cases 1-1 to 1-4, we will describe the other Use Case analyses more concisely. If you experience lack of skills in basic Galaxy operations (tool usage, copy of datasets, etc), do not hesitate to go back and examine the previous chapters step by step. Discovery of novel viruses Here we are going to use Metavisitor workflows to discover new viruses infecting a laboratory colony of Anopheles coluzzii mosquitoes. Workflow for Use Case 2-1: Takes reads from EBI SRA ERP012577 (small RNAs), assembles contigs, blastx them against vir2 , selects contigs hitting Dicistroviridae proteins, re-assembles selected contigs to align them to the Drosophila C virus (DCV) genome and integrates them to its sequence Worflow for Use Case 2-2: Takes reads from EBI SRA ERS977505 (mRNA), assembles contigs, and blastx them against vir2 Input data for Use Cases 2-1 and 2-2 As for the previous Use Case 1, the first step is to collect all input data in an history that we will name Input data for Use Cases 2-1 and 2-2 Create a new history Rename this history Input data for Use Cases 2-1 and 2-2 For the small RNA sequence datasets (ERP012577) in this study, we are going to use another tool to upload to the Galaxy Metavisitor server: the EBI SRA ENA SRA tool which in the \"Get data\" section of the left tool bar. Click on this tool and enter ERP012577 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the ERP012577 link. In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar). Repeat the exact same operation, for the three other \"fastq file 1\". At final you should have uploaded four fastq datasets corresponding to the sequencing runs \"post_infected_rep1.fastq\", \"post_infected_rep2.fastq\", \"post_non-infected_rep1.fastq\" and \"post_non-infected_rep2.fastq\" Select a dataset to make sure its datatype is fastqsanger.gz . If not click on the crayon button of any one of these four datasets and select the Datatypes tab and set it to fastqsanger.gz . Repeat this operation for the other 3 datasets. Create a dataset collection as previously explained (step 9) and name it Small RNA reads ERP012577 For the RNA sequence datasets (ERS977505) that will be used in Use Case 2-2, use again the EBI SRA ENA SRA tool which in the \"Get data\" section of the left tool bar. Click on this tool and enter ERS977505 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the ERS977505 link (Sample 1 result found). In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar). Repeat the exact same operation for the other \"fastq file 1\" and the two other \"fastq file 2\" In the end you should have uploaded four additional fastq datasets corresponding to the sequencing runs \"IP-isoT-1_AGTCAA_L001_R_1.fastq\", \"IP-isoT-1_AGTCAA_L001_R_2.fastq\", \"IP-isoT-2_ATGTCA_L002_R_1.fastq\" and \"IP-isoT-2_ATGTCA_L002_R_2.fastq\" Create a dataset collection as explained in the previous chapter and name it long read RNAseq datasets . Note that we are not handling the files as paired-ends, thus use the Build dataset list command and not the Build list of dataset pairs command. Use the Retrieve FASTA from NCBI , paste phix174[title] in the \"Query to NCBI in entrez format\" field and select Nucleotide for the NCBI database. This will upload 154 fasta sequences from phix174. Use the wheel icon at the top of the history bar to copy nucleotide vir2 blast database , protein vir2 blast database and P. berghei from the history References to the current history Input data for Use Cases 2-1 and 2-2 . If you don't remember well how to copy datasets between histories, you may read again the explanation here (step 3) Your are now ready for generating Uses Cases 2-1 and 2-2 History for Use Case 2-1 Stay in the current history Input data for Use Cases 2-1 and 2-2 ! In the Workflow menu, select the workflow Metavisitor: Workflow for Use Case 2-1 and directly select Run (you may also look at the workflow using the edit option). Be careful at selecting Small RNA reads ERP012577 in step 1 (Input Dataset Collection). Be careful in selecting P. berghei in step 2 . Be careful in step 3 select : 1 Retrieve FASTA from NCBI ( Nucleotide ) with queryString phix174[title] In step 4, the option protein vir2 blast database is forced, because the workflow is expecting of protein blast database in this step and only one dataset with this datatype is available in the history Click the Send results to a new history checkbox and rename the history to \"History for Use Case 2-1\". Run Workflow ! You may follow the link to the new history when the workflow has started. History for Use Case 2-2 If you are not already in, go back to the history Input data for Use Cases 2-1 and 2-2 In the Workflow menu, select the workflow Metavisitor: Workflow for Use Case 2-2 and directly select Run (you may also look at the workflow using the edit option) Be careful at selecting long read RNAseq datasets in step 1 (Input Dataset Collection) In step 2, the option protein vir2 blast database is forced, because the workflow is expecting of protein blast database in this step and only one dataset with this datatype is available in the history Click the Send results to a new history checkbox and rename the history to \"History for Use Case 2-2\". Run Workflow. Re-mapping of the small RNA reads (ERP012577) to the AnCV genome (KU169878). The previous Workflow for Use Case 2-2 allowed to assemble a large contig of 8919 nt which significantly matched structural and non-structural polyproteins of Drosophila C Virus and Cricket Paralysis Virus in blastx alignments (see the dataset blastx Filter sequences by length on data 17 vs 'protein BLAST database from data 2' of the history). This large contig corresponds to the genome of a new Anopheles C Virus deposited to the NCBI nucleotide database under accession number KU169878 (see the companion Metavisitor article and Carissimo et al ). Here, we are going to perform manually a few steps, before using another workflow in the history 2-2 to remap the ERP012577 small RNA reads to the AnCV genome. Look at the blast analysis, by subjects dataset and copy the name of the 8919 nt contig that aligned to DCV and CrPV sequences. It is noteworthy that the names may vary from one Oases run to another because the Oases algorithm is not totally deterministic. In the companion Metavisitor article , this name was Locus_69_Transcript_1/1_Confidence_0.000_Length_8919. Copy this name, find the tool Pick Fasta sequences with header satisfying a query string in the Galaxy tool bar, and paste the name in the field Select sequences with this string in their header of the tool form. Select the dataset Oases_optimiser on data 21: Denovo assembled transcripts as a source file, and run the tool. Now, we are going to change the header of the previously extracted fasta sequences using the tool Regex Find And Replace . Select the previous dataset Concatenated datasets as input dataset for this tool. Click on + Insert Check . Use .*Confidence(.*)_Length_8919 (or the equivalent you extracted) as Find Regex and Anopheles_C_Virus|KU169878_confidence\\1 as Replacement . Copy the dataset collection Small RNA reads ERP012577 from the history Input data for Use Cases 2-1 and 2-2 into the current history Use Case 2-2 . You may have to refresh the history bar to see this collection and the attached datasets popping up. We are now ready to run the workflow. In the workflow menu, pick up the workflow Metavisitor: Workflow for remapping in Use Cases 2-1,2 and select the run option. In the workflow form, ensure that Small RNA reads ERP012577 are selected in step 1 and Regex Find And Replace on data 28 is selected in step 2 (this should be the case if you followed the instructions). This time, do not check the box Send results to a new history and directly click the Run workflow button. This workflow will provide you with a graphical view of ERP012577 small RNA mapping to the AnCV genome.","title":"Use Cases 2-1 and 2-2"},{"location":"metavisitor/use_case_2/#histories-for-use-cases-2-1-2-2","text":"Now that are more familiar with manipulations in Galaxy with the Use Cases 1-1 to 1-4, we will describe the other Use Case analyses more concisely. If you experience lack of skills in basic Galaxy operations (tool usage, copy of datasets, etc), do not hesitate to go back and examine the previous chapters step by step.","title":"Histories for Use Cases 2-1, 2-2"},{"location":"metavisitor/use_case_2/#discovery-of-novel-viruses","text":"Here we are going to use Metavisitor workflows to discover new viruses infecting a laboratory colony of Anopheles coluzzii mosquitoes. Workflow for Use Case 2-1: Takes reads from EBI SRA ERP012577 (small RNAs), assembles contigs, blastx them against vir2 , selects contigs hitting Dicistroviridae proteins, re-assembles selected contigs to align them to the Drosophila C virus (DCV) genome and integrates them to its sequence Worflow for Use Case 2-2: Takes reads from EBI SRA ERS977505 (mRNA), assembles contigs, and blastx them against vir2","title":"Discovery of novel viruses"},{"location":"metavisitor/use_case_2/#input-data-for-use-cases-2-1-and-2-2","text":"As for the previous Use Case 1, the first step is to collect all input data in an history that we will name Input data for Use Cases 2-1 and 2-2 Create a new history Rename this history Input data for Use Cases 2-1 and 2-2 For the small RNA sequence datasets (ERP012577) in this study, we are going to use another tool to upload to the Galaxy Metavisitor server: the EBI SRA ENA SRA tool which in the \"Get data\" section of the left tool bar. Click on this tool and enter ERP012577 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the ERP012577 link. In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar). Repeat the exact same operation, for the three other \"fastq file 1\". At final you should have uploaded four fastq datasets corresponding to the sequencing runs \"post_infected_rep1.fastq\", \"post_infected_rep2.fastq\", \"post_non-infected_rep1.fastq\" and \"post_non-infected_rep2.fastq\" Select a dataset to make sure its datatype is fastqsanger.gz . If not click on the crayon button of any one of these four datasets and select the Datatypes tab and set it to fastqsanger.gz . Repeat this operation for the other 3 datasets. Create a dataset collection as previously explained (step 9) and name it Small RNA reads ERP012577 For the RNA sequence datasets (ERS977505) that will be used in Use Case 2-2, use again the EBI SRA ENA SRA tool which in the \"Get data\" section of the left tool bar. Click on this tool and enter ERS977505 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the ERS977505 link (Sample 1 result found). In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar). Repeat the exact same operation for the other \"fastq file 1\" and the two other \"fastq file 2\" In the end you should have uploaded four additional fastq datasets corresponding to the sequencing runs \"IP-isoT-1_AGTCAA_L001_R_1.fastq\", \"IP-isoT-1_AGTCAA_L001_R_2.fastq\", \"IP-isoT-2_ATGTCA_L002_R_1.fastq\" and \"IP-isoT-2_ATGTCA_L002_R_2.fastq\" Create a dataset collection as explained in the previous chapter and name it long read RNAseq datasets . Note that we are not handling the files as paired-ends, thus use the Build dataset list command and not the Build list of dataset pairs command. Use the Retrieve FASTA from NCBI , paste phix174[title] in the \"Query to NCBI in entrez format\" field and select Nucleotide for the NCBI database. This will upload 154 fasta sequences from phix174. Use the wheel icon at the top of the history bar to copy nucleotide vir2 blast database , protein vir2 blast database and P. berghei from the history References to the current history Input data for Use Cases 2-1 and 2-2 . If you don't remember well how to copy datasets between histories, you may read again the explanation here (step 3) Your are now ready for generating Uses Cases 2-1 and 2-2","title":"Input data for Use Cases 2-1 and 2-2"},{"location":"metavisitor/use_case_2/#history-for-use-case-2-1","text":"Stay in the current history Input data for Use Cases 2-1 and 2-2 ! In the Workflow menu, select the workflow Metavisitor: Workflow for Use Case 2-1 and directly select Run (you may also look at the workflow using the edit option). Be careful at selecting Small RNA reads ERP012577 in step 1 (Input Dataset Collection). Be careful in selecting P. berghei in step 2 . Be careful in step 3 select : 1 Retrieve FASTA from NCBI ( Nucleotide ) with queryString phix174[title] In step 4, the option protein vir2 blast database is forced, because the workflow is expecting of protein blast database in this step and only one dataset with this datatype is available in the history Click the Send results to a new history checkbox and rename the history to \"History for Use Case 2-1\". Run Workflow ! You may follow the link to the new history when the workflow has started.","title":"History for Use Case 2-1"},{"location":"metavisitor/use_case_2/#history-for-use-case-2-2","text":"If you are not already in, go back to the history Input data for Use Cases 2-1 and 2-2 In the Workflow menu, select the workflow Metavisitor: Workflow for Use Case 2-2 and directly select Run (you may also look at the workflow using the edit option) Be careful at selecting long read RNAseq datasets in step 1 (Input Dataset Collection) In step 2, the option protein vir2 blast database is forced, because the workflow is expecting of protein blast database in this step and only one dataset with this datatype is available in the history Click the Send results to a new history checkbox and rename the history to \"History for Use Case 2-2\". Run Workflow.","title":"History for Use Case 2-2"},{"location":"metavisitor/use_case_2/#re-mapping-of-the-small-rna-reads-erp012577-to-the-ancv-genome-ku169878","text":"The previous Workflow for Use Case 2-2 allowed to assemble a large contig of 8919 nt which significantly matched structural and non-structural polyproteins of Drosophila C Virus and Cricket Paralysis Virus in blastx alignments (see the dataset blastx Filter sequences by length on data 17 vs 'protein BLAST database from data 2' of the history). This large contig corresponds to the genome of a new Anopheles C Virus deposited to the NCBI nucleotide database under accession number KU169878 (see the companion Metavisitor article and Carissimo et al ). Here, we are going to perform manually a few steps, before using another workflow in the history 2-2 to remap the ERP012577 small RNA reads to the AnCV genome. Look at the blast analysis, by subjects dataset and copy the name of the 8919 nt contig that aligned to DCV and CrPV sequences. It is noteworthy that the names may vary from one Oases run to another because the Oases algorithm is not totally deterministic. In the companion Metavisitor article , this name was Locus_69_Transcript_1/1_Confidence_0.000_Length_8919. Copy this name, find the tool Pick Fasta sequences with header satisfying a query string in the Galaxy tool bar, and paste the name in the field Select sequences with this string in their header of the tool form. Select the dataset Oases_optimiser on data 21: Denovo assembled transcripts as a source file, and run the tool. Now, we are going to change the header of the previously extracted fasta sequences using the tool Regex Find And Replace . Select the previous dataset Concatenated datasets as input dataset for this tool. Click on + Insert Check . Use .*Confidence(.*)_Length_8919 (or the equivalent you extracted) as Find Regex and Anopheles_C_Virus|KU169878_confidence\\1 as Replacement . Copy the dataset collection Small RNA reads ERP012577 from the history Input data for Use Cases 2-1 and 2-2 into the current history Use Case 2-2 . You may have to refresh the history bar to see this collection and the attached datasets popping up. We are now ready to run the workflow. In the workflow menu, pick up the workflow Metavisitor: Workflow for remapping in Use Cases 2-1,2 and select the run option. In the workflow form, ensure that Small RNA reads ERP012577 are selected in step 1 and Regex Find And Replace on data 28 is selected in step 2 (this should be the case if you followed the instructions). This time, do not check the box Send results to a new history and directly click the Run workflow button. This workflow will provide you with a graphical view of ERP012577 small RNA mapping to the AnCV genome.","title":"Re-mapping of the small RNA reads (ERP012577) to the AnCV genome (KU169878)."},{"location":"metavisitor/use_case_3-1/","text":"Now that you are familiar with manipulations in Galaxy with the Use Cases 1-1 to 1-4 described in detail in the previous chapters, we will describe the other Use Case analyses more concisely. If you experience lack of skills in basic Galaxy operations (tool usage, copy of datasets, etc), do not hesitate to go back and examine the previous chapters step by step. Virus detection in human RNAseq libraries In the Use Cases 3-X we'll use Metavisitor to detect viruses in RNA sequencing dataset of human patients from 3 different studies. In Use Case 3-1 we use Metavisitor to detect and assemble HIV genomes from patients Innate lymphoid cells sequencing data EBI SRP068722. Input data for Use Case 3-1 As for the previous Use Cases 1 and 2, the first step is to collect all the input data in a history that we will name Input data for Use Case 3-1 . Create a new history Rename this history Input data for Use Case 3-1 We are going to upload 40 datasets form the EBI ENA SRP068722 : Go to the upload files menu and select Paste/Fetch data . Copy-Paste the following table (excluding the headers): SRR id Patient id SRR3111582 patient 0450-318 SRR3111583 patient 0450-318 SRR3111584 patient 0450-318 SRR3111585 patient 0450-318 SRR3111586 patient 0450-318 SRR3111587 patient 0450-318 SRR3111588 patient 0387-272 SRR3111589 patient 0387-272 SRR3111590 patient 0387-272 SRR3111591 patient 0387-272 SRR3111592 patient 0387-272 SRR3111593 patient 0387-272 SRR3111594 patient 0629-453 SRR3111595 patient 0629-453 SRR3111596 patient 0629-453 SRR3111597 patient 0629-453 SRR3111598 patient 0629-453 SRR3111599 patient 0629-453 SRR3111600 patient 0444-312 SRR3111601 patient 0444-312 SRR3111602 patient 0444-312 SRR3111603 patient 0444-312 SRR3111604 patient 0500-355neg SRR3111605 patient 0500-355neg SRR3111606 patient 0292-xxxneg SRR3111607 patient 0292-xxxneg SRR3111608 patient 0394-274 SRR3111609 patient 0394-274 SRR3111610 patient 0218-162neg SRR3111611 patient 0218-162neg SRR3111612 patient 0311-217HIVneg SRR3111613 patient 0311-217HIVneg SRR3111614 patient 0440-307neg SRR3111616 patient 0440-307neg SRR3111617 patient 0518-370neg SRR3111618 patient 0518-370neg SRR3111619 patient 0560-420neg SRR3111620 patient 0560-420neg SRR3111621 patient 0575-419neg SRR3111622 patient 0575-419neg Click the Start button Name and rename the dataset \"Use-Case_3-1_information\". Use the tool Cut columns from table . In the \"Cut columns field\" write c1 and make sure you select \"Use-Case_3-1_information\" file in the \"From\" field before executing. Rename the output \"Use-Case_3-1_accessions\". Use the tool Download and Extract Reads in FASTA/Q format from NCBI SRA , select List of SRA accession, one per line from select input type and \"Use-Case_3-1_accessions\" in sra accession list. Click the Execute button. When the tool is finished running you should have 2 new dataset collections in your history, one of them is empty. Delete the empty collection and verify that you have 40 pairs of datasets in the second collection. If you are missing some sequences you'll have to re-do the steps above with only the missing identifiers. Once done, merge the collections using the tool Merge Collections . Use the Concatenate multiple datasets tail-to-head tool and select \"Paired collection\" as type of data. Set the paired collection as input and select \"Concatenate pairs of datasets\" as type of concatenation. Execute the tool. Rename the outputed collection to SRP068722 and delete the previous one by clicking the X button and selecting \"Permanently Delete Datasets\". Copy the vir2 nucleotide BLAST database from the References history to the current history Input data for Use Case 3-1 . Now we still have to associate sequencing dataset coming from the same patient. We are going to use the tool Tag elements from file to add the patient information as metadata. Click on the Tag elements from file tool and select the collection \"SRP068722\" in \"Input Collection\" and \"Use-Case_3-1_information\" in \"Tag collection elements according to this file\". Execute the tool. Rename the new dataset collection SRP068722_with_patient_information . Select the Apply Rule to Collection and set \"SRP068722_with_patient_information\" as \"Input Collection\". Click on the \"Edit\" button at the right of the form. Click the \"Column\" button and select Add Column from Metadata from the list. In the \"From\" list select \"Tags\". Then click the \"Apply\" button. Click the \"Rules\" button and select Add / Modify Colmn Definitions from the list. Click the \"Add Definition\" button and select the List identifier(s) from the list. In the \"Select a column\" list select \"B\" then click on ... Assign Another Column and select \"A\". Click the \"Apply\" button. Click the \"Save\" and execute the tool. Select the Concatenate multiple datasets tail-to head tool. In \"What type of data do you wish to concatenate?\" select \"Nested collection\". In \"Input nested collection\" select \"SRP068722_with_patient_information (re-organized)\". Execute the tool. Rename the resulting collection \"patient collection\". We are done. You can now permanently delete \"SRP068722_with_patient_information\", \"SRP068722_with_patient_information (re-organized)\" and \"SRP068722\". This will save you some disk space. History for Use Case 3-1 Stay in the history Input data for Use Case 3-1 pick the workflow Metavisitor: Workflow for Use Case 3-1 in the workflows menu, and select the run option. For Step 1 (Fever Patient Sequences collection), select patient collection (this should be already selected). For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to History for Use Case 3-1 , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started may take time to show up. Results The results for this use case differ whether you use Metavisitor or Metavisitor2. There are failed (red), empty datasets in this history. These datasets correspond to patients who didn't have any sequence matchng the viral database. You will notice that only patient 0629-453 has contigs matching HIV sequences. However, this patient is a false positive. In support to this conclusion, you can: Copy the name of the contig Select Pick Fasta sequences Paste the contig name in the \"Select sequences with this string in their header\" section Select dataset 85: Oases viral contigs and run the tool In a new browser tab go to the Blastn web page Copy paste the contig sequence in the query section and mae sure the Nucleotide collection nr is selected as database before running blast The sequence does not match viruses but cloning vectors.","title":"Use Case 3-1"},{"location":"metavisitor/use_case_3-1/#virus-detection-in-human-rnaseq-libraries","text":"In the Use Cases 3-X we'll use Metavisitor to detect viruses in RNA sequencing dataset of human patients from 3 different studies. In Use Case 3-1 we use Metavisitor to detect and assemble HIV genomes from patients Innate lymphoid cells sequencing data EBI SRP068722.","title":"Virus detection in human RNAseq libraries"},{"location":"metavisitor/use_case_3-1/#input-data-for-use-case-3-1","text":"As for the previous Use Cases 1 and 2, the first step is to collect all the input data in a history that we will name Input data for Use Case 3-1 . Create a new history Rename this history Input data for Use Case 3-1 We are going to upload 40 datasets form the EBI ENA SRP068722 : Go to the upload files menu and select Paste/Fetch data . Copy-Paste the following table (excluding the headers): SRR id Patient id SRR3111582 patient 0450-318 SRR3111583 patient 0450-318 SRR3111584 patient 0450-318 SRR3111585 patient 0450-318 SRR3111586 patient 0450-318 SRR3111587 patient 0450-318 SRR3111588 patient 0387-272 SRR3111589 patient 0387-272 SRR3111590 patient 0387-272 SRR3111591 patient 0387-272 SRR3111592 patient 0387-272 SRR3111593 patient 0387-272 SRR3111594 patient 0629-453 SRR3111595 patient 0629-453 SRR3111596 patient 0629-453 SRR3111597 patient 0629-453 SRR3111598 patient 0629-453 SRR3111599 patient 0629-453 SRR3111600 patient 0444-312 SRR3111601 patient 0444-312 SRR3111602 patient 0444-312 SRR3111603 patient 0444-312 SRR3111604 patient 0500-355neg SRR3111605 patient 0500-355neg SRR3111606 patient 0292-xxxneg SRR3111607 patient 0292-xxxneg SRR3111608 patient 0394-274 SRR3111609 patient 0394-274 SRR3111610 patient 0218-162neg SRR3111611 patient 0218-162neg SRR3111612 patient 0311-217HIVneg SRR3111613 patient 0311-217HIVneg SRR3111614 patient 0440-307neg SRR3111616 patient 0440-307neg SRR3111617 patient 0518-370neg SRR3111618 patient 0518-370neg SRR3111619 patient 0560-420neg SRR3111620 patient 0560-420neg SRR3111621 patient 0575-419neg SRR3111622 patient 0575-419neg Click the Start button Name and rename the dataset \"Use-Case_3-1_information\". Use the tool Cut columns from table . In the \"Cut columns field\" write c1 and make sure you select \"Use-Case_3-1_information\" file in the \"From\" field before executing. Rename the output \"Use-Case_3-1_accessions\". Use the tool Download and Extract Reads in FASTA/Q format from NCBI SRA , select List of SRA accession, one per line from select input type and \"Use-Case_3-1_accessions\" in sra accession list. Click the Execute button. When the tool is finished running you should have 2 new dataset collections in your history, one of them is empty. Delete the empty collection and verify that you have 40 pairs of datasets in the second collection. If you are missing some sequences you'll have to re-do the steps above with only the missing identifiers. Once done, merge the collections using the tool Merge Collections . Use the Concatenate multiple datasets tail-to-head tool and select \"Paired collection\" as type of data. Set the paired collection as input and select \"Concatenate pairs of datasets\" as type of concatenation. Execute the tool. Rename the outputed collection to SRP068722 and delete the previous one by clicking the X button and selecting \"Permanently Delete Datasets\". Copy the vir2 nucleotide BLAST database from the References history to the current history Input data for Use Case 3-1 . Now we still have to associate sequencing dataset coming from the same patient. We are going to use the tool Tag elements from file to add the patient information as metadata. Click on the Tag elements from file tool and select the collection \"SRP068722\" in \"Input Collection\" and \"Use-Case_3-1_information\" in \"Tag collection elements according to this file\". Execute the tool. Rename the new dataset collection SRP068722_with_patient_information . Select the Apply Rule to Collection and set \"SRP068722_with_patient_information\" as \"Input Collection\". Click on the \"Edit\" button at the right of the form. Click the \"Column\" button and select Add Column from Metadata from the list. In the \"From\" list select \"Tags\". Then click the \"Apply\" button. Click the \"Rules\" button and select Add / Modify Colmn Definitions from the list. Click the \"Add Definition\" button and select the List identifier(s) from the list. In the \"Select a column\" list select \"B\" then click on ... Assign Another Column and select \"A\". Click the \"Apply\" button. Click the \"Save\" and execute the tool. Select the Concatenate multiple datasets tail-to head tool. In \"What type of data do you wish to concatenate?\" select \"Nested collection\". In \"Input nested collection\" select \"SRP068722_with_patient_information (re-organized)\". Execute the tool. Rename the resulting collection \"patient collection\". We are done. You can now permanently delete \"SRP068722_with_patient_information\", \"SRP068722_with_patient_information (re-organized)\" and \"SRP068722\". This will save you some disk space.","title":"Input data for Use Case 3-1"},{"location":"metavisitor/use_case_3-1/#history-for-use-case-3-1","text":"Stay in the history Input data for Use Case 3-1 pick the workflow Metavisitor: Workflow for Use Case 3-1 in the workflows menu, and select the run option. For Step 1 (Fever Patient Sequences collection), select patient collection (this should be already selected). For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to History for Use Case 3-1 , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started may take time to show up.","title":"History for Use Case 3-1"},{"location":"metavisitor/use_case_3-1/#results","text":"The results for this use case differ whether you use Metavisitor or Metavisitor2. There are failed (red), empty datasets in this history. These datasets correspond to patients who didn't have any sequence matchng the viral database. You will notice that only patient 0629-453 has contigs matching HIV sequences. However, this patient is a false positive. In support to this conclusion, you can: Copy the name of the contig Select Pick Fasta sequences Paste the contig name in the \"Select sequences with this string in their header\" section Select dataset 85: Oases viral contigs and run the tool In a new browser tab go to the Blastn web page Copy paste the contig sequence in the query section and mae sure the Nucleotide collection nr is selected as database before running blast The sequence does not match viruses but cloning vectors.","title":"Results"},{"location":"metavisitor/use_case_3-2/","text":"Use Case 3-2's aim In this Use Case, Metavisitor is used to search for the presence of viruses and identify them in RNA sequencing data of serums of children suffering from fevers of unknown origins. To compare Metavisitor's results to Yozwiak et al . Input data for Use Case 3-2 As for the previous Use Cases 1, 2 and 3-1, the first step is to collect all input data in an history that we will name Input data for Use Case 3-2 . Create a new history Rename this history Input data for Use Case 3-2 Using the tool Extract reads in FASTQ/A format from NCBI SRA , we are going to upload 43 paired end datasets. Indeed, these 43 datasets correspond to 86 fastq paired-ended sequence files. In addition, some datasets derive from the same patient; in those cases we will merge those datasets using the tool Concatenate multiple datasets tail-to-head and delete and purge the original datasets. Open the \"Upload datasets\" menu. Click on the Paste/Fetch data button, name the file \"Use-Case_3-2_SRR_information\" and copy-paste the following text: SRR id Patient id SRR453487 patient 566 SRR453437 patient 438 SRR453443 patient 401 SRR453458 patient 401 SRR453430 patient 382 SRR453491 patient 377 SRR453499 patient 375 SRR453484 patient 350 SRR453464 patient 349 SRR453506 patient 345 SRR453417 patient 344 SRR453490 patient 335 SRR453478 patient 331 SRR453465 patient 330 SRR453480 patient 330 SRR453489 patient 329 SRR453505 patient 329 SRR453498 patient 322 SRR453446 patient 321 SRR453427 patient 315 SRR453440 patient 315 SRR453438 patient 282 SRR453450 patient 275 SRR453460 patient 274 SRR453485 patient 270 SRR453448 patient 266 SRR453424 patient 263 SRR453457 patient 263 SRR453510 patient 193 SRR453456 patient 187 SRR453425 patient 186 SRR453469 patient 186 SRR453481 patient 183 SRR453531 patient 180 SRR453474 patient 179 SRR453509 patient 171 SRR453451 patient 168 SRR453495 patient 161 SRR453504 patient 161 SRR453500 patient 159 SRR453493 patient 156 SRR453444 patient 131 SRR453426 patient 78 Click the Start button. Select the Cut columns from a table tool and set the \"Cut columns\" parameter to \"c1\" and select \"Use-Case_3-2_SRR_information\" as input file in the \"From\" list. Execute the tool and rename the new dataset collection \"Use_Case_3-2_accessions\". Select the tool Download and Extract Reads in FASTA/Q format from NCBI SRA and select \"List of SRA accession, one per line\" in \"select input type\" and \"Use_Case_3-2_accessions\" in \"sra accession list\". Execute the tool. The data downloading step might take 40 minutes to 1h. Delete \"Single-end data (fastq-dump)\". Select the Concatenate multiple datasets tail-to-head tool and set \"Paired collection\" in \"What type of data do you wish to concatenate?\" and \"Pair-end data (fastq-dump)\" as \"Input paired collection to concatenate\". In \"What type of concatenation do you wish to perform?\" select \"Concatenate pairs of datasets (outputs an unpaired collection of datasets)\". Execute the tool. Select Tag elements from file tool and set \"Concatenation by pairs\" as \"Input Collection\" and \"Use-Case_3-2_SRR_information\" as \"Tag collection elements according to this file\". Execute the tool. Select Apply Rule to Collection tool and set \"data 1, data 144, and others (Tagged)\" as \"Input Collection\" and click the \"Edit\" button. Click the \"Column\" button and select \"Add Column from Metadata\" from the list. Select \"Tags\" from the \"For\" list and click the \"Apply\" button. Click the \"Rules\" button and select \"Add / Modify Column Definitions\". Click \"Add Definitions\" button and select \"List identifier(s)\" from the list. Select \"B\" from the \"Select a column\" list. Click \" ... Assign Another Column \" and select \"A\" from the \"Select column\" list. Click the \"Apply\" button and the \"Save\" button. Execute the tool. Select the Conatenate multiple datasets tail-to-head tool ans set \"Nested collection\" in \"What type of data do you wish to concatenate?\" and select the \"... (re-organized)\" dataset collection in \"Input nested collection\". Click the \"Execute\" button. Rename the output collection as \"Tractable Patient Datasets\". Copy the vir2 nucleotide BLAST database from the References history to the current history Input data for Use Case 3-2 . History for Use Case 3-2 Stay in the history Input data for Use Case 3-2 pick the workflow Metavisitor: Workflow for Use Case 3-2 in the workflows menu, and select the run option. For Step 1 (Fever Patient Sequences collection), select Tractable Patient Datasets (this should be already selected). For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to History for Use Case 3-2 , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see you History for Use Case 3-2 running and you will be able to access it. As a last note, the workflow for Use Case 3-2 may take a long time. Be patient.","title":"Use Case 3-2"},{"location":"metavisitor/use_case_3-2/#use-case-3-2s-aim","text":"In this Use Case, Metavisitor is used to search for the presence of viruses and identify them in RNA sequencing data of serums of children suffering from fevers of unknown origins. To compare Metavisitor's results to Yozwiak et al .","title":"Use Case 3-2's aim"},{"location":"metavisitor/use_case_3-2/#input-data-for-use-case-3-2","text":"As for the previous Use Cases 1, 2 and 3-1, the first step is to collect all input data in an history that we will name Input data for Use Case 3-2 . Create a new history Rename this history Input data for Use Case 3-2 Using the tool Extract reads in FASTQ/A format from NCBI SRA , we are going to upload 43 paired end datasets. Indeed, these 43 datasets correspond to 86 fastq paired-ended sequence files. In addition, some datasets derive from the same patient; in those cases we will merge those datasets using the tool Concatenate multiple datasets tail-to-head and delete and purge the original datasets. Open the \"Upload datasets\" menu. Click on the Paste/Fetch data button, name the file \"Use-Case_3-2_SRR_information\" and copy-paste the following text: SRR id Patient id SRR453487 patient 566 SRR453437 patient 438 SRR453443 patient 401 SRR453458 patient 401 SRR453430 patient 382 SRR453491 patient 377 SRR453499 patient 375 SRR453484 patient 350 SRR453464 patient 349 SRR453506 patient 345 SRR453417 patient 344 SRR453490 patient 335 SRR453478 patient 331 SRR453465 patient 330 SRR453480 patient 330 SRR453489 patient 329 SRR453505 patient 329 SRR453498 patient 322 SRR453446 patient 321 SRR453427 patient 315 SRR453440 patient 315 SRR453438 patient 282 SRR453450 patient 275 SRR453460 patient 274 SRR453485 patient 270 SRR453448 patient 266 SRR453424 patient 263 SRR453457 patient 263 SRR453510 patient 193 SRR453456 patient 187 SRR453425 patient 186 SRR453469 patient 186 SRR453481 patient 183 SRR453531 patient 180 SRR453474 patient 179 SRR453509 patient 171 SRR453451 patient 168 SRR453495 patient 161 SRR453504 patient 161 SRR453500 patient 159 SRR453493 patient 156 SRR453444 patient 131 SRR453426 patient 78 Click the Start button. Select the Cut columns from a table tool and set the \"Cut columns\" parameter to \"c1\" and select \"Use-Case_3-2_SRR_information\" as input file in the \"From\" list. Execute the tool and rename the new dataset collection \"Use_Case_3-2_accessions\". Select the tool Download and Extract Reads in FASTA/Q format from NCBI SRA and select \"List of SRA accession, one per line\" in \"select input type\" and \"Use_Case_3-2_accessions\" in \"sra accession list\". Execute the tool. The data downloading step might take 40 minutes to 1h. Delete \"Single-end data (fastq-dump)\". Select the Concatenate multiple datasets tail-to-head tool and set \"Paired collection\" in \"What type of data do you wish to concatenate?\" and \"Pair-end data (fastq-dump)\" as \"Input paired collection to concatenate\". In \"What type of concatenation do you wish to perform?\" select \"Concatenate pairs of datasets (outputs an unpaired collection of datasets)\". Execute the tool. Select Tag elements from file tool and set \"Concatenation by pairs\" as \"Input Collection\" and \"Use-Case_3-2_SRR_information\" as \"Tag collection elements according to this file\". Execute the tool. Select Apply Rule to Collection tool and set \"data 1, data 144, and others (Tagged)\" as \"Input Collection\" and click the \"Edit\" button. Click the \"Column\" button and select \"Add Column from Metadata\" from the list. Select \"Tags\" from the \"For\" list and click the \"Apply\" button. Click the \"Rules\" button and select \"Add / Modify Column Definitions\". Click \"Add Definitions\" button and select \"List identifier(s)\" from the list. Select \"B\" from the \"Select a column\" list. Click \" ... Assign Another Column \" and select \"A\" from the \"Select column\" list. Click the \"Apply\" button and the \"Save\" button. Execute the tool. Select the Conatenate multiple datasets tail-to-head tool ans set \"Nested collection\" in \"What type of data do you wish to concatenate?\" and select the \"... (re-organized)\" dataset collection in \"Input nested collection\". Click the \"Execute\" button. Rename the output collection as \"Tractable Patient Datasets\". Copy the vir2 nucleotide BLAST database from the References history to the current history Input data for Use Case 3-2 .","title":"Input data for Use Case 3-2"},{"location":"metavisitor/use_case_3-2/#history-for-use-case-3-2","text":"Stay in the history Input data for Use Case 3-2 pick the workflow Metavisitor: Workflow for Use Case 3-2 in the workflows menu, and select the run option. For Step 1 (Fever Patient Sequences collection), select Tractable Patient Datasets (this should be already selected). For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to History for Use Case 3-2 , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see you History for Use Case 3-2 running and you will be able to access it. As a last note, the workflow for Use Case 3-2 may take a long time. Be patient.","title":"History for Use Case 3-2"},{"location":"metavisitor/use_case_3-3/","text":"Workflow for Use Case 3-3's aim In ths Use Case, we take the datasets from Matranga et al. , relevant in the context of Lassa and Ebola outbreak and epidemic response, to demonstrate the versatility of Metavisitor as well as its ability to generate high throughput reconstruction of viral genomes. Input data for Use Case 3-3 As for the previous Use Cases, the first step is to collect all input data in an history that we will name Input data for Use Case 3-3 . Create a new history Rename this history Input data for Use Case 3-3 Using the tool Extract reads in FASTA/Q format from NCBI SRA , we are going to upload 63 paired end datasets. For Ebola virus samples: Select the upload File tool and click on the Paste/Fetch data button. Name the file \"Ebola_accessions\" and copy-paste the following text: SRR id SRR1613381 SRR1613377 SRR1613382 SRR1613378 SRR1613383 SRR1613379 SRR1613384 SRR1613380 Click the \"Start\" button. Use the Download and Extract Reads in FASTA/Q format from NCBI SRA tool. Set \"List of SRA accession\" in \"select input type\" and enter \"Ebola_accessions\" as input. Execute the tool. Select Concatenate multiple datasets tail-to-head . Change \"What type of data do you wish to concatenate?\" to \"Paired collection\", set the collection as input and \"Concatenate pairs of datasets\" in \"What type of concatenation do you wish to perform?\". When you are finished, you'll have 8 datasets. Make sure to verify their datatype is fastqsanger or fastqsanger.gz , and create a dataset collection (as explained in the previous chapter) of these 8 datasets that you will name Ebola virus . For Lassa virus samples: Upload, Download and Concatenate the Lassa virus datasets the same way as above, but this time name the file \"Lassa_accessions\" and copy-paste this text: SRR id SRR1595772 SRR1595696 SRR1595665 SRR1595500 SRR1594619 SRR1595943 SRR1595673 SRR1595797 SRR1595763 SRR1595558 SRR1594664 SRR1595909 SRR1594651 SRR1595835 SRR1594698 SRR1613388 SRR1613389 SRR1613390 SRR1613391 SRR1613392 SRR1613393 SRR1613394 SRR1613395 SRR1613396 SRR1613397 SRR1613398 SRR1613399 SRR1595853 SRR1606288 SRR1613412 SRR1613403 SRR1606277 SRR1613386 SRR1613387 SRR1606267 SRR1614275 SRR1610580 SRR1595846 SRR1594606 SRR1606236 SRR1594723 SRR1594671 SRR1613414 SRR1613400 SRR1613401 SRR1613404 SRR1613402 SRR1613405 SRR1613407 SRR1613408 SRR1613409 SRR1613410 SRR1613406 SRR1613411 SRR1613413 When you are finished you'll have 55 datasets. Make sure their datatype is fastqsanger or fastqsanger.gz , and create a dataset collection (as explained in the previous chapter) of these 55 datasets that you will name Lassa virus . Copy the nucleotide vir2 blast database from the References history to the current history Input data for Use Case 3-3 . History for Use Case 3-3 / Ebola virus Stay in the history Input data for Use Case 3-3 Pick the workflow Metavisitor: Workflow for Use Case 3-3 in the workflows menu, and select the run option. Before Step 1, you have to specify some parameters at run time. For Ebola virus, the field reference_virus has to be filled with NC_002549.1 (as a guide for reconstruction of the Ebola virus genome) and the field target_virus has to be filled with Ebola . For Step 1, select Ebola virus . For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to Use Case 3-3 Ebola virus , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see your Use Case 3-3 Ebola virus history running and you will be able to access it. The workflow for Use Case 3-3 may take a long time. Be patient. History for Use Case 3-3 / Lassa virus, segment L Stay in the history Input data for Use Case 3-3 Pick the workflow Metavisitor: Workflow for Use Case 3-3 in the workflows menu, and select the run option. Before Step 1, you have to specify some parameters at run time. For Lassa virus, the field reference_virus has to be filled with NC_004297.1 (as a guide for reconstruction of the segment L of the Lassa virus genome) and the field target_virus has to be filled with Lassa . For Step 1, select Lassa virus . For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to Use Case 3-3 Lassa virus segment L , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see your Use Case 3-3 Lassa virus segment L history running and you will be able to access it. The workflow for Use Case 3-3 may take a long time. Be patient. History for Use Case 3-3 / Lassa virus, segment S Stay in the history Input data for Use Case 3-3 Pick the workflow Metavisitor: Workflow for Use Case 3-3 in the workflows menu, and select the run option. Before Step 1, you have to specify some parameters at run time. For Lassa virus, the field reference_virus has to be filled with NC_004296.1 (as a guide for reconstruction of the segment S of the Lassa virus genome) and the field target_virus has to be filled with Lassa . For Step 1, select Lassa virus (this should be already selected). For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to Use Case 3-3 Lassa virus segment S , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see your Use Case 3-3 Lassa virus segment S history running and you will be able to access it. The workflow for Use Case 3-3 may take a long time. Be patient.","title":"Use Case 3-3"},{"location":"metavisitor/use_case_3-3/#workflow-for-use-case-3-3s-aim","text":"In ths Use Case, we take the datasets from Matranga et al. , relevant in the context of Lassa and Ebola outbreak and epidemic response, to demonstrate the versatility of Metavisitor as well as its ability to generate high throughput reconstruction of viral genomes.","title":"Workflow for Use Case 3-3's aim"},{"location":"metavisitor/use_case_3-3/#input-data-for-use-case-3-3","text":"As for the previous Use Cases, the first step is to collect all input data in an history that we will name Input data for Use Case 3-3 . Create a new history Rename this history Input data for Use Case 3-3 Using the tool Extract reads in FASTA/Q format from NCBI SRA , we are going to upload 63 paired end datasets.","title":"Input data for Use Case 3-3"},{"location":"metavisitor/use_case_3-3/#for-ebola-virus-samples","text":"Select the upload File tool and click on the Paste/Fetch data button. Name the file \"Ebola_accessions\" and copy-paste the following text: SRR id SRR1613381 SRR1613377 SRR1613382 SRR1613378 SRR1613383 SRR1613379 SRR1613384 SRR1613380 Click the \"Start\" button. Use the Download and Extract Reads in FASTA/Q format from NCBI SRA tool. Set \"List of SRA accession\" in \"select input type\" and enter \"Ebola_accessions\" as input. Execute the tool. Select Concatenate multiple datasets tail-to-head . Change \"What type of data do you wish to concatenate?\" to \"Paired collection\", set the collection as input and \"Concatenate pairs of datasets\" in \"What type of concatenation do you wish to perform?\". When you are finished, you'll have 8 datasets. Make sure to verify their datatype is fastqsanger or fastqsanger.gz , and create a dataset collection (as explained in the previous chapter) of these 8 datasets that you will name Ebola virus .","title":"For Ebola virus samples:"},{"location":"metavisitor/use_case_3-3/#for-lassa-virus-samples","text":"Upload, Download and Concatenate the Lassa virus datasets the same way as above, but this time name the file \"Lassa_accessions\" and copy-paste this text: SRR id SRR1595772 SRR1595696 SRR1595665 SRR1595500 SRR1594619 SRR1595943 SRR1595673 SRR1595797 SRR1595763 SRR1595558 SRR1594664 SRR1595909 SRR1594651 SRR1595835 SRR1594698 SRR1613388 SRR1613389 SRR1613390 SRR1613391 SRR1613392 SRR1613393 SRR1613394 SRR1613395 SRR1613396 SRR1613397 SRR1613398 SRR1613399 SRR1595853 SRR1606288 SRR1613412 SRR1613403 SRR1606277 SRR1613386 SRR1613387 SRR1606267 SRR1614275 SRR1610580 SRR1595846 SRR1594606 SRR1606236 SRR1594723 SRR1594671 SRR1613414 SRR1613400 SRR1613401 SRR1613404 SRR1613402 SRR1613405 SRR1613407 SRR1613408 SRR1613409 SRR1613410 SRR1613406 SRR1613411 SRR1613413 When you are finished you'll have 55 datasets. Make sure their datatype is fastqsanger or fastqsanger.gz , and create a dataset collection (as explained in the previous chapter) of these 55 datasets that you will name Lassa virus . Copy the nucleotide vir2 blast database from the References history to the current history Input data for Use Case 3-3 .","title":"For Lassa virus samples:"},{"location":"metavisitor/use_case_3-3/#history-for-use-case-3-3-ebola-virus","text":"Stay in the history Input data for Use Case 3-3 Pick the workflow Metavisitor: Workflow for Use Case 3-3 in the workflows menu, and select the run option. Before Step 1, you have to specify some parameters at run time. For Ebola virus, the field reference_virus has to be filled with NC_002549.1 (as a guide for reconstruction of the Ebola virus genome) and the field target_virus has to be filled with Ebola . For Step 1, select Ebola virus . For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to Use Case 3-3 Ebola virus , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see your Use Case 3-3 Ebola virus history running and you will be able to access it. The workflow for Use Case 3-3 may take a long time. Be patient.","title":"History for Use Case 3-3 / Ebola virus"},{"location":"metavisitor/use_case_3-3/#history-for-use-case-3-3-lassa-virus-segment-l","text":"Stay in the history Input data for Use Case 3-3 Pick the workflow Metavisitor: Workflow for Use Case 3-3 in the workflows menu, and select the run option. Before Step 1, you have to specify some parameters at run time. For Lassa virus, the field reference_virus has to be filled with NC_004297.1 (as a guide for reconstruction of the segment L of the Lassa virus genome) and the field target_virus has to be filled with Lassa . For Step 1, select Lassa virus . For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to Use Case 3-3 Lassa virus segment L , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see your Use Case 3-3 Lassa virus segment L history running and you will be able to access it. The workflow for Use Case 3-3 may take a long time. Be patient.","title":"History for Use Case 3-3 / Lassa virus, segment L"},{"location":"metavisitor/use_case_3-3/#history-for-use-case-3-3-lassa-virus-segment-s","text":"Stay in the history Input data for Use Case 3-3 Pick the workflow Metavisitor: Workflow for Use Case 3-3 in the workflows menu, and select the run option. Before Step 1, you have to specify some parameters at run time. For Lassa virus, the field reference_virus has to be filled with NC_004296.1 (as a guide for reconstruction of the segment S of the Lassa virus genome) and the field target_virus has to be filled with Lassa . For Step 1, select Lassa virus (this should be already selected). For Step 2, select the nucleotide vir2 blast database (this should also be already selected) As usual, check the box Send results to a new history , edit the name of the new history to Use Case 3-3 Lassa virus segment S , and Execute the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your User - Saved history menu, you will see your Use Case 3-3 Lassa virus segment S history running and you will be able to access it. The workflow for Use Case 3-3 may take a long time. Be patient.","title":"History for Use Case 3-3 / Lassa virus, segment S"},{"location":"metavisitor/use_cases_input_data/","text":"We are now entering into real analyses using Metavisitor. These analyses as well as their biological context are presented as Use Cases in the metavisitor article . We invite readers of this manual to refer to this article if they need to better understand the biological context of the described procedures. In this section, we are going to create step by step a Galaxy history that contains the input data required to run the workflows for Use Cases 1-1, 1-2, 1-3 and 1-4. Detection of known viruses Using small RNA sequencing libraries SRP013822 (EBI ENA) and Metavisitor workflows, we are going to reconstruct Nora virus genomes. Workflow for Use Case 1-1: Takes the raw reads and collapses them into unique sequences to reconstruct a Nora virus genome referred to as Nora_MV Workflow for Use Case 1-2: Takes raw reads and reconstructs a Nora_raw_reads genome Workflow for Use Case 1-3: Takes raw reads, normalizes the abundances and reconstructs a Nora_Median-Norm-reads genome In order to show Metavisitor's ability to detect multiple known viruses we'll use an other workflow with SRP013822 sequences. Workflow for Use Case 1-4: Takes raw reads, assembles contigs and aligns them against vir2 History with input data for Use Cases 1-1, 1-2, 1-3 and 1-4 Create a new history and rename it \"Input data for Use Cases 1-1, 1-2, 1-3 and 1-4\" Get SRP013822 datasets list Use the the tool Upload File and click on the Paste/Fetch data button Copy - Paste the following text (not including the header): SRR id SRR515090 SRR513993 SRR513992 SRR513990 SRR513989 SRR513981 SRR513901 Edit the file name by clicking the \"New File\" section and writing \"use_case_1_accessions\" or by selecting the Start button and changing the file name. Import SRP013822 datasets Use the tool Extract reads in FASTQ/A format from NCBI SRA and select in the select input type list List of SRA accession, one per line . Select in the sra accession list the use_case_1_accessions file and run the tool. Rename a dataset collection SRP013822 Click on the Single-end data (fastqdump) collection Click on the title \"Single-end data (fastqdump)\" and rename it \"SRP013822\" You can delete the Pair-end data (fastq-dump) collection by clicking the X button and selecting \"Collection Only\". Copy the vir2 blast nucleotide database that we prepared earlier in the Reference history. To do so, click on the little wheel icon in the history top menu (in the history right bar). Select \"Copy Datasets\" In the open page, select \"References\" in the Source History menu, check the \"nucleotide vir2 blast database\" dataset; select \"Input data for Use Case 1_1, ...\"; and click the \"Copy History Items\". If you refresh the history, you will see the \"nucleotide vir2 blast database\" dataset showing up. That is all for the moment. We will latter add datasets in the history Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 . However, these datasets do no exist yet: this will be produced by the Use Cases 1-1, 1-2, 1-3 workflows !","title":"Prepare input data histories"},{"location":"metavisitor/use_cases_input_data/#detection-of-known-viruses","text":"Using small RNA sequencing libraries SRP013822 (EBI ENA) and Metavisitor workflows, we are going to reconstruct Nora virus genomes. Workflow for Use Case 1-1: Takes the raw reads and collapses them into unique sequences to reconstruct a Nora virus genome referred to as Nora_MV Workflow for Use Case 1-2: Takes raw reads and reconstructs a Nora_raw_reads genome Workflow for Use Case 1-3: Takes raw reads, normalizes the abundances and reconstructs a Nora_Median-Norm-reads genome In order to show Metavisitor's ability to detect multiple known viruses we'll use an other workflow with SRP013822 sequences. Workflow for Use Case 1-4: Takes raw reads, assembles contigs and aligns them against vir2","title":"Detection of known viruses"},{"location":"metavisitor/use_cases_input_data/#history-with-input-data-for-use-cases-1-1-1-2-1-3-and-1-4","text":"Create a new history and rename it \"Input data for Use Cases 1-1, 1-2, 1-3 and 1-4\" Get SRP013822 datasets list Use the the tool Upload File and click on the Paste/Fetch data button Copy - Paste the following text (not including the header): SRR id SRR515090 SRR513993 SRR513992 SRR513990 SRR513989 SRR513981 SRR513901 Edit the file name by clicking the \"New File\" section and writing \"use_case_1_accessions\" or by selecting the Start button and changing the file name. Import SRP013822 datasets Use the tool Extract reads in FASTQ/A format from NCBI SRA and select in the select input type list List of SRA accession, one per line . Select in the sra accession list the use_case_1_accessions file and run the tool. Rename a dataset collection SRP013822 Click on the Single-end data (fastqdump) collection Click on the title \"Single-end data (fastqdump)\" and rename it \"SRP013822\" You can delete the Pair-end data (fastq-dump) collection by clicking the X button and selecting \"Collection Only\". Copy the vir2 blast nucleotide database that we prepared earlier in the Reference history. To do so, click on the little wheel icon in the history top menu (in the history right bar). Select \"Copy Datasets\" In the open page, select \"References\" in the Source History menu, check the \"nucleotide vir2 blast database\" dataset; select \"Input data for Use Case 1_1, ...\"; and click the \"Copy History Items\". If you refresh the history, you will see the \"nucleotide vir2 blast database\" dataset showing up. That is all for the moment. We will latter add datasets in the history Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 . However, these datasets do no exist yet: this will be produced by the Use Cases 1-1, 1-2, 1-3 workflows !","title":"History with input data for Use Cases 1-1, 1-2, 1-3 and 1-4"},{"location":"reference_based_RNAseq/","text":"Galaxy Spring Day 2019 - Reference-based RNAseq analysis March 21 st 2019 In this training session, we are going to perform a RNAseq analysis as first outlined in Galaxy training material for Reference-based RNAseq analysis We have updated the instructions given in this training material and put these instructions in this readthedoc site for the spring day 2019. We will propose the update for a pull request to the galaxyproject training-material repository In this tutorial, we will deal with: Pretreatments Data upload Quality control Mapping Mapping Inspection of the mapping results Analysis of the differential gene expression Count the number of reads per annotated gene Viewing datasets side by side using the Scratchbook Identification of the differentially expressed features Visualization of the differentially expressed genes Analysis of functional enrichment among the differentially expressed genes","title":"Introduction"},{"location":"reference_based_RNAseq/#galaxy-spring-day-2019-reference-based-rnaseq-analysis","text":"","title":"Galaxy Spring Day 2019 - Reference-based RNAseq analysis"},{"location":"reference_based_RNAseq/#march-21st-2019","text":"In this training session, we are going to perform a RNAseq analysis as first outlined in Galaxy training material for Reference-based RNAseq analysis We have updated the instructions given in this training material and put these instructions in this readthedoc site for the spring day 2019. We will propose the update for a pull request to the galaxyproject training-material repository In this tutorial, we will deal with: Pretreatments Data upload Quality control Mapping Mapping Inspection of the mapping results Analysis of the differential gene expression Count the number of reads per annotated gene Viewing datasets side by side using the Scratchbook Identification of the differentially expressed features Visualization of the differentially expressed genes Analysis of functional enrichment among the differentially expressed genes","title":"March 21st 2019"},{"location":"reference_based_RNAseq/Cutadapt/","text":"Filtering datasets to remove or trim low quality sequences This step is optional and should be performed by 50% of attendees. Cutadapt with single reads Create a new history Cutapdapt ( wheel Create New ) Copy the fastq files from the RNAseq data library to this new history ( wheel Copy datasets ) Select the Cutadapt tool Start with selecting Single-end in the Single-end or Paired-end reads? menu Select the multiple datasets button for this menu Cmd-Click for discontinuous multiple selection of single fastq.gz files (3 datasets) Filter Options Minimum length : 20 Read Modification Options Quality cutoff : 20 Output Options Report : Yes Do not change the other available parameters and click Execute Cutadapt with paired-end reads Repeat the same procedure as above, except that you select Paired-end in step 4: Re-Run the tool using the re-run button on one Cutadapt instance and just select Paired-end instead of Single-end Then you have two input boxes, one for file #1 and one for file #2. In the box file #1 click the multiple datasets button and carefully Select the fastq.gz files with the _1 suffix In the box file #2 click the multiple datasets button and carefully Select the fastq.gz files with the _2 suffix Do not change the other parameters (they are set to the same value as previously because you used the re-run button). Click the Execute button Run MultiQC on Cutadapt jobs Select MultiQC tool Select Cutadapt/Trim Galore! in the menu Which tool was used generate logs? Cmd-Select the Report datasets generated by Cutadapt Press Execute Now, the boring but essential job: Rename carefully the Output datasets generated by Cutadapt. To do so, help yourself to the Info button at the bottom of dataset green boxes. Example: Rename Cutadapt on data 10 and data 9: Read 2 Output in GSM461181_2_treat_paired.fastq.gz Trash the 11 unfiltered/trimmed fastq.gz files. This is important to avoid mixing filtered and non filtered datasets in the next steps.","title":"Optional filtering of reads on sequence quality"},{"location":"reference_based_RNAseq/Cutadapt/#filtering-datasets-to-remove-or-trim-low-quality-sequences","text":"","title":"Filtering datasets to remove or trim low quality sequences"},{"location":"reference_based_RNAseq/Cutadapt/#this-step-is-optional-and-should-be-performed-by-50-of-attendees","text":"","title":"This step is optional and should be performed by 50% of attendees."},{"location":"reference_based_RNAseq/Cutadapt/#cutadapt-with-single-reads","text":"Create a new history Cutapdapt ( wheel Create New ) Copy the fastq files from the RNAseq data library to this new history ( wheel Copy datasets ) Select the Cutadapt tool Start with selecting Single-end in the Single-end or Paired-end reads? menu Select the multiple datasets button for this menu Cmd-Click for discontinuous multiple selection of single fastq.gz files (3 datasets) Filter Options Minimum length : 20 Read Modification Options Quality cutoff : 20 Output Options Report : Yes Do not change the other available parameters and click Execute","title":"Cutadapt with single reads"},{"location":"reference_based_RNAseq/Cutadapt/#cutadapt-with-paired-end-reads","text":"Repeat the same procedure as above, except that you select Paired-end in step 4: Re-Run the tool using the re-run button on one Cutadapt instance and just select Paired-end instead of Single-end Then you have two input boxes, one for file #1 and one for file #2. In the box file #1 click the multiple datasets button and carefully Select the fastq.gz files with the _1 suffix In the box file #2 click the multiple datasets button and carefully Select the fastq.gz files with the _2 suffix Do not change the other parameters (they are set to the same value as previously because you used the re-run button). Click the Execute button","title":"Cutadapt with paired-end reads"},{"location":"reference_based_RNAseq/Cutadapt/#run-multiqc-on-cutadapt-jobs","text":"Select MultiQC tool Select Cutadapt/Trim Galore! in the menu Which tool was used generate logs? Cmd-Select the Report datasets generated by Cutadapt Press Execute Now, the boring but essential job: Rename carefully the Output datasets generated by Cutadapt. To do so, help yourself to the Info button at the bottom of dataset green boxes. Example: Rename Cutadapt on data 10 and data 9: Read 2 Output in GSM461181_2_treat_paired.fastq.gz Trash the 11 unfiltered/trimmed fastq.gz files. This is important to avoid mixing filtered and non filtered datasets in the next steps.","title":"Run MultiQC on Cutadapt jobs"},{"location":"reference_based_RNAseq/DEDESeq2/","text":"DESeq2 Let's create a clean fresh history ( wheel Create New ) and name it DESeq2 Copy the .Counts datasets from your STAR / HISAT2 history to this new history ( wheel Copy datasets ) Select the DESeq2 tool with the following parameters: how : Select group tags corresponding to levels In Factor : In 1: Factor Specify a factor name : Treatment In Factor level : In 1: Factor level : Specify a factor level : treated Counts file(s) : the 3 gene count files with treat in their name In 2: Factor level : Specify a factor level : untreated Counts file(s) : the 4 gene count files with untreat in their name Click on Insert Factor (not on Insert Factor level ) In 2: Factor Specify a factor name to Sequencing In Factor level : In 1: Factor level : Specify a factor level : Paired Counts file(s) : the 4 gene count files with paired in their name In 2: Factor level : Specify a factor level : Single Counts file(s) : the 3 gene count files with single in their name Files have header? : Yes Output normalized counts table : Yes Execute","title":"DESeq2 use"},{"location":"reference_based_RNAseq/DEDESeq2/#deseq2","text":"Let's create a clean fresh history ( wheel Create New ) and name it DESeq2 Copy the .Counts datasets from your STAR / HISAT2 history to this new history ( wheel Copy datasets ) Select the DESeq2 tool with the following parameters: how : Select group tags corresponding to levels In Factor : In 1: Factor Specify a factor name : Treatment In Factor level : In 1: Factor level : Specify a factor level : treated Counts file(s) : the 3 gene count files with treat in their name In 2: Factor level : Specify a factor level : untreated Counts file(s) : the 4 gene count files with untreat in their name Click on Insert Factor (not on Insert Factor level ) In 2: Factor Specify a factor name to Sequencing In Factor level : In 1: Factor level : Specify a factor level : Paired Counts file(s) : the 4 gene count files with paired in their name In 2: Factor level : Specify a factor level : Single Counts file(s) : the 3 gene count files with single in their name Files have header? : Yes Output normalized counts table : Yes Execute","title":"DESeq2"},{"location":"reference_based_RNAseq/DE_intro/","text":"Analysis of the differential gene expression using DESeq2 DESeq2 is a great tool for Differential Gene Expression (DGE) analysis. It takes read counts and combines them into a table (with genes in the rows and samples in the columns). Importantly, it applies size factor normalization by: Computing for each gene the geometric mean of read counts across all samples Dividing every gene count by the geometric mean accross samples Using the median of these ratios as a sample\u2019s size factor for normalization Multiple factors with several levels can then be incorporated in the analysis. After normalization we can compare the response of the expression of any gene to the presence of different levels of a factor in a statistically reliable way. In our example, we have samples with two varying factors that can contribute to differences in gene expression: Treatment (either treated or untreated) Sequencing type (paired-end or single-end) Here, treatment is the primary factor that we are interested in. The sequencing type is further information we know about the data that might affect the analysis. Multi-factor analysis allows us to assess the effect of the treatment, while taking the sequencing type into account too. 1 2 3 4 We recommend that you add as many factors as you think may affect gene expression in your experiment. It can be the sequencing type like here, but it can also be the manipulation (if different persons are involved in the library preparation), other batch effects, etc\u2026","title":"Identification of the differentially expressed features - DESeq2"},{"location":"reference_based_RNAseq/DE_intro/#analysis-of-the-differential-gene-expression-using-deseq2","text":"DESeq2 is a great tool for Differential Gene Expression (DGE) analysis. It takes read counts and combines them into a table (with genes in the rows and samples in the columns). Importantly, it applies size factor normalization by: Computing for each gene the geometric mean of read counts across all samples Dividing every gene count by the geometric mean accross samples Using the median of these ratios as a sample\u2019s size factor for normalization Multiple factors with several levels can then be incorporated in the analysis. After normalization we can compare the response of the expression of any gene to the presence of different levels of a factor in a statistically reliable way. In our example, we have samples with two varying factors that can contribute to differences in gene expression: Treatment (either treated or untreated) Sequencing type (paired-end or single-end) Here, treatment is the primary factor that we are interested in. The sequencing type is further information we know about the data that might affect the analysis. Multi-factor analysis allows us to assess the effect of the treatment, while taking the sequencing type into account too. 1 2 3 4 We recommend that you add as many factors as you think may affect gene expression in your experiment. It can be the sequencing type like here, but it can also be the manipulation (if different persons are involved in the library preparation), other batch effects, etc\u2026","title":"Analysis of the differential gene expression using DESeq2"},{"location":"reference_based_RNAseq/DEseq2visu/","text":"Visualisation of differential expression Now we would like to extract the most differentially expressed genes due to the treatment, and then visualize them using an heatmap of the normalized counts and also the z-score for each sample. We will proceed in several steps: Extract the most differentially expressed genes using the DESeq2 summary file Extract the normalized counts for these genes for each sample, using the normalized count file generated by DESeq2 Plot the heatmap of the normalized counts Compute the Z score of the normalized counts Plot the heatmap of the Z score of the genes Extract the most differentially expressed genes Select the tool Filter data on any column using simple expressions to extract genes with a significant change in gene expression (adjusted p-value below 0.05) between treated and untreated samples: Filter : the DESeq2 result file With following condition : c7 0.05 The file with the independent filtered results can be used for further downstream analysis as it excludes genes with only few read counts as these genes will not be considered as significantly differentially expressed. The generated file contains too many genes (632/STAR, ) to get a meaningful heatmap. Therefore, in the next step, we will take only the genes with an absolute fold change 2 (log2(fold change) 1) Select the tool Filter data on any column using simple expressions Filter : the differentially expressed genes (output of previous Filter tool) With following condition : abs(c3) 1 We now have a table with 84/STAR, /HISAT2 lines corresponding to the most differentially expressed genes. And for each of the gene, we have its id, its mean normalized counts (averaged over all samples from both conditions), its log2FC and other information. We could plot the log2FC for the different genes, but here we would like to look at a heatmap of expression for these genes in the different samples. So we need to extract the normalized counts for these genes. We will join the normalized count table generated by DESeq2 with the table we just generated, to conserve only the lines corresponding to the most differentially expressed genes. Extract the normalized counts of the most differentially expressed genes Create a Pasted Entry from the header line of the Filter output: Copy the header of the final Filter output Using the Upload tool select Paste/Fetch data and paste the copied data Set the Type to tabular and select Start to upload a new Pasted Entry Concatenate datasets tool to add this header line to the Filter output: select the Concatenate datasets tail-to-head tool select the Pasted entry dataset + Insert Dataset select the final Filter output This ensures that the table of most differentially expressed genes has a header line and can be used in the next step. join the normalized count table generated by DESeq2 with the table we just generated, to conserve only the lines corresponding to the most differentially expressed genes select the Join two Datasets side by side on a specified field tool Join : the Normalized counts file (output of DESeq2 tool) using column : Column: 1 with : most differentially expressed genes (output of the Concatenate tool tool) and column : Column: 1 Keep lines of first input that do not join with second input : No Keep the header lines : Yes The generated file has more columns than we need for the heatmap. In addition to the columns with mean normalized counts, there is the log2FC and other information. We need to remove the extra columns. Cut tool to extract the columns with the gene ids and normalized counts: Select the Cut columns from a table tool Cut columns : c1-c8 Delimited by : Tab From : the joined dataset (output of Join two Datasets tool) We now have a table with 85 lines (the most differentially expressed genes) and the normalized counts for these genes in the 7 samples. Plot the heatmap of the normalized counts of these genes for the samples Select the heatmap2 tool to plot the heatmap: Input should have column headers : the generated table (output of Cut tool) Data transformation : Log2(value+1) transform my data Enable data clustering : Yes Labeling columns and rows : Label columns and not rows Coloring groups : Blue to white to red You should obtain something similar to:","title":"Visualization of the differentially expressed genes"},{"location":"reference_based_RNAseq/DEseq2visu/#visualisation-of-differential-expression","text":"Now we would like to extract the most differentially expressed genes due to the treatment, and then visualize them using an heatmap of the normalized counts and also the z-score for each sample. We will proceed in several steps: Extract the most differentially expressed genes using the DESeq2 summary file Extract the normalized counts for these genes for each sample, using the normalized count file generated by DESeq2 Plot the heatmap of the normalized counts Compute the Z score of the normalized counts Plot the heatmap of the Z score of the genes","title":"Visualisation of differential expression"},{"location":"reference_based_RNAseq/DEseq2visu/#extract-the-most-differentially-expressed-genes","text":"Select the tool Filter data on any column using simple expressions to extract genes with a significant change in gene expression (adjusted p-value below 0.05) between treated and untreated samples: Filter : the DESeq2 result file With following condition : c7 0.05 The file with the independent filtered results can be used for further downstream analysis as it excludes genes with only few read counts as these genes will not be considered as significantly differentially expressed. The generated file contains too many genes (632/STAR, ) to get a meaningful heatmap. Therefore, in the next step, we will take only the genes with an absolute fold change 2 (log2(fold change) 1) Select the tool Filter data on any column using simple expressions Filter : the differentially expressed genes (output of previous Filter tool) With following condition : abs(c3) 1 We now have a table with 84/STAR, /HISAT2 lines corresponding to the most differentially expressed genes. And for each of the gene, we have its id, its mean normalized counts (averaged over all samples from both conditions), its log2FC and other information. We could plot the log2FC for the different genes, but here we would like to look at a heatmap of expression for these genes in the different samples. So we need to extract the normalized counts for these genes. We will join the normalized count table generated by DESeq2 with the table we just generated, to conserve only the lines corresponding to the most differentially expressed genes.","title":"Extract the most differentially expressed genes"},{"location":"reference_based_RNAseq/DEseq2visu/#extract-the-normalized-counts-of-the-most-differentially-expressed-genes","text":"Create a Pasted Entry from the header line of the Filter output: Copy the header of the final Filter output Using the Upload tool select Paste/Fetch data and paste the copied data Set the Type to tabular and select Start to upload a new Pasted Entry Concatenate datasets tool to add this header line to the Filter output: select the Concatenate datasets tail-to-head tool select the Pasted entry dataset + Insert Dataset select the final Filter output This ensures that the table of most differentially expressed genes has a header line and can be used in the next step. join the normalized count table generated by DESeq2 with the table we just generated, to conserve only the lines corresponding to the most differentially expressed genes select the Join two Datasets side by side on a specified field tool Join : the Normalized counts file (output of DESeq2 tool) using column : Column: 1 with : most differentially expressed genes (output of the Concatenate tool tool) and column : Column: 1 Keep lines of first input that do not join with second input : No Keep the header lines : Yes The generated file has more columns than we need for the heatmap. In addition to the columns with mean normalized counts, there is the log2FC and other information. We need to remove the extra columns. Cut tool to extract the columns with the gene ids and normalized counts: Select the Cut columns from a table tool Cut columns : c1-c8 Delimited by : Tab From : the joined dataset (output of Join two Datasets tool) We now have a table with 85 lines (the most differentially expressed genes) and the normalized counts for these genes in the 7 samples. Plot the heatmap of the normalized counts of these genes for the samples Select the heatmap2 tool to plot the heatmap: Input should have column headers : the generated table (output of Cut tool) Data transformation : Log2(value+1) transform my data Enable data clustering : Yes Labeling columns and rows : Label columns and not rows Coloring groups : Blue to white to red You should obtain something similar to:","title":"Extract the normalized counts of the most differentially expressed genes"},{"location":"reference_based_RNAseq/GO-intro/","text":"Analysis of functional enrichment among the differentially expressed genes We have extracted genes that are differentially expressed in treated (Pasilla gene-depleted) samples compared to untreated samples. We would like to know if there are categories of genes that are enriched among the differentially expressed genes. Gene Ontology (GO) analysis is widely used to reduce complexity and highlight biological processes in genome-wide expression studies. However, standard methods give biased results on RNA-seq data due to over-detection of differential expression for long and highly-expressed transcripts. The goseq tool provides methods for performing GO analysis of RNA-seq data, taking length bias into account. The methods and software used by goseq are equally applicable to other category based tests of RNA-seq data, such as KEGG pathway analysis.","title":"Introduction"},{"location":"reference_based_RNAseq/GO-intro/#analysis-of-functional-enrichment-among-the-differentially-expressed-genes","text":"We have extracted genes that are differentially expressed in treated (Pasilla gene-depleted) samples compared to untreated samples. We would like to know if there are categories of genes that are enriched among the differentially expressed genes. Gene Ontology (GO) analysis is widely used to reduce complexity and highlight biological processes in genome-wide expression studies. However, standard methods give biased results on RNA-seq data due to over-detection of differential expression for long and highly-expressed transcripts. The goseq tool provides methods for performing GO analysis of RNA-seq data, taking length bias into account. The methods and software used by goseq are equally applicable to other category based tests of RNA-seq data, such as KEGG pathway analysis.","title":"Analysis of functional enrichment among the differentially expressed genes"},{"location":"reference_based_RNAseq/GO-tool/","text":"Prepare the datasets for GOSeq Select Compute an expression on every row tool with Add expression : bool(c7 0.05) as a new column to : the DESeq2 result file Cut tool with Cut columns : c1,c8 Delimited by : Tab From : the output of the Compute tool Change Case tool with From : the output of the previous Cut tool Change case of columns : c1 Delimited by : Tab To : Upper case This generates the first input for goseq. We need as second input for goseq, the gene lengths. We can use there the gene length generated by featureCounts tool and reformat it a bit. Copy one output of type ...: Feature lengths of the 7 featureCounts runs in the history STAR / HISAT2 Rename it Lengths Change Case tool with From : the feature lengths (output of featureCounts tool) Change case of columns : c1 Delimited by : Tab To : Upper case We have now the two required input files for goseq. Perform GO analysis Select goseq tool with Differentially expressed genes file : first file generated by Change Case tool on previous step Gene lengths file : second file generated by Change Case tool on previous step Gene categories : Get categories Select a genome to use : Fruit fly (dm6) Select Gene ID format : Ensembl Gene ID Select one or more categories : GO: Cellular Component, GO: Biological Process, GO: Molecular Function goseq generates a big table with the following columns for each GO term: Column Description category GO category over_rep_pval p-value for over representation of the term in the differentially expressed genes under_rep_pval p-value for under representation of the term in the differentially expressed genes numDEInCat number of differentially expressed genes in this category numInCat number of genes in this category term detail of the term ontology MF (Molecular Function - molecular activities of gene products), CC (Cellular Component - where gene products are active), BP (Biological Process - pathways and larger processes made up of the activities of multiple gene products) p.adjust.over_represented p-value for over representation of the term in the differentially expressed genes, adjusted for multiple testing with the Benjamini-Hochberg procedure p.adjust.under_represented p-value for over representation of the term in the differentially expressed genes, adjusted for multiple testing with the Benjamini-Hochberg procedure To identify categories significantly enriched/unenriched below some p-value cutoff, it is necessary to use the adjusted p-value. How many GO terms are over-represented at adjusted P value 0.05? Under-represented? How are the over-represented GO terms divided between MF, CC and BP? And for under-represented GO terms?","title":"GO"},{"location":"reference_based_RNAseq/GO-tool/#prepare-the-datasets-for-goseq","text":"Select Compute an expression on every row tool with Add expression : bool(c7 0.05) as a new column to : the DESeq2 result file Cut tool with Cut columns : c1,c8 Delimited by : Tab From : the output of the Compute tool Change Case tool with From : the output of the previous Cut tool Change case of columns : c1 Delimited by : Tab To : Upper case This generates the first input for goseq. We need as second input for goseq, the gene lengths. We can use there the gene length generated by featureCounts tool and reformat it a bit. Copy one output of type ...: Feature lengths of the 7 featureCounts runs in the history STAR / HISAT2 Rename it Lengths Change Case tool with From : the feature lengths (output of featureCounts tool) Change case of columns : c1 Delimited by : Tab To : Upper case We have now the two required input files for goseq.","title":"Prepare the datasets for GOSeq"},{"location":"reference_based_RNAseq/GO-tool/#perform-go-analysis","text":"Select goseq tool with Differentially expressed genes file : first file generated by Change Case tool on previous step Gene lengths file : second file generated by Change Case tool on previous step Gene categories : Get categories Select a genome to use : Fruit fly (dm6) Select Gene ID format : Ensembl Gene ID Select one or more categories : GO: Cellular Component, GO: Biological Process, GO: Molecular Function goseq generates a big table with the following columns for each GO term: Column Description category GO category over_rep_pval p-value for over representation of the term in the differentially expressed genes under_rep_pval p-value for under representation of the term in the differentially expressed genes numDEInCat number of differentially expressed genes in this category numInCat number of genes in this category term detail of the term ontology MF (Molecular Function - molecular activities of gene products), CC (Cellular Component - where gene products are active), BP (Biological Process - pathways and larger processes made up of the activities of multiple gene products) p.adjust.over_represented p-value for over representation of the term in the differentially expressed genes, adjusted for multiple testing with the Benjamini-Hochberg procedure p.adjust.under_represented p-value for over representation of the term in the differentially expressed genes, adjusted for multiple testing with the Benjamini-Hochberg procedure To identify categories significantly enriched/unenriched below some p-value cutoff, it is necessary to use the adjusted p-value.","title":"Perform GO analysis"},{"location":"reference_based_RNAseq/GO-tool/#how-many-go-terms-are-over-represented-at-adjusted-p-value-005","text":"","title":"How many GO terms are over-represented at adjusted P value &lt; 0.05?"},{"location":"reference_based_RNAseq/GO-tool/#under-represented","text":"","title":"Under-represented?"},{"location":"reference_based_RNAseq/GO-tool/#how-are-the-over-represented-go-terms-divided-between-mf-cc-and-bp","text":"","title":"How are the over-represented GO terms divided between MF, CC and BP?"},{"location":"reference_based_RNAseq/GO-tool/#and-for-under-represented-go-terms","text":"","title":"And for under-represented GO terms?"},{"location":"reference_based_RNAseq/QC/","text":"Quality Control FastQC tool to analyse the fastq (or fastq.gz) datasets Create a new history and name it Quality Control Copy again all fastq.gz files from the data library into this history. You should have 11 datasets in your history Select the fastqc tool. In the Short read data from your current history menu, select the multiple datasets button. Shift-Click to select all 11 datasets Click Execute Look at the results of FastQC : These are the datasets named FastQC on data xx: Webpage MultiQC to aggregate and have a general view of sequence qualities in the project Select the MultiQC tool (you can use the search bar). Which tool was used generate logs? : Select FastQC Type of FastQC output? : Select Raw data FastQC output Cmd-Click (discontinuous, multiple selection) the 11 files named FastQC on xx: RawData Click Execute Look at the result of MultiQC , dataset named MultiQC on ...: Webpage Pay attention to the General Statistics that indicate the read sizes. Pay attention to the Sequence Quality Histograms . What can you say about the quality of the samples ? Have a look to the Adapter Content section.","title":"Quality control"},{"location":"reference_based_RNAseq/QC/#quality-control","text":"","title":"Quality Control"},{"location":"reference_based_RNAseq/QC/#fastqc-tool-to-analyse-the-fastq-or-fastqgz-datasets","text":"Create a new history and name it Quality Control Copy again all fastq.gz files from the data library into this history. You should have 11 datasets in your history Select the fastqc tool. In the Short read data from your current history menu, select the multiple datasets button. Shift-Click to select all 11 datasets Click Execute Look at the results of FastQC : These are the datasets named FastQC on data xx: Webpage","title":"FastQC tool to analyse the fastq (or fastq.gz) datasets"},{"location":"reference_based_RNAseq/QC/#multiqc-to-aggregate-and-have-a-general-view-of-sequence-qualities-in-the-project","text":"Select the MultiQC tool (you can use the search bar). Which tool was used generate logs? : Select FastQC Type of FastQC output? : Select Raw data FastQC output Cmd-Click (discontinuous, multiple selection) the 11 files named FastQC on xx: RawData Click Execute Look at the result of MultiQC , dataset named MultiQC on ...: Webpage Pay attention to the General Statistics that indicate the read sizes. Pay attention to the Sequence Quality Histograms . What can you say about the quality of the samples ? Have a look to the Adapter Content section.","title":"MultiQC to aggregate and have a general view of sequence qualities in the project"},{"location":"reference_based_RNAseq/RNAseq_DE/","text":"Statistical Analysis of Differential expression","title":"Statistical Analysis of Differential expression"},{"location":"reference_based_RNAseq/RNAseq_DE/#statistical-analysis-of-differential-expression","text":"","title":"Statistical Analysis of Differential expression"},{"location":"reference_based_RNAseq/bam/","text":"Inspection of BAM files Click on the small eye icon of a Bam dataset (generated either with RNA STAR or HISAT2 ) The header contains the chromosome specifications (their name and length) and other informations such as the software that generation the Bam file and the command line to run the software. A BAM file (or a SAM file, the non compressed version) consists of: A header section (the lines starting with @) containing metadata, in particular the chromosome names and lengths (lines starting with the @SQ symbol) An alignment section consisting of a table with 11 mandatory fields, as well as a variable number of optional fields: Col Field Type Brief Description 1 QNAME String Query template NAME 2 FLAG Integer bitwise FLAG 3 RNAME String References sequence NAME 4 POS Integer 1-based leftmost mapping POSition 5 MAPQ Integer MAPping Quality 6 CIGAR String CIGAR String 7 RNEXT String Ref. name of the mate/next read 8 PNEXT Integer Position of the mate/next read 9 TLEN Integer observed Template LENgth 10 SEQ String segment SEQuence 11 QUAL String ASCII of Phred-scaled base QUALity+33","title":"BAM inspection"},{"location":"reference_based_RNAseq/bam/#inspection-of-bam-files","text":"Click on the small eye icon of a Bam dataset (generated either with RNA STAR or HISAT2 ) The header contains the chromosome specifications (their name and length) and other informations such as the software that generation the Bam file and the command line to run the software. A BAM file (or a SAM file, the non compressed version) consists of: A header section (the lines starting with @) containing metadata, in particular the chromosome names and lengths (lines starting with the @SQ symbol) An alignment section consisting of a table with 11 mandatory fields, as well as a variable number of optional fields: Col Field Type Brief Description 1 QNAME String Query template NAME 2 FLAG Integer bitwise FLAG 3 RNAME String References sequence NAME 4 POS Integer 1-based leftmost mapping POSition 5 MAPQ Integer MAPping Quality 6 CIGAR String CIGAR String 7 RNEXT String Ref. name of the mate/next read 8 PNEXT Integer Position of the mate/next read 9 TLEN Integer observed Template LENgth 10 SEQ String segment SEQuence 11 QUAL String ASCII of Phred-scaled base QUALity+33","title":"Inspection of BAM files"},{"location":"reference_based_RNAseq/cDNAs/","text":"cDNA synthesis Oligo-dT Random priming","title":"RNAseq librairies - cDNA synthesis"},{"location":"reference_based_RNAseq/cDNAs/#cdna-synthesis","text":"","title":"cDNA synthesis"},{"location":"reference_based_RNAseq/cDNAs/#oligo-dt","text":"","title":"Oligo-dT"},{"location":"reference_based_RNAseq/cDNAs/#random-priming","text":"","title":"Random priming"},{"location":"reference_based_RNAseq/count/","text":"featureCounts In your history HISAT2 or STAR Select the featureCounts tool with the following parameters to count your reads: Alignment file : select multiple datasets button and shift-click the 7 bam files you have generated Specify strand information : Unstranded Gene annotation file : in your history Gene annotation file : Drosophila_melanogaster.BDGP6.95.gtf FASTA/Q file : Gene-ID \"\\t\" read-count (MultiQC/DESeq2/edgeR/limma-voom compatible) Create gene-length file : Yes In Options for paired-end reads : Count fragments instead of reads : Enabled; fragments (or templates) will be counted instead of reads In Advanced options : GFF feature type filter : exon GFF gene identifier : gene_id Allow read to contribute to multiple features : No Count multi-mapping reads/fragments : Disabled; multi-mapping reads are excluded (default) Minimum mapping quality per read : 10 Leave other settings as defaults Execute You need now to rename you datasets to facilitate your downstream analysis. Be quiet and focus ! No hurry, this is an important task in the analysis. Search and select datasets with featurecounts (as we did before for renaming datasets) Click on the info icon of featureCounts on xxx: Counts Copy the name of the dataset Alignment file in the tool parameters table (for instance, GSM461176_untreat_single.bam ) Now click on the pencil icon of the same dataset Paste your text in the Name field of the dataset Edit your text by replacing bam by Counts (e.g. GSM461176_untreat_single.Counts) repeat ad lib for all counts files generated by featureCounts MultiQC We have now generated (1) Bam alignments and (2) Counts files with feature counts, and we have carefully and courageously edited the names of generated datasets. We are going to be rewarded for this effort in the next steps ! In your history HISAT2 or STAR Select the MultiQC tool with the following parameters: 1: Results / Which tool was used generate logs? : STAR or HISAT2 (depending on your analysis track) STAR or HISAT output : shift-click select all files with the extension .log Click on the + Insert Result button 2: Results / Which tool was used generate logs? : featureCounts Output of FeatureCounts : shift-click select all files with the extension : Summary Execute Examine the results and","title":"Count the number of reads per annotated gene"},{"location":"reference_based_RNAseq/count/#featurecounts","text":"In your history HISAT2 or STAR Select the featureCounts tool with the following parameters to count your reads: Alignment file : select multiple datasets button and shift-click the 7 bam files you have generated Specify strand information : Unstranded Gene annotation file : in your history Gene annotation file : Drosophila_melanogaster.BDGP6.95.gtf FASTA/Q file : Gene-ID \"\\t\" read-count (MultiQC/DESeq2/edgeR/limma-voom compatible) Create gene-length file : Yes In Options for paired-end reads : Count fragments instead of reads : Enabled; fragments (or templates) will be counted instead of reads In Advanced options : GFF feature type filter : exon GFF gene identifier : gene_id Allow read to contribute to multiple features : No Count multi-mapping reads/fragments : Disabled; multi-mapping reads are excluded (default) Minimum mapping quality per read : 10 Leave other settings as defaults Execute You need now to rename you datasets to facilitate your downstream analysis. Be quiet and focus ! No hurry, this is an important task in the analysis. Search and select datasets with featurecounts (as we did before for renaming datasets) Click on the info icon of featureCounts on xxx: Counts Copy the name of the dataset Alignment file in the tool parameters table (for instance, GSM461176_untreat_single.bam ) Now click on the pencil icon of the same dataset Paste your text in the Name field of the dataset Edit your text by replacing bam by Counts (e.g. GSM461176_untreat_single.Counts) repeat ad lib for all counts files generated by featureCounts","title":"featureCounts"},{"location":"reference_based_RNAseq/count/#multiqc","text":"We have now generated (1) Bam alignments and (2) Counts files with feature counts, and we have carefully and courageously edited the names of generated datasets. We are going to be rewarded for this effort in the next steps ! In your history HISAT2 or STAR Select the MultiQC tool with the following parameters: 1: Results / Which tool was used generate logs? : STAR or HISAT2 (depending on your analysis track) STAR or HISAT output : shift-click select all files with the extension .log Click on the + Insert Result button 2: Results / Which tool was used generate logs? : featureCounts Output of FeatureCounts : shift-click select all files with the extension : Summary Execute Examine the results and","title":"MultiQC"},{"location":"reference_based_RNAseq/hisat2/","text":"HISAT2 (option for 50 % of attendees) create a new history and name it HISAT2 Import the 11 datasets from the RNAseq data library to this HISAT2 history, plus the Drosophila_melanogaster.BDGP6.95.gtf file Select the HISAT2 tool with the following parameters to map your reads on the reference genome: Source for the reference genome : Use a builtin genome Select a reference genome : dm6 Is this a single or paired library: Single-End FASTA/Q file : GSM461176_untreat_single.fastq.gz GSM461179_treat_single.fastq.gz GSM461182_untreat_single.fastq.gz Specify strand information: Unstranded Summary options Output alignment summary in a more machine-friendly style. : YES Print alignment summary to a file. : YES Leave other settings as defaults Execute Redo the HISAT2 run for paired-end files Rerun the HISAT2 tool with the following parameters to map your reads on the reference genome: Source for the reference genome : Use a builtin genome Select a reference genome : dm6 Is this a single or paired library: Paired-End FASTA/Q file #1 : GSM461177_1_untreat_paired.fastq.gz GSM461178_1_untreat_paired.fastq.gz GSM461180_1_treat_paired.fastq.gz GSM461181_1_treat_paired.fastq.gz FASTA/Q file #2 : GSM461177_2_untreat_paired.fastq.gz GSM461178_2_untreat_paired.fastq.gz GSM461180_2_treat_paired.fastq.gz GSM461181_2_treat_paired.fastq.gz Specify strand information: Unstranded Leave other settings as defaults (since you are redoing a run) Execute Rename your datasets ! You need now to rename you datasets to facilitate your downstream analysis. Be quiet and focus ! No hurry, this is an important task in the analysis. Search and select datasets with HISAT2 Click on the info icon of both (BAM) and Mapping summary files Copy the name or one of the two names of the datasets as shown bellow Now click on the pencil icon of the same dataset Paste your text in the Name field of the dataset Edit your text as follow for Mapping summary files Edit your text as follow for (BAM) files repeat ad lib for all Mapping summary and (BAM) files","title":"HiSAT2"},{"location":"reference_based_RNAseq/hisat2/#hisat2-option-for-50-of-attendees","text":"create a new history and name it HISAT2 Import the 11 datasets from the RNAseq data library to this HISAT2 history, plus the Drosophila_melanogaster.BDGP6.95.gtf file Select the HISAT2 tool with the following parameters to map your reads on the reference genome: Source for the reference genome : Use a builtin genome Select a reference genome : dm6 Is this a single or paired library: Single-End FASTA/Q file : GSM461176_untreat_single.fastq.gz GSM461179_treat_single.fastq.gz GSM461182_untreat_single.fastq.gz Specify strand information: Unstranded Summary options Output alignment summary in a more machine-friendly style. : YES Print alignment summary to a file. : YES Leave other settings as defaults Execute Redo the HISAT2 run for paired-end files Rerun the HISAT2 tool with the following parameters to map your reads on the reference genome: Source for the reference genome : Use a builtin genome Select a reference genome : dm6 Is this a single or paired library: Paired-End FASTA/Q file #1 : GSM461177_1_untreat_paired.fastq.gz GSM461178_1_untreat_paired.fastq.gz GSM461180_1_treat_paired.fastq.gz GSM461181_1_treat_paired.fastq.gz FASTA/Q file #2 : GSM461177_2_untreat_paired.fastq.gz GSM461178_2_untreat_paired.fastq.gz GSM461180_2_treat_paired.fastq.gz GSM461181_2_treat_paired.fastq.gz Specify strand information: Unstranded Leave other settings as defaults (since you are redoing a run) Execute","title":"HISAT2  (option for 50 % of attendees)"},{"location":"reference_based_RNAseq/hisat2/#rename-your-datasets","text":"You need now to rename you datasets to facilitate your downstream analysis. Be quiet and focus ! No hurry, this is an important task in the analysis. Search and select datasets with HISAT2 Click on the info icon of both (BAM) and Mapping summary files Copy the name or one of the two names of the datasets as shown bellow Now click on the pencil icon of the same dataset Paste your text in the Name field of the dataset Edit your text as follow for Mapping summary files Edit your text as follow for (BAM) files repeat ad lib for all Mapping summary and (BAM) files","title":"Rename your datasets !"},{"location":"reference_based_RNAseq/intro_counting/","text":"Counting strategy Count the number of reads per annotated gene To compare the expression of single genes between different conditions (e.g. with or without Pasilla depletion), an essential first step is to quantify the number of reads per gene. From the image above, we can compute: Number of reads per exons Gene Exon Number of reads gene1 exon1 3 gene1 exon2 2 gene2 exon1 3 gene2 exon2 4 gene2 exon3 3 The gene1 has 4 reads, not 5 (gene1 - exon1 + gene1 - exon2) because of the splicing of the last read. The gene2 has 6 reads (3 spliced reads) Counting tools Two main tools could be used for that: HTSeq-count ( Anders et al, Bioinformatics, 2015 ) or featureCounts ( Liao et al, Bioinformatics, 2014 ). FeatureCounts is considerably faster and requires far less computational resources, so we will use it here. In principle, the counting of reads overlapping with genomic features is a fairly simple task. But there are some details that need to be given to featureCounts: for example the strandness.","title":"Introduction to read counting"},{"location":"reference_based_RNAseq/intro_counting/#counting-strategy","text":"","title":"Counting strategy"},{"location":"reference_based_RNAseq/intro_counting/#count-the-number-of-reads-per-annotated-gene","text":"To compare the expression of single genes between different conditions (e.g. with or without Pasilla depletion), an essential first step is to quantify the number of reads per gene. From the image above, we can compute:","title":"Count the number of reads per annotated gene"},{"location":"reference_based_RNAseq/intro_counting/#number-of-reads-per-exons","text":"Gene Exon Number of reads gene1 exon1 3 gene1 exon2 2 gene2 exon1 3 gene2 exon2 4 gene2 exon3 3 The gene1 has 4 reads, not 5 (gene1 - exon1 + gene1 - exon2) because of the splicing of the last read. The gene2 has 6 reads (3 spliced reads)","title":"Number of reads per exons"},{"location":"reference_based_RNAseq/intro_counting/#counting-tools","text":"Two main tools could be used for that: HTSeq-count ( Anders et al, Bioinformatics, 2015 ) or featureCounts ( Liao et al, Bioinformatics, 2014 ). FeatureCounts is considerably faster and requires far less computational resources, so we will use it here. In principle, the counting of reads overlapping with genomic features is a fairly simple task. But there are some details that need to be given to featureCounts: for example the strandness.","title":"Counting tools"},{"location":"reference_based_RNAseq/mapping/","text":"Mapping the reads to the reference genome","title":"Mapping"},{"location":"reference_based_RNAseq/mapping/#mapping-the-reads-to-the-reference-genome","text":"","title":"Mapping the reads to the reference genome"},{"location":"reference_based_RNAseq/outline_conclusion/","text":"Experimental procedures affect downstream analyses","title":"Experimental procedures affect downstream analyses"},{"location":"reference_based_RNAseq/outline_conclusion/#experimental-procedures-affect-downstream-analyses","text":"","title":"Experimental procedures affect downstream analyses"},{"location":"reference_based_RNAseq/read_filtering/","text":"Focus on quality control \u201cfiltering\u201d in RNAseq analysis It is tempting to filter the data to get \u201cgood counts\u201d low quality alignments PCR duplicates But.. Why low quality reads should be skipped if they were aligned ? Is the implicit hypothesis \"low quality read are miss-mapped\" a likely hypothesis ? When we remove PCR duplicates (exact same sequence and exact same location), are we sure that we remove PCR duplicates ? What are the metrics that support the implicit hypothesis that read with same sequence same location are PCR duplicates ? Reflect of miRNA sequencing...","title":"Reflecting on quality control & \u201cfiltering\u201d in RNAseq analysis"},{"location":"reference_based_RNAseq/read_filtering/#focus-on-quality-control-filtering-in-rnaseq-analysis","text":"","title":"Focus on quality control &amp; \u201cfiltering\u201d in RNAseq analysis"},{"location":"reference_based_RNAseq/read_filtering/#it-is-tempting-to-filter-the-data-to-get-good-counts","text":"low quality alignments PCR duplicates","title":"It is tempting to filter the data to get \u201cgood counts\u201d"},{"location":"reference_based_RNAseq/read_filtering/#but","text":"Why low quality reads should be skipped if they were aligned ? Is the implicit hypothesis \"low quality read are miss-mapped\" a likely hypothesis ? When we remove PCR duplicates (exact same sequence and exact same location), are we sure that we remove PCR duplicates ? What are the metrics that support the implicit hypothesis that read with same sequence same location are PCR duplicates ? Reflect of miRNA sequencing...","title":"But.."},{"location":"reference_based_RNAseq/readcounts/","text":"Reference-base Expression analysis: the key idea Map reads to a reference genome with aligners TopHat TopHat2 HiSat HiSat2 STAR \u2192 These aligners are \u201csplice aware\u201d \u2192 They generate a BAM Alignment file Use a read counting software and annotation information (GTF, GFF3, BED, \u2026) to count the read spanning a gene / transcript The input file for this counting software is the BAM Alignment file Read counts are proxies to RNA steady state levels","title":"The key idea in Reference-base Expression analysis"},{"location":"reference_based_RNAseq/readcounts/#reference-base-expression-analysis-the-key-idea","text":"","title":"Reference-base Expression analysis: the key idea"},{"location":"reference_based_RNAseq/readcounts/#map-reads-to-a-reference-genome-with-aligners","text":"TopHat TopHat2 HiSat HiSat2 STAR \u2192 These aligners are \u201csplice aware\u201d \u2192 They generate a BAM Alignment file","title":"Map reads to a reference genome with aligners"},{"location":"reference_based_RNAseq/readcounts/#use-a-read-counting-software-and-annotation-information-gtf-gff3-bed-to-count-the-read-spanning-a-gene-transcript","text":"The input file for this counting software is the BAM Alignment file","title":"Use a read counting software and annotation information (GTF, GFF3, BED, \u2026) to count the read spanning a gene / transcript"},{"location":"reference_based_RNAseq/readcounts/#read-counts-are-proxies-to-rna-steady-state-levels","text":"","title":"Read counts are proxies to RNA steady state levels"},{"location":"reference_based_RNAseq/sequencing_strategies/","text":"Inserts and sequencing strategies You can retrieve three different informations : The relative orientation of reads : I : Inwards M : Matching O : Outwards The strandedness of the library : S : Stranded U : Unstranded The strand origin of reads : F : read 1 (or single-end read) comes from the forward strand R : read 1 (or single-end read) comes from the reverse strand in practice, with Illumina paired-end RNAseq protocols you will either deal with: Unstranded RNAseq data IU type from above. Also called fr-unstranded in TopHat/Cufflinks nomenclature Stranded RNAseq data produced with Illumina TrueSeq RNAseq kits ISR type from above or fr-firststrand in TopHat/Cufflinks nomenclature","title":"RNAseq librairies - Inserts and sequencing strategies"},{"location":"reference_based_RNAseq/sequencing_strategies/#inserts-and-sequencing-strategies","text":"You can retrieve three different informations : The relative orientation of reads : I : Inwards M : Matching O : Outwards The strandedness of the library : S : Stranded U : Unstranded The strand origin of reads : F : read 1 (or single-end read) comes from the forward strand R : read 1 (or single-end read) comes from the reverse strand","title":"Inserts and sequencing strategies"},{"location":"reference_based_RNAseq/sequencing_strategies/#in-practice-with-illumina-paired-end-rnaseq-protocols-you-will-either-deal-with","text":"","title":"in practice, with Illumina paired-end RNAseq protocols you will either deal with:"},{"location":"reference_based_RNAseq/sequencing_strategies/#unstranded-rnaseq-data","text":"IU type from above. Also called fr-unstranded in TopHat/Cufflinks nomenclature","title":"Unstranded RNAseq data"},{"location":"reference_based_RNAseq/sequencing_strategies/#stranded-rnaseq-data-produced-with-illumina-trueseq-rnaseq-kits","text":"ISR type from above or fr-firststrand in TopHat/Cufflinks nomenclature","title":"Stranded RNAseq data produced with Illumina TrueSeq RNAseq kits"},{"location":"reference_based_RNAseq/star/","text":"RNA STAR (option for 50 % of attendees) For information to set proper value for STAR parameters: create a new history and name it RNA STAR Import the 11 datasets from the RNAseq data library to this RNA STAR history, plus the Drosophila_melanogaster.BDGP6.95.gtf file Select the RNA STAR tool with the following parameters to map your reads on the reference genome: Single-end or paired-end reads : Single-end RNA-Seq FASTQ/FASTA file (multiple datasets button), Cmd-shift Select: GSM461176_untreat_single.fastq.gz GSM461179_treat_single.fastq.gz Custom or built-in reference genome: Use a built-in index Reference genome with or without an annotation: use genome reference without builtin gene-model Select reference genome: Drosophila Melanogaster (dm6) Gene model (gff3,gtf) file for splice junctions: the imported Drosophila_melanogaster.BDGP6.95.gtf Length of the genomic sequence around annotated junctions: 44 (This parameter should be length of reads - 1, see above table from fastQC/multiQC analysis) Execute Redo the STAR run with 3.2 Select the RNA STAR tool with the following parameters to map your reads on the reference genome: RNA-Seq FASTQ/FASTA file (as multiple datasets), Cmd-shift Select: - GSM461182_untreat_single.fastq.gz 3.7 Length of the genomic sequence around annotated junctions: 74 (This parameter should be length of reads - 1, see above table from fastQC/multiQC analysis) Redo a last STAR run for paired-end datasets With the following parameters to map your reads on the reference genome: Single-end or paired-end reads : Paired-end (as multiple datasets) RNA-Seq FASTQ/FASTA file, forward reads (multiple datasets button), Cmd-shift Select: GSM461177_1_untreat_paired.fastq.gz GSM461178_1_untreat_paired.fastq.gz GSM461180_1_treat_paired.fastq.gz `GSM461181_1_treat_paired.fastq.gz RNA-Seq FASTQ/FASTA file, forward reads (multiple datasets button), Cmd-shift Select: GSM461177_2_untreat_paired.fastq.gz GSM461178_2_untreat_paired.fastq.gz GSM461180_2_treat_paired.fastq.gz `GSM461181_2_treat_paired.fastq.gz Custom or built-in reference genome: Use a built-in index Reference genome with or without an annotation: use genome reference without builtin gene-model Select reference genome: Drosophila Melanogaster (dm6) Gene model (gff3,gtf) file for splice junctions: the imported Drosophila_melanogaster.BDGP6.95.gtf Length of the genomic sequence around annotated junctions: 36 (This parameter should be length of reads - 1, see above table from fastQC/multiQC analysis) Execute Rename your datasets ! You need now to rename you datasets to facilitate your downstream analysis. Be quiet and focus ! No hurry, this is an important task in the analysis. Search and select datasets with RNA STAR Click on the info icon of both log and bam files Copy the name or one of the two names of the datasets as shown bellow Now click on the pencil icon of the same dataset Paste your text in the Name field of the dataset Edit your text as follow for log files Edit your text as follow for bam files repeat ad lib for all log and bam files","title":"RNA STAR"},{"location":"reference_based_RNAseq/star/#rna-star-option-for-50-of-attendees","text":"For information to set proper value for STAR parameters: create a new history and name it RNA STAR Import the 11 datasets from the RNAseq data library to this RNA STAR history, plus the Drosophila_melanogaster.BDGP6.95.gtf file Select the RNA STAR tool with the following parameters to map your reads on the reference genome: Single-end or paired-end reads : Single-end RNA-Seq FASTQ/FASTA file (multiple datasets button), Cmd-shift Select: GSM461176_untreat_single.fastq.gz GSM461179_treat_single.fastq.gz Custom or built-in reference genome: Use a built-in index Reference genome with or without an annotation: use genome reference without builtin gene-model Select reference genome: Drosophila Melanogaster (dm6) Gene model (gff3,gtf) file for splice junctions: the imported Drosophila_melanogaster.BDGP6.95.gtf Length of the genomic sequence around annotated junctions: 44 (This parameter should be length of reads - 1, see above table from fastQC/multiQC analysis) Execute Redo the STAR run with 3.2 Select the RNA STAR tool with the following parameters to map your reads on the reference genome: RNA-Seq FASTQ/FASTA file (as multiple datasets), Cmd-shift Select: - GSM461182_untreat_single.fastq.gz 3.7 Length of the genomic sequence around annotated junctions: 74 (This parameter should be length of reads - 1, see above table from fastQC/multiQC analysis) Redo a last STAR run for paired-end datasets With the following parameters to map your reads on the reference genome: Single-end or paired-end reads : Paired-end (as multiple datasets) RNA-Seq FASTQ/FASTA file, forward reads (multiple datasets button), Cmd-shift Select: GSM461177_1_untreat_paired.fastq.gz GSM461178_1_untreat_paired.fastq.gz GSM461180_1_treat_paired.fastq.gz `GSM461181_1_treat_paired.fastq.gz RNA-Seq FASTQ/FASTA file, forward reads (multiple datasets button), Cmd-shift Select: GSM461177_2_untreat_paired.fastq.gz GSM461178_2_untreat_paired.fastq.gz GSM461180_2_treat_paired.fastq.gz `GSM461181_2_treat_paired.fastq.gz Custom or built-in reference genome: Use a built-in index Reference genome with or without an annotation: use genome reference without builtin gene-model Select reference genome: Drosophila Melanogaster (dm6) Gene model (gff3,gtf) file for splice junctions: the imported Drosophila_melanogaster.BDGP6.95.gtf Length of the genomic sequence around annotated junctions: 36 (This parameter should be length of reads - 1, see above table from fastQC/multiQC analysis) Execute","title":"RNA STAR (option for 50 % of attendees)"},{"location":"reference_based_RNAseq/star/#rename-your-datasets","text":"You need now to rename you datasets to facilitate your downstream analysis. Be quiet and focus ! No hurry, this is an important task in the analysis. Search and select datasets with RNA STAR Click on the info icon of both log and bam files Copy the name or one of the two names of the datasets as shown bellow Now click on the pencil icon of the same dataset Paste your text in the Name field of the dataset Edit your text as follow for log files Edit your text as follow for bam files repeat ad lib for all log and bam files","title":"Rename your datasets !"},{"location":"reference_based_RNAseq/strandness/","text":"Estimation of the strandness In practice, with Illumina RNA-seq protocols you will most likely deal with either: Unstranded RNAseq data Stranded RNA-seq data produced with - kits and dUTP tagging (ISR) This information should be provided with your FASTQ files, ask your sequencing facility! If not, try to find it on the site where you downloaded the data or in the corresponding publication. Another option is to estimate these parameters with a tool called Infer Experiment from the RSeQC tool suite. This tool takes the output of your mappings (BAM files), selects a subsample of your reads and compares their genome coordinates and strands with those of the reference gene model (from an annotation file). Based on the strand of the genes, it can gauge whether sequencing is strand-specific, and if so, how reads are stranded. Use of Infer Experiment tool Convert GTF to BED12 tool to convert the GTF file to BED Go to your history STAR or HISAT2 Select the tool Convert GTF to BED12 GTF File to convert : Drosophila_melanogaster.BDGP6.95.gtf Execute Infer Experiment tool to determine the library strandness In the same history STAR or HISAT2 Select the tool Infer Experiment Input .bam file : mapped.bam files (outputs of RNA STAR or HISAT2 tools) Reference gene model : BED12 file (output of Convert GTF to BED12 tool) Number of reads sampled from SAM/BAM file (default = 200000) : 200000 Execute Summarize results with MultiQC tool Select the tool MultiQC Which tool was used generate logs? RSeQC RSeQC output (Type of RSeQC output?) infer_experiment Select the 7 datasets of type Infer Experiment on ... Execute Infer Experiment tool generates one file with information on: Paired-end or single-end library Fraction of reads failed to determine 2 lines: For single-end Fraction of reads explained by \u201c++,\u2013\u201d (SF in previous figure) Fraction of reads explained by \u201c+-,-+\u201d (SR in previous figure) For paired-end Fraction of reads explained by \u201c1++,1\u2013,2+-,2-+\u201d (SF in previous figure) Fraction of reads explained by \u201c1+-,1-+,2++,2\u2013\u201d (SR in previous figure) If the two \u201cFraction of reads explained by\u201d numbers are close to each other ( i.e. a mix of SF and SR), we conclude that the library is not a strand-specific dataset (U in previous figure). As it is sometimes quite difficult to find out which settings correspond to those of other programs, the following table might be helpful to identify the library type: Library type Infer Experiment TopHat HISAT htseq-count featureCounts Paired-End (PE) - SF 1++,1\u2013,2+-,2-+ FR Second Strand Second Strand F/FR yes Forward (1) PE - SR 1+-,1-+,2++,2\u2013 FR First Strand First Strand R/RF reverse Reverse (2) Single-End (SE) - SF +,\u2013 FR Second Strand Second Strand F/FR yes Forward (1) SE - SR +-,-+ FR First Strand First Strand R/RF reverse Reverse (2) PE, SE - U undecided FR Unstranded default no Unstranded (0)","title":"Estimation of the strandness"},{"location":"reference_based_RNAseq/strandness/#estimation-of-the-strandness","text":"In practice, with Illumina RNA-seq protocols you will most likely deal with either: Unstranded RNAseq data Stranded RNA-seq data produced with - kits and dUTP tagging (ISR) This information should be provided with your FASTQ files, ask your sequencing facility! If not, try to find it on the site where you downloaded the data or in the corresponding publication. Another option is to estimate these parameters with a tool called Infer Experiment from the RSeQC tool suite. This tool takes the output of your mappings (BAM files), selects a subsample of your reads and compares their genome coordinates and strands with those of the reference gene model (from an annotation file). Based on the strand of the genes, it can gauge whether sequencing is strand-specific, and if so, how reads are stranded.","title":"Estimation of the strandness"},{"location":"reference_based_RNAseq/strandness/#use-of-infer-experiment-tool","text":"","title":"Use of Infer Experiment tool"},{"location":"reference_based_RNAseq/strandness/#convert-gtf-to-bed12-tool-to-convert-the-gtf-file-to-bed","text":"Go to your history STAR or HISAT2 Select the tool Convert GTF to BED12 GTF File to convert : Drosophila_melanogaster.BDGP6.95.gtf Execute","title":"Convert GTF to BED12 tool to convert the GTF file to BED"},{"location":"reference_based_RNAseq/strandness/#infer-experiment-tool-to-determine-the-library-strandness","text":"In the same history STAR or HISAT2 Select the tool Infer Experiment Input .bam file : mapped.bam files (outputs of RNA STAR or HISAT2 tools) Reference gene model : BED12 file (output of Convert GTF to BED12 tool) Number of reads sampled from SAM/BAM file (default = 200000) : 200000 Execute","title":"Infer Experiment tool to determine the library strandness"},{"location":"reference_based_RNAseq/strandness/#summarize-results-with-multiqc-tool","text":"Select the tool MultiQC Which tool was used generate logs? RSeQC RSeQC output (Type of RSeQC output?) infer_experiment Select the 7 datasets of type Infer Experiment on ... Execute Infer Experiment tool generates one file with information on: Paired-end or single-end library Fraction of reads failed to determine 2 lines: For single-end Fraction of reads explained by \u201c++,\u2013\u201d (SF in previous figure) Fraction of reads explained by \u201c+-,-+\u201d (SR in previous figure) For paired-end Fraction of reads explained by \u201c1++,1\u2013,2+-,2-+\u201d (SF in previous figure) Fraction of reads explained by \u201c1+-,1-+,2++,2\u2013\u201d (SR in previous figure) If the two \u201cFraction of reads explained by\u201d numbers are close to each other ( i.e. a mix of SF and SR), we conclude that the library is not a strand-specific dataset (U in previous figure). As it is sometimes quite difficult to find out which settings correspond to those of other programs, the following table might be helpful to identify the library type: Library type Infer Experiment TopHat HISAT htseq-count featureCounts Paired-End (PE) - SF 1++,1\u2013,2+-,2-+ FR Second Strand Second Strand F/FR yes Forward (1) PE - SR 1+-,1-+,2++,2\u2013 FR First Strand First Strand R/RF reverse Reverse (2) Single-End (SE) - SF +,\u2013 FR Second Strand Second Strand F/FR yes Forward (1) SE - SR +-,-+ FR First Strand First Strand R/RF reverse Reverse (2) PE, SE - U undecided FR Unstranded default no Unstranded (0)","title":"Summarize results with MultiQC tool"},{"location":"reference_based_RNAseq/transcript_quant/","text":"Transcript Quantification Note that we use absolute read counts because we are going to compare counts across samples. Other metrics for comparison of genes within the same sample are: CPM (Counts Per Million) Each gene count is divided by the corresponding library size (in millions). RPKM (reads per kilobase of exons per million mapped reads) TPM (Transcript per Million) Divide the read counts by the length of each gene in kilobases. This gives you reads per kilobase (RPK). Sum up all the RPK values in a sample and divide this number by 1,000,000. This is your \u201cper million\u201d scaling factor. Divide the RPK values by the \u201cper million\u201d scaling factor. This gives you TPM","title":"Transcript Quantification"},{"location":"reference_based_RNAseq/transcript_quant/#transcript-quantification","text":"Note that we use absolute read counts because we are going to compare counts across samples. Other metrics for comparison of genes within the same sample are: CPM (Counts Per Million) Each gene count is divided by the corresponding library size (in millions). RPKM (reads per kilobase of exons per million mapped reads) TPM (Transcript per Million) Divide the read counts by the length of each gene in kilobases. This gives you reads per kilobase (RPK). Sum up all the RPK values in a sample and divide this number by 1,000,000. This is your \u201cper million\u201d scaling factor. Divide the RPK values by the \u201cper million\u201d scaling factor. This gives you TPM","title":"Transcript Quantification"},{"location":"reference_based_RNAseq/uploads/","text":"Data The original data is available at NCBI Gene Expression Omnibus (GEO) under accession number GSE18508 . It is also mirrored at the EBI Small Read Archive under the accession number SRP001537 The data was generated through deep Sequencing of mRNA from the Drosophila melanogaster S2-DRSC cells that have been RNAi depleted of mRNAs encoding RNA binding proteins. In the tutorial, we are going to focus on 7 datasets generated to study the effect of the Pasilla gene inactivation by RNAi knock-down. 4 untreated samples: GSM461176, GSM461177, GSM461178, GSM461182 3 treated samples (Pasilla gene depleted by RNAi): GSM461179, GSM461180, GSM461181 Each sample constitutes a separate biological replicate of the corresponding condition (treated or untreated). Two of the treated and two of the untreated samples are from a paired-end sequencing assay, while the remaining samples are from a single-end sequencing experiment. Thus the following table will be (very) useful in our analysis since each of the 7 datasets are designated with (i) its original ID in GEO (or EBI SRA) (ii) its condition (untreated or treated) and (iii) the sequencing technology used (single read or paired-end). id. in GEO id. in EBI SRA GSM461176_untreat_single SRR031709_untreat_single GSM461177_untreat_paired SRR031714_untreat_paired GSM461178_untreat_paired SRR031716_untreat_paired GSM461179_treat_single SRR031718_treat_single GSM461180_treat_paired SRR031724_treat_paired GSM461181_treat_paired SRR031726_treat_paired GSM461182_untreat_single SRR031728_untreat_single Data upload We will take benefit of this mandatory stage, to review various possibilities to upload datasets in Galaxy. Specifically, we will review two options for uploading the gtf annotations for the Drosophila genome dm6 in a Galaxy history. We will also have a look to a third option that allows specifically to directly transfer FASTQ sequence files from the EBI SRA to a Galaxy history. Transfers of Big Files take time, especially when the Internet connection speed is moderate to low... To avoid consuming too much time on this task, you will have the possibility to import the full set of the 11 FASTQ files in one of your histories, from a data library that has been pre-set in your Galaxy server for this training session. Uploading data from your local computer Download from the Ensembl database the sample Drosophila_melanogaster.BDGP6.95.gtf.gz to your computer. Upload this local file Drosophila_melanogaster.BDGP6.95.gtf.gz to your Galaxy history using the upload/Download Galaxy interface that pops up if you click the upload icone Importing data via links is more efficient and reliable ! The previous strategy is not efficient. Indeed, we can directly transfert the Drosophila_melanogaster.BDGP6.95.gtf.gz from its primary location in the Ensembl database server to your Galaxy History ! Copy its URL below 1 ftp://ftp.ensembl.org/pub/release-95/gtf/drosophila_melanogaster/Drosophila_melanogaster.BDGP6.95.gtf.gz and paste it in the Paste/Fetch data tab of the Galaxy upload interface. In addition, select gtf in the Type menu. Press the start button. Importing data via the EBI SRA ENA SRA Finally there is a tool to specifically fetch fastq sequence file from the EBI SRA to Galaxy The sample GSM461178/SRR031716 was sequenced using a paired-end strategy (both ends of fragments in the library are sequenced, giving rise to 2 read files, a forward read fastq file and a reverse read fastq file). We are going to download the fastq.gz files directly from the EBI SRA using the tool EBI SRA ENA SRA in the Get data tool submenu. Click on the tool EBI SRA ENA SRA (you can select it rapidly using the search bar) In the search box of the EBI SRA website, enter SRR031716 Two categories of results are retrieved, Experiment and Run. What we want to get are the files from the sequencing runs. Thus, click the SRR031716 link in the Run section (1 results found). Click on \"File 1\" in the FASTQ files (Galaxy) Column. You will be switched back to the Galaxy interface, and the download of the SRR031716_1.fastq.gz file will start immediately as a yellow dataset in the history right panel. Without waiting for the complete download of SRR031716_1.fastq.gz, you can repeat the previous steps 1, 2, 3 and 4. Just Click on File 2 instead of File 1 in step 4 Then, to save time, stop the tools (by clicking the small cross) and go to the next section. Importing data from data libraries For collaborative work, Galaxy offers data libraries, where datasets can be stored and available to one or multiple users. This is what we are going to use to import rapidly all the input data you need for this RNAseq analysis. All datasets have been preloaded in the data library named RNAseq . To access this library and import its content in your histories: Click the menu Donn\u00e9es partag\u00e9es ( Shared data ) and select the submenu Biblioth\u00e8que de Donn\u00e9es ( Data libraries ). Navigate to the data library RNAseq Select all datasets Click the To History button and select as Datasets In the pop up window, or create new and type Input data to transfer the datasets in a new history with this name. Click on the green box to navigate to this new history (or click on the main menu analyse data ) and start using these datasets.","title":"Data upload"},{"location":"reference_based_RNAseq/uploads/#data","text":"The original data is available at NCBI Gene Expression Omnibus (GEO) under accession number GSE18508 . It is also mirrored at the EBI Small Read Archive under the accession number SRP001537 The data was generated through deep Sequencing of mRNA from the Drosophila melanogaster S2-DRSC cells that have been RNAi depleted of mRNAs encoding RNA binding proteins. In the tutorial, we are going to focus on 7 datasets generated to study the effect of the Pasilla gene inactivation by RNAi knock-down. 4 untreated samples: GSM461176, GSM461177, GSM461178, GSM461182 3 treated samples (Pasilla gene depleted by RNAi): GSM461179, GSM461180, GSM461181 Each sample constitutes a separate biological replicate of the corresponding condition (treated or untreated). Two of the treated and two of the untreated samples are from a paired-end sequencing assay, while the remaining samples are from a single-end sequencing experiment. Thus the following table will be (very) useful in our analysis since each of the 7 datasets are designated with (i) its original ID in GEO (or EBI SRA) (ii) its condition (untreated or treated) and (iii) the sequencing technology used (single read or paired-end). id. in GEO id. in EBI SRA GSM461176_untreat_single SRR031709_untreat_single GSM461177_untreat_paired SRR031714_untreat_paired GSM461178_untreat_paired SRR031716_untreat_paired GSM461179_treat_single SRR031718_treat_single GSM461180_treat_paired SRR031724_treat_paired GSM461181_treat_paired SRR031726_treat_paired GSM461182_untreat_single SRR031728_untreat_single","title":"Data"},{"location":"reference_based_RNAseq/uploads/#data-upload","text":"We will take benefit of this mandatory stage, to review various possibilities to upload datasets in Galaxy. Specifically, we will review two options for uploading the gtf annotations for the Drosophila genome dm6 in a Galaxy history. We will also have a look to a third option that allows specifically to directly transfer FASTQ sequence files from the EBI SRA to a Galaxy history. Transfers of Big Files take time, especially when the Internet connection speed is moderate to low... To avoid consuming too much time on this task, you will have the possibility to import the full set of the 11 FASTQ files in one of your histories, from a data library that has been pre-set in your Galaxy server for this training session.","title":"Data upload"},{"location":"reference_based_RNAseq/uploads/#uploading-data-from-your-local-computer","text":"Download from the Ensembl database the sample Drosophila_melanogaster.BDGP6.95.gtf.gz to your computer. Upload this local file Drosophila_melanogaster.BDGP6.95.gtf.gz to your Galaxy history using the upload/Download Galaxy interface that pops up if you click the upload icone","title":"Uploading data from your local computer"},{"location":"reference_based_RNAseq/uploads/#importing-data-via-links-is-more-efficient-and-reliable","text":"The previous strategy is not efficient. Indeed, we can directly transfert the Drosophila_melanogaster.BDGP6.95.gtf.gz from its primary location in the Ensembl database server to your Galaxy History ! Copy its URL below 1 ftp://ftp.ensembl.org/pub/release-95/gtf/drosophila_melanogaster/Drosophila_melanogaster.BDGP6.95.gtf.gz and paste it in the Paste/Fetch data tab of the Galaxy upload interface. In addition, select gtf in the Type menu. Press the start button.","title":"Importing data via links is more efficient and reliable !"},{"location":"reference_based_RNAseq/uploads/#importing-data-via-the-ebi-sra-ena-sra","text":"Finally there is a tool to specifically fetch fastq sequence file from the EBI SRA to Galaxy The sample GSM461178/SRR031716 was sequenced using a paired-end strategy (both ends of fragments in the library are sequenced, giving rise to 2 read files, a forward read fastq file and a reverse read fastq file). We are going to download the fastq.gz files directly from the EBI SRA using the tool EBI SRA ENA SRA in the Get data tool submenu.","title":"Importing data via the EBI SRA ENA SRA"},{"location":"reference_based_RNAseq/uploads/#importing-data-from-data-libraries","text":"For collaborative work, Galaxy offers data libraries, where datasets can be stored and available to one or multiple users. This is what we are going to use to import rapidly all the input data you need for this RNAseq analysis. All datasets have been preloaded in the data library named RNAseq . To access this library and import its content in your histories: Click the menu Donn\u00e9es partag\u00e9es ( Shared data ) and select the submenu Biblioth\u00e8que de Donn\u00e9es ( Data libraries ). Navigate to the data library RNAseq Select all datasets Click the To History button and select as Datasets In the pop up window, or create new and type Input data to transfer the datasets in a new history with this name. Click on the green box to navigate to this new history (or click on the main menu analyse data ) and start using these datasets.","title":"Importing data from data libraries"},{"location":"reference_based_RNAseq/visu_map/","text":"Inspection of the mapping results UCSC genome browser click the ucsc main link as indicated by the orange arrow Zoom to chr4:540,000-560,000 (Chromosome 4 between 540 kb to 560 kb) IGV To use IGV with galaxy you need to have this tool on your computer. (If not you can download IGV from their main site .) Open locally IGV click the IGV local ling as indicated by the red arrow. Zoom to chr4:540,000-560,000 (Chromosome 4 between 540 kb to 560 kb)","title":"Visualisation of read mapping"},{"location":"reference_based_RNAseq/visu_map/#inspection-of-the-mapping-results","text":"","title":"Inspection of the mapping results"},{"location":"reference_based_RNAseq/visu_map/#ucsc-genome-browser","text":"click the ucsc main link as indicated by the orange arrow Zoom to chr4:540,000-560,000 (Chromosome 4 between 540 kb to 560 kb)","title":"UCSC genome browser"},{"location":"reference_based_RNAseq/visu_map/#igv","text":"To use IGV with galaxy you need to have this tool on your computer. (If not you can download IGV from their main site .) Open locally IGV click the IGV local ling as indicated by the red arrow. Zoom to chr4:540,000-560,000 (Chromosome 4 between 540 kb to 560 kb)","title":"IGV"},{"location":"reference_based_RNAseq/volcano/","text":"Volcano Plot create a volcano plot Select the Volcano Plot create a volcano plot tool with the following parameters: Specify an input file : the DESeq2 result file FDR (adjusted P value) : Column: 7 P value (raw) : Column: 6 Log Fold Change : Column: 3 Labels : Column: 1 Points to label : Significant Only label top most significant : 15 Plot Options : Label Boxes : No Labels for Legend : Down,NotSig,Up Execute You should obtain something like :","title":"Volcano Plot"},{"location":"reference_based_RNAseq/volcano/#volcano-plot-create-a-volcano-plot","text":"Select the Volcano Plot create a volcano plot tool with the following parameters: Specify an input file : the DESeq2 result file FDR (adjusted P value) : Column: 7 P value (raw) : Column: 6 Log Fold Change : Column: 3 Labels : Column: 1 Points to label : Significant Only label top most significant : 15 Plot Options : Label Boxes : No Labels for Legend : Down,NotSig,Up Execute You should obtain something like :","title":"Volcano Plot create a volcano plot"},{"location":"reference_based_RNAseq/workflow_intro/","text":"Galaxy Workflows At this point, you should be more familiar with importing and manipulating datasets in Galaxy using tools in single consecutive steps visualising the metadata associated to these steps as well as the results. However, this is only the tip of the Galaxy. Indeed, as you may have noticed, histories can become very complicated with a lot of datasets whose origin and purpose is not so easy to remember after a while (shorter that you may believe). Actually, the best way to preserve an analysis is to get it completely scripted in a computational workflow. This is where you find the Galaxy workflows ! Galaxy workflow can be extracted from an history or built from scratch using the Galaxy workflow editor (Menu worflows ). A workflow can be replayed at any time to regenerate an analysis. Importantly, they can be exported as a .ga file and imported in another Galaxy server. Provided that this new server has the input data and the tools specified by the workflow, the exact same analysis will be generated. Take home message: \"advanced Galaxy users use workflows, to capture their work and make convincing, transparent and re-usable their computational protocols\" In the next and last section, you will test 2 workflows that are available in your Galaxy server and recapitulate most of the analyses you have performed today.","title":"Introduction"},{"location":"reference_based_RNAseq/workflow_intro/#galaxy-workflows","text":"At this point, you should be more familiar with importing and manipulating datasets in Galaxy using tools in single consecutive steps visualising the metadata associated to these steps as well as the results. However, this is only the tip of the Galaxy. Indeed, as you may have noticed, histories can become very complicated with a lot of datasets whose origin and purpose is not so easy to remember after a while (shorter that you may believe). Actually, the best way to preserve an analysis is to get it completely scripted in a computational workflow. This is where you find the Galaxy workflows ! Galaxy workflow can be extracted from an history or built from scratch using the Galaxy workflow editor (Menu worflows ). A workflow can be replayed at any time to regenerate an analysis. Importantly, they can be exported as a .ga file and imported in another Galaxy server. Provided that this new server has the input data and the tools specified by the workflow, the exact same analysis will be generated. Take home message: \"advanced Galaxy users use workflows, to capture their work and make convincing, transparent and re-usable their computational protocols\" In the next and last section, you will test 2 workflows that are available in your Galaxy server and recapitulate most of the analyses you have performed today.","title":"Galaxy Workflows"},{"location":"reference_based_RNAseq/workflow_use/","text":"Workflow upload Same as data libraries, you can import workflows, from shared data that has been pre-set in your Galaxy server for this training session. To access these workflows : Click the menu Donn\u00e9es partag\u00e9es ( Shared data ) and select the submenu Workflows . You should see two workflows : paired-data-STAR-RNAseq and paired-data-HISAT2-RNAseq For each workflow, click on the arrow and select Import . Now, you'll be able to see these workflows in the Workflow menu. Running workflows You need to return to our first galaxy history Inputs , to do so : Click the menu Utilisateur and select the submenu Historiques sauvegard\u00e9s . Click on Inputs . Its status is now current history . Prepare inputs These workflows use data collection as inputs, one per condition treat and untreat . Let's create our two data collections ! Click on the checked box. Select all treated datasets in pair ends : GSM461180_1_treat_paired.fastq.gz GSM461181_1_treat_paired.fastq.gz GSM461180_2_treat_paired.fastq.gz GSM461181_2_treat_paired.fastq.gz Then click on the button Pour toute la s\u00e9lection... and Build List of Dataset Pairs . Enter a name for your dataset collection. Name : Treat data pairs. Create list Redo a data collections for untreated datasets. Unchecked the previous datasets. Select all untreated datasets in pair ends : GSM461177_1_untreat_paired.fastq.gz GSM461178_1_untreat_paired.fastq.gz GSM461177_2_untreat_paired.fastq.gz GSM461178_2_untreat_paired.fastq.gz Then click on the button Pour toute la s\u00e9lection... and Build List of Dataset Pairs . Enter a name for your dataset collection. Name : Untreat data pairs. Create list You are now the happy owner of two dataset paired collections ! It's time to test the worflows ! Go to Menu Workflow . For the workflow imported: paired-data-HISAT2-RNAseq , click on the arrow and then Run . History Options Send results to a new history : Yes 1: treated data pairs : Treat data pairs 2:GTF : Drosophila_melanogaster.BDGP6.95.gtf.gz 3: un-treated data pairs : Untreat data pairs Run workflow Redo the same for the workflow imported: paired-data-STAR-RNAseq .","title":"Example"},{"location":"reference_based_RNAseq/workflow_use/#workflow-upload","text":"Same as data libraries, you can import workflows, from shared data that has been pre-set in your Galaxy server for this training session. To access these workflows : Click the menu Donn\u00e9es partag\u00e9es ( Shared data ) and select the submenu Workflows . You should see two workflows : paired-data-STAR-RNAseq and paired-data-HISAT2-RNAseq For each workflow, click on the arrow and select Import . Now, you'll be able to see these workflows in the Workflow menu.","title":"Workflow upload"},{"location":"reference_based_RNAseq/workflow_use/#running-workflows","text":"You need to return to our first galaxy history Inputs , to do so : Click the menu Utilisateur and select the submenu Historiques sauvegard\u00e9s . Click on Inputs . Its status is now current history .","title":"Running workflows"},{"location":"reference_based_RNAseq/workflow_use/#prepare-inputs","text":"These workflows use data collection as inputs, one per condition treat and untreat . Let's create our two data collections ! Click on the checked box. Select all treated datasets in pair ends : GSM461180_1_treat_paired.fastq.gz GSM461181_1_treat_paired.fastq.gz GSM461180_2_treat_paired.fastq.gz GSM461181_2_treat_paired.fastq.gz Then click on the button Pour toute la s\u00e9lection... and Build List of Dataset Pairs . Enter a name for your dataset collection. Name : Treat data pairs. Create list Redo a data collections for untreated datasets. Unchecked the previous datasets. Select all untreated datasets in pair ends : GSM461177_1_untreat_paired.fastq.gz GSM461178_1_untreat_paired.fastq.gz GSM461177_2_untreat_paired.fastq.gz GSM461178_2_untreat_paired.fastq.gz Then click on the button Pour toute la s\u00e9lection... and Build List of Dataset Pairs . Enter a name for your dataset collection. Name : Untreat data pairs. Create list You are now the happy owner of two dataset paired collections ! It's time to test the worflows ! Go to Menu Workflow . For the workflow imported: paired-data-HISAT2-RNAseq , click on the arrow and then Run . History Options Send results to a new history : Yes 1: treated data pairs : Treat data pairs 2:GTF : Drosophila_melanogaster.BDGP6.95.gtf.gz 3: un-treated data pairs : Untreat data pairs Run workflow Redo the same for the workflow imported: paired-data-STAR-RNAseq .","title":"Prepare inputs"},{"location":"translocations/","text":"In this training session, we are going to analyse 2 datasets from two patients using Galaxy in order to find translocations between BCR (chr22) and ABL1 (chr9). We will visualize the coverage of these two genes by the datasets sequencing reads. We will use the tool lumpy to find putative translocations sites in patients genomes and we will visualize the position of these sites in the genome. This training manual is available in a better resolution as a Google Slide doc","title":"Introduction"},{"location":"translocations/BWA/","text":"BWA alignment of reads Select the BWA-MEM tool in the Galaxy tool bar Fill the Map with BWA-MEM - map medium and long reads ( 100 bp) against reference genome (Galaxy Version 0.7.17.1) tool form carefully: There are 5 parameters to check ! 3. Click the run button","title":"BWA mem tool"},{"location":"translocations/BWA/#bwa-alignment-of-reads","text":"Select the BWA-MEM tool in the Galaxy tool bar Fill the Map with BWA-MEM - map medium and long reads ( 100 bp) against reference genome (Galaxy Version 0.7.17.1) tool form carefully: There are 5 parameters to check ! 3. Click the run button","title":"BWA alignment of reads"},{"location":"translocations/connection/","text":"Connect to your Galaxy server account You are going to work as student pairs, each of these pairs having its own Galaxy account. Account | server mg-1 | http://lbcd41.snv.jussieu.fr mg-2 | http://lbcd41.snv.jussieu.fr mg-3 | http://artbio.snv.jussieu.fr mg-4 | http://artbio.snv.jussieu.fr mg-5 | http://artbio.snv.jussieu.fr mg-6 | http://artbio.snv.jussieu.fr mg-7 | http://artbio.snv.jussieu.fr Your password will be communicated during the training session At login, and before entering into the analysis, let's do the \"Galaxy tour\" !","title":"Trainee connection"},{"location":"translocations/connection/#connect-to-your-galaxy-server-account","text":"You are going to work as student pairs, each of these pairs having its own Galaxy account.","title":"Connect to your Galaxy server account"},{"location":"translocations/connection/#account-server","text":"","title":"Account | server"},{"location":"translocations/connection/#mg-1-httplbcd41snvjussieufr","text":"","title":"mg-1 | http://lbcd41.snv.jussieu.fr"},{"location":"translocations/connection/#mg-2-httplbcd41snvjussieufr","text":"","title":"mg-2 | http://lbcd41.snv.jussieu.fr"},{"location":"translocations/connection/#mg-3-httpartbiosnvjussieufr","text":"","title":"mg-3 | http://artbio.snv.jussieu.fr"},{"location":"translocations/connection/#mg-4-httpartbiosnvjussieufr","text":"","title":"mg-4 | http://artbio.snv.jussieu.fr"},{"location":"translocations/connection/#mg-5-httpartbiosnvjussieufr","text":"","title":"mg-5 | http://artbio.snv.jussieu.fr"},{"location":"translocations/connection/#mg-6-httpartbiosnvjussieufr","text":"","title":"mg-6 | http://artbio.snv.jussieu.fr"},{"location":"translocations/connection/#mg-7-httpartbiosnvjussieufr","text":"Your password will be communicated during the training session","title":"mg-7 | http://artbio.snv.jussieu.fr"},{"location":"translocations/connection/#at-login-and-before-entering-into-the-analysis-lets-do-the-galaxy-tour","text":"","title":"At login, and before entering into the analysis, let's do the \"Galaxy tour\" !"},{"location":"translocations/coverage/","text":"Read Coverage using the Bamcoverage tool Select the tool bamCoverage generates a coverage bigWig file from a given BAM or CRAM file (Galaxy Version 3.1.2.0.0) BAM/CRAM file: Dataset Collection and Map with BWA-MEM on collection 3 (mapped reads in BAM format) Bin size in bases: 100 Scaling/Normalization method: Do not normalize or scale Coverage file format: bigWig Compute an exact scaling factor: no Region of the genome to limit the operation to: chr9 Show advanced options: yes Ignore missing data?: yes Other options unchanged run the tool rerun the tool paying attention to reselect BAM/CRAM file: Dataset Collection and Map with BWA-MEM on collection 3 (mapped reads in BAM format) Region of the genome to limit the operation to: chr22","title":"Compute genome coverage"},{"location":"translocations/coverage/#read-coverage-using-the-bamcoverage-tool","text":"Select the tool bamCoverage generates a coverage bigWig file from a given BAM or CRAM file (Galaxy Version 3.1.2.0.0) BAM/CRAM file: Dataset Collection and Map with BWA-MEM on collection 3 (mapped reads in BAM format) Bin size in bases: 100 Scaling/Normalization method: Do not normalize or scale Coverage file format: bigWig Compute an exact scaling factor: no Region of the genome to limit the operation to: chr9 Show advanced options: yes Ignore missing data?: yes Other options unchanged run the tool rerun the tool paying attention to reselect BAM/CRAM file: Dataset Collection and Map with BWA-MEM on collection 3 (mapped reads in BAM format) Region of the genome to limit the operation to: chr22","title":"Read Coverage using the Bamcoverage tool"},{"location":"translocations/filter_vcf/","text":"Reformat the vcf output of lumpy-sv 1. Save the header of the vcf This is just done using the tool Select lines that match an expression (Galaxy Version 1.0.1) with the parameter Dataset collection Select lines from: 9: Variant Lumpy Calling that: matching the pattern: ^# 2. Save the rest of the vcf file in another Dataset Here we use the same Select lines that match an expression tools, this time with the parameters: Select lines from: 9: Variant Lumpy Calling that: matching the pattern: SVTYPE=BND TIP ! Just re-play the previous tool run, and just change the pattern from ^# to SVTYPE=BND This allows to kill two birds with the same stone: Selecting non-header part of the vcf filtering out the variations that are not of type BND (Bondaries) 3. Reorder the vcf lines in the previous dataset To do that, we use the tool Sort data in ascending or descending order (Galaxy Version 1.1.0) Sort Query: Dataset collection and 16: Select on collection 9 Number of header lines: 0 1: Column selections on column: Column: 1 in: Ascending order Flavor: Natural / Version sort (-V) 2: Column selections on column: Column: 2 in: Ascending order Flavor: Fast numeric sort (-n) Output unique values: No Ignore case: No 4. Reassemble the saved header with the sorted/filtered vcf lines To do that, we use the tool Concatenate datasets tail-to-head (Galaxy Version 1.0.0) Pay extra attention to the version of the tool, because there is a number of concatenate tools with the same name. Concatenate Dataset: Dataset collection and 14: Select on collection 9 Click on + Insert Dataset to trigger an additional form section Dataset: Dataset collection and 18: Sort on collection 16 1: Column selections 5. Rename the dataset within the last dataset collection. Here we will rename the dataset of the last dataset collection. Here, there is something that maybe tricky to understand. A dataset collection is a kind of dictionary whose elements, the datasets, have as labels the key of this dictionary. However, the real name of this dataset is hidden. To see the real name of the dataset, navigate in the last collection (click on it) and further click the pencil icone. Here you can change the real name of the dataset, which is useful for the next step of visualisation in Genome Browser Rename it as patient A vcf or patient B vcf and click Save","title":"Filter VCF file for visualization"},{"location":"translocations/filter_vcf/#reformat-the-vcf-output-of-lumpy-sv","text":"","title":"Reformat the vcf output of lumpy-sv"},{"location":"translocations/filter_vcf/#1-save-the-header-of-the-vcf","text":"This is just done using the tool Select lines that match an expression (Galaxy Version 1.0.1) with the parameter Dataset collection Select lines from: 9: Variant Lumpy Calling that: matching the pattern: ^#","title":"1. Save the header of the vcf"},{"location":"translocations/filter_vcf/#2-save-the-rest-of-the-vcf-file-in-another-dataset","text":"Here we use the same Select lines that match an expression tools, this time with the parameters: Select lines from: 9: Variant Lumpy Calling that: matching the pattern: SVTYPE=BND","title":"2. Save the rest of the vcf file in another Dataset"},{"location":"translocations/filter_vcf/#tip","text":"Just re-play the previous tool run, and just change the pattern from ^# to SVTYPE=BND This allows to kill two birds with the same stone: Selecting non-header part of the vcf filtering out the variations that are not of type BND (Bondaries)","title":"TIP !"},{"location":"translocations/filter_vcf/#3-reorder-the-vcf-lines-in-the-previous-dataset","text":"To do that, we use the tool Sort data in ascending or descending order (Galaxy Version 1.1.0) Sort Query: Dataset collection and 16: Select on collection 9 Number of header lines: 0 1: Column selections on column: Column: 1 in: Ascending order Flavor: Natural / Version sort (-V) 2: Column selections on column: Column: 2 in: Ascending order Flavor: Fast numeric sort (-n) Output unique values: No Ignore case: No","title":"3. Reorder the vcf lines in the previous dataset"},{"location":"translocations/filter_vcf/#4-reassemble-the-saved-header-with-the-sortedfiltered-vcf-lines","text":"To do that, we use the tool Concatenate datasets tail-to-head (Galaxy Version 1.0.0) Pay extra attention to the version of the tool, because there is a number of concatenate tools with the same name. Concatenate Dataset: Dataset collection and 14: Select on collection 9 Click on + Insert Dataset to trigger an additional form section Dataset: Dataset collection and 18: Sort on collection 16 1: Column selections","title":"4. Reassemble the saved header with the sorted/filtered vcf lines"},{"location":"translocations/filter_vcf/#5-rename-the-dataset-within-the-last-dataset-collection","text":"Here we will rename the dataset of the last dataset collection. Here, there is something that maybe tricky to understand. A dataset collection is a kind of dictionary whose elements, the datasets, have as labels the key of this dictionary. However, the real name of this dataset is hidden. To see the real name of the dataset, navigate in the last collection (click on it) and further click the pencil icone. Here you can change the real name of the dataset, which is useful for the next step of visualisation in Genome Browser Rename it as patient A vcf or patient B vcf and click Save","title":"5. Rename the dataset within the last dataset collection."},{"location":"translocations/import/","text":"Prepare your input data history Rename your Unnamed history to Input Dataset and collections Go to menu Shared Data Data Libraries ( Donn\u00e9es Partag\u00e9es Biblioth\u00e8que de Donn\u00e9es ) Choose Mouse Genetics library Select the 4 fastq files (A_R1.fastq, A_R2.fastq, B_R1.fastq and B_R2.fastq) Select the To History tab as datasets Select your freshly renamed Input Dataset and collections in the select history menu Click Import button After the import, navigate directly to this history by clicking the green warning Prepare two collections from your raw input datasets. Toggle the \"checkbox\" mode by clicking the small checkbox icon at the top of the history bar Select the 2 A fastq files OR the 2 B fasq files (not all 4 files, choose as you feel it!) Select Build List of Dataset Pairs from the tab Pour toute la s\u00e9lection in the pop up window, replace _1 by _R1 and _2 by _R2 Click the Pair these datasets tab Name your new \"paired dataset\" collection with a single element A_fastq (or B_fastq if you chose the B fastq file at the previous step) and click on Create list Back to your history, that is still in \"checkbox\" mode, select the 4 fastq files, and repeat the operation to produce this time a collection of 2 paired-sequences element, which you will name this time patient sequences Time to start the analysis: Select the Copy datasets in the history \"wheel\" menu Select the first collection with a single element (A or B) that you first prepared in the destination history area, fill the New history named field with Single sequence dataset analysis and click the Copy History Items button Click the link that shows up to navigate directely to this new history !","title":"Import datasets"},{"location":"translocations/import/#prepare-your-input-data-history","text":"Rename your Unnamed history to Input Dataset and collections Go to menu Shared Data Data Libraries ( Donn\u00e9es Partag\u00e9es Biblioth\u00e8que de Donn\u00e9es ) Choose Mouse Genetics library Select the 4 fastq files (A_R1.fastq, A_R2.fastq, B_R1.fastq and B_R2.fastq) Select the To History tab as datasets Select your freshly renamed Input Dataset and collections in the select history menu Click Import button After the import, navigate directly to this history by clicking the green warning Prepare two collections from your raw input datasets. Toggle the \"checkbox\" mode by clicking the small checkbox icon at the top of the history bar Select the 2 A fastq files OR the 2 B fasq files (not all 4 files, choose as you feel it!) Select Build List of Dataset Pairs from the tab Pour toute la s\u00e9lection in the pop up window, replace _1 by _R1 and _2 by _R2 Click the Pair these datasets tab Name your new \"paired dataset\" collection with a single element A_fastq (or B_fastq if you chose the B fastq file at the previous step) and click on Create list Back to your history, that is still in \"checkbox\" mode, select the 4 fastq files, and repeat the operation to produce this time a collection of 2 paired-sequences element, which you will name this time patient sequences Time to start the analysis: Select the Copy datasets in the history \"wheel\" menu Select the first collection with a single element (A or B) that you first prepared in the destination history area, fill the New history named field with Single sequence dataset analysis and click the Copy History Items button Click the link that shows up to navigate directely to this new history !","title":"Prepare your input data history"},{"location":"translocations/lumpy/","text":"Lumpy tool - lumpy step Now you are going to analyse the BAM alignment file generated by BWA, using lumpy-sv. The lumpy-sv tool analyse the alignments and search for split alignment a read whose 5' and 3' parts map to non-contiguous regions discordant alignments of read pairs Either one mate maps to one chromosome and the other mate maps to another chromosome, or the distance between the two mates is beyond what is statistically expected (the library fragment size in average). For an insertion, the distance increases, for a deletion, the distance decreases from, this parsing, lumpy then constructs models of break points that can explain its findings, and report these models in a vcf file (vcf stands for variant calling format), with statistical significance, number of evidences founds, etc... 1 2 3 4 5 6 7 8 9 10 1 . launch the lumpy - sv tool 2 . review the parameters : - input ( s ) : One sample ( because we are not comparing the alignments with a reference alignment ) - One BAM alignment file produced by BWA - mem : be sure to toggle the dataset collection mode ( arrow in the screen shot below ). - sequencing method : paired - end sequencing - variant calling format : vcf 3 . run the tool ! 1 4. Look at the vcf returned by lumpy-sv. It should be easy to read: lumpy output variation as a suite of single lines (for deletions, insertion, or SNPs), or as a suite of line pairs, for translocation (one line for the translocation, another line for the reciprocal translocation). Note that sometimes, there is evidence for one translocation, but not for the reciprocal event. Look at the ID column: a pair of translocation event model will have for instance IDs and 2_1 and 2_2, respectively. However, the lumpy vcf format is not suitable for visualisation in a genome browser such as UCSC genome browser. For these browser, lines have to be sorted in the order of chromosomes (first) and in the order of coordinates (secondly). The next steps of the analysis are for reorganise the lumpy vcf output to follow these rules. Also, we are going to focus on translocations, filtering out other variation we are not interested in.","title":"lumpy analysis"},{"location":"translocations/lumpy/#lumpy-tool-lumpy-step","text":"Now you are going to analyse the BAM alignment file generated by BWA, using lumpy-sv. The lumpy-sv tool analyse the alignments and search for split alignment a read whose 5' and 3' parts map to non-contiguous regions discordant alignments of read pairs Either one mate maps to one chromosome and the other mate maps to another chromosome, or the distance between the two mates is beyond what is statistically expected (the library fragment size in average). For an insertion, the distance increases, for a deletion, the distance decreases from, this parsing, lumpy then constructs models of break points that can explain its findings, and report these models in a vcf file (vcf stands for variant calling format), with statistical significance, number of evidences founds, etc... 1 2 3 4 5 6 7 8 9 10 1 . launch the lumpy - sv tool 2 . review the parameters : - input ( s ) : One sample ( because we are not comparing the alignments with a reference alignment ) - One BAM alignment file produced by BWA - mem : be sure to toggle the dataset collection mode ( arrow in the screen shot below ). - sequencing method : paired - end sequencing - variant calling format : vcf 3 . run the tool ! 1 4. Look at the vcf returned by lumpy-sv. It should be easy to read: lumpy output variation as a suite of single lines (for deletions, insertion, or SNPs), or as a suite of line pairs, for translocation (one line for the translocation, another line for the reciprocal translocation). Note that sometimes, there is evidence for one translocation, but not for the reciprocal event. Look at the ID column: a pair of translocation event model will have for instance IDs and 2_1 and 2_2, respectively. However, the lumpy vcf format is not suitable for visualisation in a genome browser such as UCSC genome browser. For these browser, lines have to be sorted in the order of chromosomes (first) and in the order of coordinates (secondly). The next steps of the analysis are for reorganise the lumpy vcf output to follow these rules. Also, we are going to focus on translocations, filtering out other variation we are not interested in.","title":"Lumpy tool - lumpy step"},{"location":"translocations/lumpy_approach/","text":"Lumpy algorithm","title":"Lumpy approach"},{"location":"translocations/lumpy_approach/#lumpy-algorithm","text":"","title":"Lumpy algorithm"},{"location":"translocations/run_workflow/","text":"Run Workflow","title":"Run the workflow"},{"location":"translocations/run_workflow/#run-workflow","text":"","title":"Run Workflow"},{"location":"translocations/seq/","text":"DNA libraries and sequencing protocol for patient samples Bone marrow samples from patients with Acute Lymphoid Leukemia (ALL) Genomic DNA from bone marrow cells DNA fragmentation (expected ~200-500nt long) Selection of DNA fragments : 2 regions of several dozens of kb centered on BCR and ABL1 Library preparation Sequencing of both ends of fragments: Paired-End sequencing (see figure below)","title":"Sequencing Protocol"},{"location":"translocations/seq/#dna-libraries-and-sequencing-protocol-for-patient-samples","text":"Bone marrow samples from patients with Acute Lymphoid Leukemia (ALL) Genomic DNA from bone marrow cells DNA fragmentation (expected ~200-500nt long) Selection of DNA fragments : 2 regions of several dozens of kb centered on BCR and ABL1 Library preparation Sequencing of both ends of fragments: Paired-End sequencing (see figure below)","title":"DNA libraries and sequencing protocol for patient samples"},{"location":"translocations/visualisation/","text":"Visualisations","title":"Visualisation in UCSC Genome Browser"},{"location":"translocations/visualisation/#visualisations","text":"","title":"Visualisations"},{"location":"translocations/workflow/","text":"Extract Workflow from Galaxy history 1 2","title":"Generate workflow from history"},{"location":"translocations/workflow/#extract-workflow-from-galaxy-history","text":"","title":"Extract Workflow from Galaxy history"},{"location":"translocations/workflow/#1","text":"","title":"1"},{"location":"translocations/workflow/#2","text":"","title":"2"}]}